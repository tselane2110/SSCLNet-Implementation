{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/tselane2110/SSCLNet-Implementation/blob/main/implementation.ipynb",
      "authorship_tag": "ABX9TyMVeOFtmh+sO5Nlue921eRl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tselane2110/SSCLNet-Implementation/blob/main/implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbIIjUI3Nd8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13520f14-b52f-41fa-ede9-e27fe51f2c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SSCLNet-Implementation'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 238 (delta 52), reused 44 (delta 18), pack-reused 143 (from 1)\u001b[K\n",
            "Receiving objects: 100% (238/238), 207.16 KiB | 1.56 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tselane2110/SSCLNet-Implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/SSCLNet-Implementation"
      ],
      "metadata": {
        "id": "VDBJRkIFBIt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading & Preprocessing"
      ],
      "metadata": {
        "id": "bp8ivRpez212"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dataset"
      ],
      "metadata": {
        "id": "Ks6yLvOFN1Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.load_data(\"https://drive.google.com/file/d/1QI9_a1qjLyKOsj8IOFdRAZVOGs3W51jL/view?usp=drive_link\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "gSSgcKkYBAvF",
        "outputId": "142bf529-f974-42a6-dea8-04bedc75bfb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1QI9_a1qjLyKOsj8IOFdRAZVOGs3W51jL\n",
            "From (redirected): https://drive.google.com/uc?id=1QI9_a1qjLyKOsj8IOFdRAZVOGs3W51jL&confirm=t&uuid=0333eb9c-3b49-48fe-a889-22e3cf4bdc06\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 209M/209M [00:02<00:00, 96.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Dataset ready at: /content/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.split_data_disjoint_pretrain(\"/content/Dataset-Brain-MRI\", \"/content/splitted-dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzkojWZpBffN",
        "outputId": "9f0812fb-b292-4514-c378-984a5481b3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DISJOINT dataset splitting (70% pretrain, 20% train, 10% test)...\n",
            "Input directory: /content/Dataset-Brain-MRI\n",
            "Output directory: /content/splitted-dataset\n",
            "Created: /content/splitted-dataset/pretrain\n",
            "Created: /content/splitted-dataset/train/2-class/yes\n",
            "Created: /content/splitted-dataset/train/2-class/no\n",
            "Created: /content/splitted-dataset/train/5-class/Glioblastoma\n",
            "Created: /content/splitted-dataset/train/5-class/glioma_tumor\n",
            "Created: /content/splitted-dataset/train/5-class/meningioma_tumor\n",
            "Created: /content/splitted-dataset/train/5-class/no_tumor\n",
            "Created: /content/splitted-dataset/train/5-class/pituitary_tumor\n",
            "Created: /content/splitted-dataset/test/2-class/yes\n",
            "Created: /content/splitted-dataset/test/2-class/no\n",
            "Created: /content/splitted-dataset/test/5-class/Glioblastoma\n",
            "Created: /content/splitted-dataset/test/5-class/glioma_tumor\n",
            "Created: /content/splitted-dataset/test/5-class/meningioma_tumor\n",
            "Created: /content/splitted-dataset/test/5-class/no_tumor\n",
            "Created: /content/splitted-dataset/test/5-class/pituitary_tumor\n",
            "\n",
            "Total images collected: 6763\n",
            "\n",
            "Copying files...\n",
            "\n",
            "=== SPLITTING COMPLETED ===\n",
            "Pre-train: 4734 images (70% of total)\n",
            "Train: 1352 images (20% of total)\n",
            "Test: 677 images (10% of total)\n",
            "Total: 6763 images\n",
            "\n",
            "Final structure:\n",
            "/content/splitted-dataset/\n",
            "├── pretrain/          # 70% of ALL data (no labels needed)\n",
            "├── train/\n",
            "│   ├── 2-class/       # 20% of original 2-class data\n",
            "│   │   ├── yes/\n",
            "│   │   └── no/\n",
            "│   └── 5-class/       # 20% of original 5-class data\n",
            "│       ├── Glioblastoma/\n",
            "│       ├── glioma_tumor/\n",
            "│       ├── meningioma_tumor/\n",
            "│       ├── no_tumor/\n",
            "│       └── pituitary_tumor/\n",
            "└── test/\n",
            "    ├── 2-class/       # 10% of original 2-class data\n",
            "    │   ├── yes/\n",
            "    │   └── no/\n",
            "    └── 5-class/       # 10% of original 5-class data\n",
            "        ├── Glioblastoma/\n",
            "        ├── glioma_tumor/\n",
            "        ├── meningioma_tumor/\n",
            "        ├── no_tumor/\n",
            "        └── pituitary_tumor/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.preprocess_split_data(\"/content/splitted-dataset\", \"/content/Preprocessed-splitted-data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FCmgjjJB7rD",
        "outputId": "baefd080-eaee-415c-f64a-37cac89c2e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing split dataset with new structure...\n",
            "Input path: /content/splitted-dataset\n",
            "Output path: /content/Preprocessed-splitted-data\n",
            "\n",
            "Processing pretrain folder...\n",
            "  Pretrain: 4556 images processed\n",
            "\n",
            "Processing train folder...\n",
            "  Train/2-class/yes: 40 images processed\n",
            "  Train/2-class/no: 25 images processed\n",
            "  Train/5-class/Glioblastoma: 194 images processed\n",
            "  Train/5-class/glioma_tumor: 282 images processed\n",
            "  Train/5-class/meningioma_tumor: 330 images processed\n",
            "  Train/5-class/no_tumor: 193 images processed\n",
            "  Train/5-class/pituitary_tumor: 288 images processed\n",
            "\n",
            "Processing test folder...\n",
            "  Test/2-class/yes: 14 images processed\n",
            "  Test/2-class/no: 8 images processed\n",
            "  Test/5-class/Glioblastoma: 90 images processed\n",
            "  Test/5-class/glioma_tumor: 149 images processed\n",
            "  Test/5-class/meningioma_tumor: 163 images processed\n",
            "  Test/5-class/no_tumor: 95 images processed\n",
            "  Test/5-class/pituitary_tumor: 158 images processed\n",
            "\n",
            "=== PREPROCESSING COMPLETED ===\n",
            "Total images successfully processed: 6585\n",
            "Total errors: 0\n",
            "Preprocessed dataset saved to: /content/Preprocessed-splitted-data\n",
            "\n",
            "Final structure:\n",
            "/content/Preprocessed-splitted-data/\n",
            "├── pretrain/          # 70% of ALL data (preprocessed, no labels)\n",
            "├── train/\n",
            "│   ├── 2-class/       # 20% of 2-class data (preprocessed)\n",
            "│   │   ├── yes/\n",
            "│   │   └── no/\n",
            "│   └── 5-class/       # 20% of 5-class data (preprocessed)\n",
            "│       ├── Glioblastoma/\n",
            "│       ├── glioma_tumor/\n",
            "│       ├── meningioma_tumor/\n",
            "│       ├── no_tumor/\n",
            "│       └── pituitary_tumor/\n",
            "└── test/\n",
            "    ├── 2-class/       # 10% of 2-class data (preprocessed)\n",
            "    │   ├── yes/\n",
            "    │   └── no/\n",
            "    └── 5-class/       # 10% of 5-class data (preprocessed)\n",
            "        ├── Glioblastoma/\n",
            "        ├── glioma_tumor/\n",
            "        ├── meningioma_tumor/\n",
            "        ├── no_tumor/\n",
            "        └── pituitary_tumor/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"/content/splitted_data.zip\" \"/content/splitted-dataset\""
      ],
      "metadata": {
        "id": "Vo2o41JLH2k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Downloaded the `splitted_data.zip` file manually and uploaded it to the gdrive folder."
      ],
      "metadata": {
        "id": "KRuhcRMkIxMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"/content/preprocessed_splitted_data.zip\" \"/content/Preprocessed-splitted-data\""
      ],
      "metadata": {
        "id": "kzOnki8CCJnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Downloaded the `preprocessed_splitted_data.zip` file manually and uploaded it to the gdrive folder."
      ],
      "metadata": {
        "id": "J7J_owlgIFz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training Phase 1 - Contrastive Learning"
      ],
      "metadata": {
        "id": "FbOAd7E-0B6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tselane2110/SSCLNet-Implementation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TiXxFBM9ItD",
        "outputId": "a250237b-60aa-4707-a0c9-43f2423b5d55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SSCLNet-Implementation'...\n",
            "remote: Enumerating objects: 261, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 261 (delta 66), reused 56 (delta 24), pack-reused 143 (from 1)\u001b[K\n",
            "Receiving objects: 100% (261/261), 446.85 KiB | 12.41 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y/view?usp=drive_link\"\n",
        "!unzip -q /content/Preprocessed-splitted-data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj4fJAf413LZ",
        "outputId": "b2cc2520-3393-463e-9f0f-3bebc7ea954f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y\n",
            "From (redirected): https://drive.google.com/uc?id=16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y&confirm=t&uuid=9da16ae1-b21b-4cef-b0fb-10eb2918f30e\n",
            "To: /content/Preprocessed-splitted-data.zip\n",
            "100% 84.7M/84.7M [00:02<00:00, 41.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# updating file paths for pretrain and train data in the config.py file\n",
        "# PRETRAIN_DATA_PATH = \"/content/Preprocessed-splitted-data/pretrain\"\n",
        "# TRAIN_DATA_PATH = \"/content/Preprocessed-splitted-data/train/5-class\"\n",
        "# we will perform the supervised training again on 2-class data"
      ],
      "metadata": {
        "id": "5eYOmyxK2VzA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/SSCLNet-Implementation/train_contrastive.py\""
      ],
      "metadata": {
        "id": "nv_YUV2GIui8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb40ceee-6f4f-4382-8097-7e4abe7436d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-01 11:16:23.839331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761995783.859072    3017 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761995783.865080    3017 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761995783.881823    3017 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761995783.881849    3017 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761995783.881854    3017 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761995783.881858    3017 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-01 11:16:23.886605: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✓ All directories created\n",
            "✓ All directories created\n",
            "✓ Random seed set to: 42\n",
            "2025-11-01 11:16:29,452 - contrastive_pretraining - INFO - === Starting Contrastive Pre-training ===\n",
            "================================================================================\n",
            "MODEL SUMMARY\n",
            "================================================================================\n",
            "Model: SSCLNet\n",
            "Total trainable parameters: 25,669,541\n",
            "Model size: 98.12 MB\n",
            "\n",
            "Layer breakdown:\n",
            "  encoder.conv1.weight: 3,136\n",
            "  encoder.bn1.weight: 64\n",
            "  encoder.bn1.bias: 64\n",
            "  encoder.layer1.0.conv1.weight: 4,096\n",
            "  encoder.layer1.0.bn1.weight: 64\n",
            "  encoder.layer1.0.bn1.bias: 64\n",
            "  encoder.layer1.0.conv2.weight: 36,864\n",
            "  encoder.layer1.0.bn2.weight: 64\n",
            "  encoder.layer1.0.bn2.bias: 64\n",
            "  encoder.layer1.0.conv3.weight: 16,384\n",
            "  encoder.layer1.0.bn3.weight: 256\n",
            "  encoder.layer1.0.bn3.bias: 256\n",
            "  encoder.layer1.0.downsample.0.weight: 16,384\n",
            "  encoder.layer1.0.downsample.1.weight: 256\n",
            "  encoder.layer1.0.downsample.1.bias: 256\n",
            "  encoder.layer1.1.conv1.weight: 16,384\n",
            "  encoder.layer1.1.bn1.weight: 64\n",
            "  encoder.layer1.1.bn1.bias: 64\n",
            "  encoder.layer1.1.conv2.weight: 36,864\n",
            "  encoder.layer1.1.bn2.weight: 64\n",
            "  encoder.layer1.1.bn2.bias: 64\n",
            "  encoder.layer1.1.conv3.weight: 16,384\n",
            "  encoder.layer1.1.bn3.weight: 256\n",
            "  encoder.layer1.1.bn3.bias: 256\n",
            "  encoder.layer1.2.conv1.weight: 16,384\n",
            "  encoder.layer1.2.bn1.weight: 64\n",
            "  encoder.layer1.2.bn1.bias: 64\n",
            "  encoder.layer1.2.conv2.weight: 36,864\n",
            "  encoder.layer1.2.bn2.weight: 64\n",
            "  encoder.layer1.2.bn2.bias: 64\n",
            "  encoder.layer1.2.conv3.weight: 16,384\n",
            "  encoder.layer1.2.bn3.weight: 256\n",
            "  encoder.layer1.2.bn3.bias: 256\n",
            "  encoder.layer2.0.conv1.weight: 32,768\n",
            "  encoder.layer2.0.bn1.weight: 128\n",
            "  encoder.layer2.0.bn1.bias: 128\n",
            "  encoder.layer2.0.conv2.weight: 147,456\n",
            "  encoder.layer2.0.bn2.weight: 128\n",
            "  encoder.layer2.0.bn2.bias: 128\n",
            "  encoder.layer2.0.conv3.weight: 65,536\n",
            "  encoder.layer2.0.bn3.weight: 512\n",
            "  encoder.layer2.0.bn3.bias: 512\n",
            "  encoder.layer2.0.downsample.0.weight: 131,072\n",
            "  encoder.layer2.0.downsample.1.weight: 512\n",
            "  encoder.layer2.0.downsample.1.bias: 512\n",
            "  encoder.layer2.1.conv1.weight: 65,536\n",
            "  encoder.layer2.1.bn1.weight: 128\n",
            "  encoder.layer2.1.bn1.bias: 128\n",
            "  encoder.layer2.1.conv2.weight: 147,456\n",
            "  encoder.layer2.1.bn2.weight: 128\n",
            "  encoder.layer2.1.bn2.bias: 128\n",
            "  encoder.layer2.1.conv3.weight: 65,536\n",
            "  encoder.layer2.1.bn3.weight: 512\n",
            "  encoder.layer2.1.bn3.bias: 512\n",
            "  encoder.layer2.2.conv1.weight: 65,536\n",
            "  encoder.layer2.2.bn1.weight: 128\n",
            "  encoder.layer2.2.bn1.bias: 128\n",
            "  encoder.layer2.2.conv2.weight: 147,456\n",
            "  encoder.layer2.2.bn2.weight: 128\n",
            "  encoder.layer2.2.bn2.bias: 128\n",
            "  encoder.layer2.2.conv3.weight: 65,536\n",
            "  encoder.layer2.2.bn3.weight: 512\n",
            "  encoder.layer2.2.bn3.bias: 512\n",
            "  encoder.layer2.3.conv1.weight: 65,536\n",
            "  encoder.layer2.3.bn1.weight: 128\n",
            "  encoder.layer2.3.bn1.bias: 128\n",
            "  encoder.layer2.3.conv2.weight: 147,456\n",
            "  encoder.layer2.3.bn2.weight: 128\n",
            "  encoder.layer2.3.bn2.bias: 128\n",
            "  encoder.layer2.3.conv3.weight: 65,536\n",
            "  encoder.layer2.3.bn3.weight: 512\n",
            "  encoder.layer2.3.bn3.bias: 512\n",
            "  encoder.layer3.0.conv1.weight: 131,072\n",
            "  encoder.layer3.0.bn1.weight: 256\n",
            "  encoder.layer3.0.bn1.bias: 256\n",
            "  encoder.layer3.0.conv2.weight: 589,824\n",
            "  encoder.layer3.0.bn2.weight: 256\n",
            "  encoder.layer3.0.bn2.bias: 256\n",
            "  encoder.layer3.0.conv3.weight: 262,144\n",
            "  encoder.layer3.0.bn3.weight: 1,024\n",
            "  encoder.layer3.0.bn3.bias: 1,024\n",
            "  encoder.layer3.0.downsample.0.weight: 524,288\n",
            "  encoder.layer3.0.downsample.1.weight: 1,024\n",
            "  encoder.layer3.0.downsample.1.bias: 1,024\n",
            "  encoder.layer3.1.conv1.weight: 262,144\n",
            "  encoder.layer3.1.bn1.weight: 256\n",
            "  encoder.layer3.1.bn1.bias: 256\n",
            "  encoder.layer3.1.conv2.weight: 589,824\n",
            "  encoder.layer3.1.bn2.weight: 256\n",
            "  encoder.layer3.1.bn2.bias: 256\n",
            "  encoder.layer3.1.conv3.weight: 262,144\n",
            "  encoder.layer3.1.bn3.weight: 1,024\n",
            "  encoder.layer3.1.bn3.bias: 1,024\n",
            "  encoder.layer3.2.conv1.weight: 262,144\n",
            "  encoder.layer3.2.bn1.weight: 256\n",
            "  encoder.layer3.2.bn1.bias: 256\n",
            "  encoder.layer3.2.conv2.weight: 589,824\n",
            "  encoder.layer3.2.bn2.weight: 256\n",
            "  encoder.layer3.2.bn2.bias: 256\n",
            "  encoder.layer3.2.conv3.weight: 262,144\n",
            "  encoder.layer3.2.bn3.weight: 1,024\n",
            "  encoder.layer3.2.bn3.bias: 1,024\n",
            "  encoder.layer3.3.conv1.weight: 262,144\n",
            "  encoder.layer3.3.bn1.weight: 256\n",
            "  encoder.layer3.3.bn1.bias: 256\n",
            "  encoder.layer3.3.conv2.weight: 589,824\n",
            "  encoder.layer3.3.bn2.weight: 256\n",
            "  encoder.layer3.3.bn2.bias: 256\n",
            "  encoder.layer3.3.conv3.weight: 262,144\n",
            "  encoder.layer3.3.bn3.weight: 1,024\n",
            "  encoder.layer3.3.bn3.bias: 1,024\n",
            "  encoder.layer3.4.conv1.weight: 262,144\n",
            "  encoder.layer3.4.bn1.weight: 256\n",
            "  encoder.layer3.4.bn1.bias: 256\n",
            "  encoder.layer3.4.conv2.weight: 589,824\n",
            "  encoder.layer3.4.bn2.weight: 256\n",
            "  encoder.layer3.4.bn2.bias: 256\n",
            "  encoder.layer3.4.conv3.weight: 262,144\n",
            "  encoder.layer3.4.bn3.weight: 1,024\n",
            "  encoder.layer3.4.bn3.bias: 1,024\n",
            "  encoder.layer3.5.conv1.weight: 262,144\n",
            "  encoder.layer3.5.bn1.weight: 256\n",
            "  encoder.layer3.5.bn1.bias: 256\n",
            "  encoder.layer3.5.conv2.weight: 589,824\n",
            "  encoder.layer3.5.bn2.weight: 256\n",
            "  encoder.layer3.5.bn2.bias: 256\n",
            "  encoder.layer3.5.conv3.weight: 262,144\n",
            "  encoder.layer3.5.bn3.weight: 1,024\n",
            "  encoder.layer3.5.bn3.bias: 1,024\n",
            "  encoder.layer4.0.conv1.weight: 524,288\n",
            "  encoder.layer4.0.bn1.weight: 512\n",
            "  encoder.layer4.0.bn1.bias: 512\n",
            "  encoder.layer4.0.conv2.weight: 2,359,296\n",
            "  encoder.layer4.0.bn2.weight: 512\n",
            "  encoder.layer4.0.bn2.bias: 512\n",
            "  encoder.layer4.0.conv3.weight: 1,048,576\n",
            "  encoder.layer4.0.bn3.weight: 2,048\n",
            "  encoder.layer4.0.bn3.bias: 2,048\n",
            "  encoder.layer4.0.downsample.0.weight: 2,097,152\n",
            "  encoder.layer4.0.downsample.1.weight: 2,048\n",
            "  encoder.layer4.0.downsample.1.bias: 2,048\n",
            "  encoder.layer4.1.conv1.weight: 1,048,576\n",
            "  encoder.layer4.1.bn1.weight: 512\n",
            "  encoder.layer4.1.bn1.bias: 512\n",
            "  encoder.layer4.1.conv2.weight: 2,359,296\n",
            "  encoder.layer4.1.bn2.weight: 512\n",
            "  encoder.layer4.1.bn2.bias: 512\n",
            "  encoder.layer4.1.conv3.weight: 1,048,576\n",
            "  encoder.layer4.1.bn3.weight: 2,048\n",
            "  encoder.layer4.1.bn3.bias: 2,048\n",
            "  encoder.layer4.2.conv1.weight: 1,048,576\n",
            "  encoder.layer4.2.bn1.weight: 512\n",
            "  encoder.layer4.2.bn1.bias: 512\n",
            "  encoder.layer4.2.conv2.weight: 2,359,296\n",
            "  encoder.layer4.2.bn2.weight: 512\n",
            "  encoder.layer4.2.bn2.bias: 512\n",
            "  encoder.layer4.2.conv3.weight: 1,048,576\n",
            "  encoder.layer4.2.bn3.weight: 2,048\n",
            "  encoder.layer4.2.bn3.bias: 2,048\n",
            "  projection_head.mlp.0.weight: 1,048,576\n",
            "  projection_head.mlp.0.bias: 512\n",
            "  projection_head.mlp.2.weight: 262,144\n",
            "  projection_head.mlp.2.bias: 512\n",
            "  projection_head.mlp.4.weight: 131,072\n",
            "  projection_head.mlp.4.bias: 256\n",
            "  projection_head.mlp.6.weight: 65,536\n",
            "  projection_head.mlp.6.bias: 256\n",
            "  projection_head.mlp.9.weight: 8,192\n",
            "  projection_head.mlp.9.bias: 32\n",
            "  classifier.classifier.0.weight: 524,288\n",
            "  classifier.classifier.0.bias: 256\n",
            "  classifier.classifier.2.weight: 65,536\n",
            "  classifier.classifier.2.bias: 256\n",
            "  classifier.classifier.4.weight: 32,768\n",
            "  classifier.classifier.4.bias: 128\n",
            "  classifier.classifier.6.weight: 16,384\n",
            "  classifier.classifier.6.bias: 128\n",
            "  classifier.classifier.8.weight: 8,192\n",
            "  classifier.classifier.8.bias: 64\n",
            "  classifier.classifier.10.weight: 2,048\n",
            "  classifier.classifier.10.bias: 32\n",
            "  classifier.classifier.12.weight: 512\n",
            "  classifier.classifier.12.bias: 16\n",
            "  classifier.classifier.14.weight: 80\n",
            "  classifier.classifier.14.bias: 5\n",
            "================================================================================\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Created pretrain DataLoader:\n",
            "  - Batch size: 64\n",
            "  - Shuffle: True\n",
            "  - Workers: 4\n",
            "  - Augmentation pool: random_cropping, random_brightness, random_contrast, random_noise\n",
            "  - Number of samples: 4556\n",
            "  - Returns: (augmented_image1, augmented_image2) pairs\n",
            "2025-11-01 11:16:30,029 - contrastive_pretraining - INFO - Pre-training with 4556 unlabeled images\n",
            "Epoch 1/100:   0% 0/72 [00:05<?, ?it/s, Loss=4.8759]2025-11-01 11:16:35,093 - contrastive_pretraining - INFO - Epoch 0 - contrastive_loss/batch: 4.8759\n",
            "2025-11-01 11:16:35,094 - contrastive_pretraining - INFO - Epoch 0 - learning_rate: 0.0003\n",
            "Epoch 1/100:  69% 50/72 [01:04<00:26,  1.20s/it, Loss=2.7528]2025-11-01 11:17:34,229 - contrastive_pretraining - INFO - Epoch 50 - contrastive_loss/batch: 2.7528\n",
            "2025-11-01 11:17:34,230 - contrastive_pretraining - INFO - Epoch 50 - learning_rate: 0.0003\n",
            "Epoch 1/100: 100% 72/72 [01:29<00:00,  1.24s/it, Loss=0.9983]\n",
            "2025-11-01 11:17:59,096 - contrastive_pretraining - INFO - Epoch 1 - Average Loss: 3.4293\n",
            "2025-11-01 11:17:59,096 - contrastive_pretraining - INFO - Epoch 0 - contrastive_loss/epoch: 3.4293\n",
            "Epoch 2/100:   0% 0/72 [00:02<?, ?it/s, Loss=2.6469]2025-11-01 11:18:01,710 - contrastive_pretraining - INFO - Epoch 72 - contrastive_loss/batch: 2.6469\n",
            "2025-11-01 11:18:01,711 - contrastive_pretraining - INFO - Epoch 72 - learning_rate: 0.0003\n",
            "Epoch 2/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=2.3229]2025-11-01 11:19:05,209 - contrastive_pretraining - INFO - Epoch 122 - contrastive_loss/batch: 2.3229\n",
            "2025-11-01 11:19:05,210 - contrastive_pretraining - INFO - Epoch 122 - learning_rate: 0.0003\n",
            "Epoch 2/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=1.1192]\n",
            "2025-11-01 11:19:30,665 - contrastive_pretraining - INFO - Epoch 2 - Average Loss: 2.2927\n",
            "2025-11-01 11:19:30,665 - contrastive_pretraining - INFO - Epoch 1 - contrastive_loss/epoch: 2.2927\n",
            "Epoch 3/100:   0% 0/72 [00:02<?, ?it/s, Loss=2.2466]2025-11-01 11:19:32,803 - contrastive_pretraining - INFO - Epoch 144 - contrastive_loss/batch: 2.2466\n",
            "2025-11-01 11:19:32,804 - contrastive_pretraining - INFO - Epoch 144 - learning_rate: 0.0003\n",
            "Epoch 3/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=1.8545]2025-11-01 11:20:36,073 - contrastive_pretraining - INFO - Epoch 194 - contrastive_loss/batch: 1.8545\n",
            "2025-11-01 11:20:36,073 - contrastive_pretraining - INFO - Epoch 194 - learning_rate: 0.0003\n",
            "Epoch 3/100: 100% 72/72 [01:30<00:00,  1.26s/it, Loss=0.5191]\n",
            "2025-11-01 11:21:01,616 - contrastive_pretraining - INFO - Epoch 3 - Average Loss: 1.9465\n",
            "2025-11-01 11:21:01,616 - contrastive_pretraining - INFO - Epoch 2 - contrastive_loss/epoch: 1.9465\n",
            "Epoch 4/100:   0% 0/72 [00:03<?, ?it/s, Loss=1.8960]2025-11-01 11:21:05,339 - contrastive_pretraining - INFO - Epoch 216 - contrastive_loss/batch: 1.8960\n",
            "2025-11-01 11:21:05,339 - contrastive_pretraining - INFO - Epoch 216 - learning_rate: 0.0003\n",
            "Epoch 4/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=1.5782]2025-11-01 11:22:08,510 - contrastive_pretraining - INFO - Epoch 266 - contrastive_loss/batch: 1.5782\n",
            "2025-11-01 11:22:08,511 - contrastive_pretraining - INFO - Epoch 266 - learning_rate: 0.0003\n",
            "Epoch 4/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.7231]\n",
            "2025-11-01 11:22:34,004 - contrastive_pretraining - INFO - Epoch 4 - Average Loss: 1.7190\n",
            "2025-11-01 11:22:34,004 - contrastive_pretraining - INFO - Epoch 3 - contrastive_loss/epoch: 1.7190\n",
            "Epoch 5/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.6800]2025-11-01 11:22:36,570 - contrastive_pretraining - INFO - Epoch 288 - contrastive_loss/batch: 1.6800\n",
            "2025-11-01 11:22:36,571 - contrastive_pretraining - INFO - Epoch 288 - learning_rate: 0.0003\n",
            "Epoch 5/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=1.4827]2025-11-01 11:23:39,737 - contrastive_pretraining - INFO - Epoch 338 - contrastive_loss/batch: 1.4827\n",
            "2025-11-01 11:23:39,738 - contrastive_pretraining - INFO - Epoch 338 - learning_rate: 0.0003\n",
            "Epoch 5/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0850]\n",
            "2025-11-01 11:24:05,360 - contrastive_pretraining - INFO - Epoch 5 - Average Loss: 1.4070\n",
            "2025-11-01 11:24:05,360 - contrastive_pretraining - INFO - Epoch 4 - contrastive_loss/epoch: 1.4070\n",
            "Epoch 6/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.2559]2025-11-01 11:24:08,025 - contrastive_pretraining - INFO - Epoch 360 - contrastive_loss/batch: 1.2559\n",
            "2025-11-01 11:24:08,026 - contrastive_pretraining - INFO - Epoch 360 - learning_rate: 0.0003\n",
            "Epoch 6/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=1.3675]2025-11-01 11:25:11,139 - contrastive_pretraining - INFO - Epoch 410 - contrastive_loss/batch: 1.3675\n",
            "2025-11-01 11:25:11,139 - contrastive_pretraining - INFO - Epoch 410 - learning_rate: 0.0003\n",
            "Epoch 6/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.2236]\n",
            "2025-11-01 11:25:36,747 - contrastive_pretraining - INFO - Epoch 6 - Average Loss: 1.2241\n",
            "2025-11-01 11:25:36,747 - contrastive_pretraining - INFO - Epoch 5 - contrastive_loss/epoch: 1.2241\n",
            "Epoch 7/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.9773]2025-11-01 11:25:40,285 - contrastive_pretraining - INFO - Epoch 432 - contrastive_loss/batch: 0.9773\n",
            "2025-11-01 11:25:40,286 - contrastive_pretraining - INFO - Epoch 432 - learning_rate: 0.0003\n",
            "Epoch 7/100:  69% 50/72 [01:06<00:27,  1.27s/it, Loss=1.0620]2025-11-01 11:26:43,487 - contrastive_pretraining - INFO - Epoch 482 - contrastive_loss/batch: 1.0620\n",
            "2025-11-01 11:26:43,488 - contrastive_pretraining - INFO - Epoch 482 - learning_rate: 0.0003\n",
            "Epoch 7/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.1023]\n",
            "2025-11-01 11:27:09,110 - contrastive_pretraining - INFO - Epoch 7 - Average Loss: 1.1182\n",
            "2025-11-01 11:27:09,110 - contrastive_pretraining - INFO - Epoch 6 - contrastive_loss/epoch: 1.1182\n",
            "Epoch 8/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.0432]2025-11-01 11:27:11,525 - contrastive_pretraining - INFO - Epoch 504 - contrastive_loss/batch: 1.0432\n",
            "2025-11-01 11:27:11,525 - contrastive_pretraining - INFO - Epoch 504 - learning_rate: 0.0003\n",
            "Epoch 8/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.9795]2025-11-01 11:28:14,602 - contrastive_pretraining - INFO - Epoch 554 - contrastive_loss/batch: 0.9795\n",
            "2025-11-01 11:28:14,603 - contrastive_pretraining - INFO - Epoch 554 - learning_rate: 0.0003\n",
            "Epoch 8/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.1851]\n",
            "2025-11-01 11:28:40,146 - contrastive_pretraining - INFO - Epoch 8 - Average Loss: 1.0562\n",
            "2025-11-01 11:28:40,146 - contrastive_pretraining - INFO - Epoch 7 - contrastive_loss/epoch: 1.0562\n",
            "Epoch 9/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.9542]2025-11-01 11:28:42,890 - contrastive_pretraining - INFO - Epoch 576 - contrastive_loss/batch: 0.9542\n",
            "2025-11-01 11:28:42,890 - contrastive_pretraining - INFO - Epoch 576 - learning_rate: 0.0003\n",
            "Epoch 9/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=1.0634]2025-11-01 11:29:45,982 - contrastive_pretraining - INFO - Epoch 626 - contrastive_loss/batch: 1.0634\n",
            "2025-11-01 11:29:45,983 - contrastive_pretraining - INFO - Epoch 626 - learning_rate: 0.0003\n",
            "Epoch 9/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1552]\n",
            "2025-11-01 11:30:11,612 - contrastive_pretraining - INFO - Epoch 9 - Average Loss: 0.9744\n",
            "2025-11-01 11:30:11,612 - contrastive_pretraining - INFO - Epoch 8 - contrastive_loss/epoch: 0.9744\n",
            "Epoch 10/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8749]2025-11-01 11:30:14,066 - contrastive_pretraining - INFO - Epoch 648 - contrastive_loss/batch: 0.8749\n",
            "2025-11-01 11:30:14,068 - contrastive_pretraining - INFO - Epoch 648 - learning_rate: 0.0003\n",
            "Epoch 10/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=1.0146]2025-11-01 11:31:17,238 - contrastive_pretraining - INFO - Epoch 698 - contrastive_loss/batch: 1.0146\n",
            "2025-11-01 11:31:17,239 - contrastive_pretraining - INFO - Epoch 698 - learning_rate: 0.0003\n",
            "Epoch 10/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0901]\n",
            "2025-11-01 11:31:42,919 - contrastive_pretraining - INFO - Epoch 10 - Average Loss: 0.9382\n",
            "2025-11-01 11:31:42,920 - contrastive_pretraining - INFO - Epoch 9 - contrastive_loss/epoch: 0.9382\n",
            "Epoch 11/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.8464]2025-11-01 11:31:46,134 - contrastive_pretraining - INFO - Epoch 720 - contrastive_loss/batch: 0.8464\n",
            "2025-11-01 11:31:46,135 - contrastive_pretraining - INFO - Epoch 720 - learning_rate: 0.0003\n",
            "Epoch 11/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.7360]2025-11-01 11:32:49,357 - contrastive_pretraining - INFO - Epoch 770 - contrastive_loss/batch: 0.7360\n",
            "2025-11-01 11:32:49,357 - contrastive_pretraining - INFO - Epoch 770 - learning_rate: 0.0003\n",
            "Epoch 11/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.1892]\n",
            "2025-11-01 11:33:15,030 - contrastive_pretraining - INFO - Epoch 11 - Average Loss: 0.8958\n",
            "2025-11-01 11:33:15,030 - contrastive_pretraining - INFO - Epoch 10 - contrastive_loss/epoch: 0.8958\n",
            "Epoch 12/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.9244]2025-11-01 11:33:17,974 - contrastive_pretraining - INFO - Epoch 792 - contrastive_loss/batch: 0.9244\n",
            "2025-11-01 11:33:17,976 - contrastive_pretraining - INFO - Epoch 792 - learning_rate: 0.0003\n",
            "Epoch 12/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.7674]2025-11-01 11:34:20,993 - contrastive_pretraining - INFO - Epoch 842 - contrastive_loss/batch: 0.7674\n",
            "2025-11-01 11:34:20,993 - contrastive_pretraining - INFO - Epoch 842 - learning_rate: 0.0003\n",
            "Epoch 12/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1229]\n",
            "2025-11-01 11:34:46,538 - contrastive_pretraining - INFO - Epoch 12 - Average Loss: 0.8462\n",
            "2025-11-01 11:34:46,538 - contrastive_pretraining - INFO - Epoch 11 - contrastive_loss/epoch: 0.8462\n",
            "Epoch 13/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.7006]2025-11-01 11:34:49,305 - contrastive_pretraining - INFO - Epoch 864 - contrastive_loss/batch: 0.7006\n",
            "2025-11-01 11:34:49,306 - contrastive_pretraining - INFO - Epoch 864 - learning_rate: 0.0003\n",
            "Epoch 13/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.7509]2025-11-01 11:35:52,300 - contrastive_pretraining - INFO - Epoch 914 - contrastive_loss/batch: 0.7509\n",
            "2025-11-01 11:35:52,301 - contrastive_pretraining - INFO - Epoch 914 - learning_rate: 0.0003\n",
            "Epoch 13/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0686]\n",
            "2025-11-01 11:36:17,923 - contrastive_pretraining - INFO - Epoch 13 - Average Loss: 0.8017\n",
            "2025-11-01 11:36:17,923 - contrastive_pretraining - INFO - Epoch 12 - contrastive_loss/epoch: 0.8017\n",
            "Epoch 14/100:   0% 0/72 [00:03<?, ?it/s, Loss=1.0266]2025-11-01 11:36:21,019 - contrastive_pretraining - INFO - Epoch 936 - contrastive_loss/batch: 1.0266\n",
            "2025-11-01 11:36:21,020 - contrastive_pretraining - INFO - Epoch 936 - learning_rate: 0.0003\n",
            "Epoch 14/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.6996]2025-11-01 11:37:24,162 - contrastive_pretraining - INFO - Epoch 986 - contrastive_loss/batch: 0.6996\n",
            "2025-11-01 11:37:24,163 - contrastive_pretraining - INFO - Epoch 986 - learning_rate: 0.0003\n",
            "Epoch 14/100: 100% 72/72 [01:31<00:00,  1.28s/it, Loss=0.0529]\n",
            "2025-11-01 11:37:49,738 - contrastive_pretraining - INFO - Epoch 14 - Average Loss: 0.8088\n",
            "2025-11-01 11:37:49,738 - contrastive_pretraining - INFO - Epoch 13 - contrastive_loss/epoch: 0.8088\n",
            "Epoch 15/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8036]2025-11-01 11:37:52,719 - contrastive_pretraining - INFO - Epoch 1008 - contrastive_loss/batch: 0.8036\n",
            "2025-11-01 11:37:52,726 - contrastive_pretraining - INFO - Epoch 1008 - learning_rate: 0.0003\n",
            "Epoch 15/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=1.0537]2025-11-01 11:38:55,798 - contrastive_pretraining - INFO - Epoch 1058 - contrastive_loss/batch: 1.0537\n",
            "2025-11-01 11:38:55,799 - contrastive_pretraining - INFO - Epoch 1058 - learning_rate: 0.0003\n",
            "Epoch 15/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0950]\n",
            "2025-11-01 11:39:21,420 - contrastive_pretraining - INFO - Epoch 15 - Average Loss: 0.7832\n",
            "2025-11-01 11:39:21,420 - contrastive_pretraining - INFO - Epoch 14 - contrastive_loss/epoch: 0.7832\n",
            "Epoch 16/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8051]2025-11-01 11:39:23,954 - contrastive_pretraining - INFO - Epoch 1080 - contrastive_loss/batch: 0.8051\n",
            "2025-11-01 11:39:23,955 - contrastive_pretraining - INFO - Epoch 1080 - learning_rate: 0.0003\n",
            "Epoch 16/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.7346]2025-11-01 11:40:27,153 - contrastive_pretraining - INFO - Epoch 1130 - contrastive_loss/batch: 0.7346\n",
            "2025-11-01 11:40:27,153 - contrastive_pretraining - INFO - Epoch 1130 - learning_rate: 0.0003\n",
            "Epoch 16/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1559]\n",
            "2025-11-01 11:40:52,749 - contrastive_pretraining - INFO - Epoch 16 - Average Loss: 0.7491\n",
            "2025-11-01 11:40:52,750 - contrastive_pretraining - INFO - Epoch 15 - contrastive_loss/epoch: 0.7491\n",
            "Epoch 17/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.7605]2025-11-01 11:40:55,271 - contrastive_pretraining - INFO - Epoch 1152 - contrastive_loss/batch: 0.7605\n",
            "2025-11-01 11:40:55,272 - contrastive_pretraining - INFO - Epoch 1152 - learning_rate: 0.0003\n",
            "Epoch 17/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.8499]2025-11-01 11:41:58,306 - contrastive_pretraining - INFO - Epoch 1202 - contrastive_loss/batch: 0.8499\n",
            "2025-11-01 11:41:58,307 - contrastive_pretraining - INFO - Epoch 1202 - learning_rate: 0.0003\n",
            "Epoch 17/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1262]\n",
            "2025-11-01 11:42:23,908 - contrastive_pretraining - INFO - Epoch 17 - Average Loss: 0.7200\n",
            "2025-11-01 11:42:23,908 - contrastive_pretraining - INFO - Epoch 16 - contrastive_loss/epoch: 0.7200\n",
            "Epoch 18/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.8294]2025-11-01 11:42:27,285 - contrastive_pretraining - INFO - Epoch 1224 - contrastive_loss/batch: 0.8294\n",
            "2025-11-01 11:42:27,292 - contrastive_pretraining - INFO - Epoch 1224 - learning_rate: 0.0003\n",
            "Epoch 18/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.8108]2025-11-01 11:43:30,404 - contrastive_pretraining - INFO - Epoch 1274 - contrastive_loss/batch: 0.8108\n",
            "2025-11-01 11:43:30,404 - contrastive_pretraining - INFO - Epoch 1274 - learning_rate: 0.0003\n",
            "Epoch 18/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.2023]\n",
            "2025-11-01 11:43:55,938 - contrastive_pretraining - INFO - Epoch 18 - Average Loss: 0.7096\n",
            "2025-11-01 11:43:55,939 - contrastive_pretraining - INFO - Epoch 17 - contrastive_loss/epoch: 0.7096\n",
            "Epoch 19/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.7225]2025-11-01 11:43:58,603 - contrastive_pretraining - INFO - Epoch 1296 - contrastive_loss/batch: 0.7225\n",
            "2025-11-01 11:43:58,604 - contrastive_pretraining - INFO - Epoch 1296 - learning_rate: 0.0003\n",
            "Epoch 19/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5744]2025-11-01 11:45:01,628 - contrastive_pretraining - INFO - Epoch 1346 - contrastive_loss/batch: 0.5744\n",
            "2025-11-01 11:45:01,629 - contrastive_pretraining - INFO - Epoch 1346 - learning_rate: 0.0003\n",
            "Epoch 19/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0626]\n",
            "2025-11-01 11:45:27,273 - contrastive_pretraining - INFO - Epoch 19 - Average Loss: 0.6814\n",
            "2025-11-01 11:45:27,274 - contrastive_pretraining - INFO - Epoch 18 - contrastive_loss/epoch: 0.6814\n",
            "Epoch 20/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6848]2025-11-01 11:45:30,022 - contrastive_pretraining - INFO - Epoch 1368 - contrastive_loss/batch: 0.6848\n",
            "2025-11-01 11:45:30,023 - contrastive_pretraining - INFO - Epoch 1368 - learning_rate: 0.0003\n",
            "Epoch 20/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.7073]2025-11-01 11:46:33,046 - contrastive_pretraining - INFO - Epoch 1418 - contrastive_loss/batch: 0.7073\n",
            "2025-11-01 11:46:33,047 - contrastive_pretraining - INFO - Epoch 1418 - learning_rate: 0.0003\n",
            "Epoch 20/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0809]\n",
            "2025-11-01 11:46:58,631 - contrastive_pretraining - INFO - Epoch 20 - Average Loss: 0.6599\n",
            "2025-11-01 11:46:58,632 - contrastive_pretraining - INFO - Epoch 19 - contrastive_loss/epoch: 0.6599\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 11:47:02,774 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_20.pth\n",
            "Epoch 21/100:   0% 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 21/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.7421]2025-11-01 11:47:05,321 - contrastive_pretraining - INFO - Epoch 1440 - contrastive_loss/batch: 0.7421\n",
            "2025-11-01 11:47:05,324 - contrastive_pretraining - INFO - Epoch 1440 - learning_rate: 0.0003\n",
            "Epoch 21/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5758]2025-11-01 11:48:08,531 - contrastive_pretraining - INFO - Epoch 1490 - contrastive_loss/batch: 0.5758\n",
            "2025-11-01 11:48:08,531 - contrastive_pretraining - INFO - Epoch 1490 - learning_rate: 0.0003\n",
            "Epoch 21/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0960]\n",
            "2025-11-01 11:48:34,062 - contrastive_pretraining - INFO - Epoch 21 - Average Loss: 0.6425\n",
            "2025-11-01 11:48:34,062 - contrastive_pretraining - INFO - Epoch 20 - contrastive_loss/epoch: 0.6425\n",
            "Epoch 22/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.5732]2025-11-01 11:48:37,208 - contrastive_pretraining - INFO - Epoch 1512 - contrastive_loss/batch: 0.5732\n",
            "2025-11-01 11:48:37,208 - contrastive_pretraining - INFO - Epoch 1512 - learning_rate: 0.0003\n",
            "Epoch 22/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.5753]2025-11-01 11:49:40,360 - contrastive_pretraining - INFO - Epoch 1562 - contrastive_loss/batch: 0.5753\n",
            "2025-11-01 11:49:40,361 - contrastive_pretraining - INFO - Epoch 1562 - learning_rate: 0.0003\n",
            "Epoch 22/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0923]\n",
            "2025-11-01 11:50:05,858 - contrastive_pretraining - INFO - Epoch 22 - Average Loss: 0.6202\n",
            "2025-11-01 11:50:05,858 - contrastive_pretraining - INFO - Epoch 21 - contrastive_loss/epoch: 0.6202\n",
            "Epoch 23/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6165]2025-11-01 11:50:08,515 - contrastive_pretraining - INFO - Epoch 1584 - contrastive_loss/batch: 0.6165\n",
            "2025-11-01 11:50:08,515 - contrastive_pretraining - INFO - Epoch 1584 - learning_rate: 0.0003\n",
            "Epoch 23/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=0.7899]2025-11-01 11:51:11,650 - contrastive_pretraining - INFO - Epoch 1634 - contrastive_loss/batch: 0.7899\n",
            "2025-11-01 11:51:11,650 - contrastive_pretraining - INFO - Epoch 1634 - learning_rate: 0.0003\n",
            "Epoch 23/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0649]\n",
            "2025-11-01 11:51:37,215 - contrastive_pretraining - INFO - Epoch 23 - Average Loss: 0.6597\n",
            "2025-11-01 11:51:37,215 - contrastive_pretraining - INFO - Epoch 22 - contrastive_loss/epoch: 0.6597\n",
            "Epoch 24/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4953]2025-11-01 11:51:39,874 - contrastive_pretraining - INFO - Epoch 1656 - contrastive_loss/batch: 0.4953\n",
            "2025-11-01 11:51:39,874 - contrastive_pretraining - INFO - Epoch 1656 - learning_rate: 0.0003\n",
            "Epoch 24/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5951]2025-11-01 11:52:42,938 - contrastive_pretraining - INFO - Epoch 1706 - contrastive_loss/batch: 0.5951\n",
            "2025-11-01 11:52:42,938 - contrastive_pretraining - INFO - Epoch 1706 - learning_rate: 0.0003\n",
            "Epoch 24/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1997]\n",
            "2025-11-01 11:53:08,563 - contrastive_pretraining - INFO - Epoch 24 - Average Loss: 0.5931\n",
            "2025-11-01 11:53:08,563 - contrastive_pretraining - INFO - Epoch 23 - contrastive_loss/epoch: 0.5931\n",
            "Epoch 25/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6151]2025-11-01 11:53:11,130 - contrastive_pretraining - INFO - Epoch 1728 - contrastive_loss/batch: 0.6151\n",
            "2025-11-01 11:53:11,131 - contrastive_pretraining - INFO - Epoch 1728 - learning_rate: 0.0003\n",
            "Epoch 25/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.7167]2025-11-01 11:54:14,201 - contrastive_pretraining - INFO - Epoch 1778 - contrastive_loss/batch: 0.7167\n",
            "2025-11-01 11:54:14,201 - contrastive_pretraining - INFO - Epoch 1778 - learning_rate: 0.0003\n",
            "Epoch 25/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0575]\n",
            "2025-11-01 11:54:39,756 - contrastive_pretraining - INFO - Epoch 25 - Average Loss: 0.6170\n",
            "2025-11-01 11:54:39,756 - contrastive_pretraining - INFO - Epoch 24 - contrastive_loss/epoch: 0.6170\n",
            "Epoch 26/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4969]2025-11-01 11:54:43,023 - contrastive_pretraining - INFO - Epoch 1800 - contrastive_loss/batch: 0.4969\n",
            "2025-11-01 11:54:43,025 - contrastive_pretraining - INFO - Epoch 1800 - learning_rate: 0.0003\n",
            "Epoch 26/100:  69% 50/72 [01:06<00:27,  1.27s/it, Loss=0.7760]2025-11-01 11:55:46,158 - contrastive_pretraining - INFO - Epoch 1850 - contrastive_loss/batch: 0.7760\n",
            "2025-11-01 11:55:46,159 - contrastive_pretraining - INFO - Epoch 1850 - learning_rate: 0.0003\n",
            "Epoch 26/100: 100% 72/72 [01:31<00:00,  1.28s/it, Loss=0.1573]\n",
            "2025-11-01 11:56:11,756 - contrastive_pretraining - INFO - Epoch 26 - Average Loss: 0.6070\n",
            "2025-11-01 11:56:11,756 - contrastive_pretraining - INFO - Epoch 25 - contrastive_loss/epoch: 0.6070\n",
            "Epoch 27/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6523]2025-11-01 11:56:14,265 - contrastive_pretraining - INFO - Epoch 1872 - contrastive_loss/batch: 0.6523\n",
            "2025-11-01 11:56:14,266 - contrastive_pretraining - INFO - Epoch 1872 - learning_rate: 0.0003\n",
            "Epoch 27/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.6154]2025-11-01 11:57:17,329 - contrastive_pretraining - INFO - Epoch 1922 - contrastive_loss/batch: 0.6154\n",
            "2025-11-01 11:57:17,330 - contrastive_pretraining - INFO - Epoch 1922 - learning_rate: 0.0003\n",
            "Epoch 27/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.2372]\n",
            "2025-11-01 11:57:42,893 - contrastive_pretraining - INFO - Epoch 27 - Average Loss: 0.5767\n",
            "2025-11-01 11:57:42,893 - contrastive_pretraining - INFO - Epoch 26 - contrastive_loss/epoch: 0.5767\n",
            "Epoch 28/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5719]2025-11-01 11:57:45,575 - contrastive_pretraining - INFO - Epoch 1944 - contrastive_loss/batch: 0.5719\n",
            "2025-11-01 11:57:45,577 - contrastive_pretraining - INFO - Epoch 1944 - learning_rate: 0.0003\n",
            "Epoch 28/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5007]2025-11-01 11:58:48,668 - contrastive_pretraining - INFO - Epoch 1994 - contrastive_loss/batch: 0.5007\n",
            "2025-11-01 11:58:48,669 - contrastive_pretraining - INFO - Epoch 1994 - learning_rate: 0.0003\n",
            "Epoch 28/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1070]\n",
            "2025-11-01 11:59:14,281 - contrastive_pretraining - INFO - Epoch 28 - Average Loss: 0.6107\n",
            "2025-11-01 11:59:14,281 - contrastive_pretraining - INFO - Epoch 27 - contrastive_loss/epoch: 0.6107\n",
            "Epoch 29/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.5425]2025-11-01 11:59:17,366 - contrastive_pretraining - INFO - Epoch 2016 - contrastive_loss/batch: 0.5425\n",
            "2025-11-01 11:59:17,366 - contrastive_pretraining - INFO - Epoch 2016 - learning_rate: 0.0003\n",
            "Epoch 29/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.6476]2025-11-01 12:00:20,479 - contrastive_pretraining - INFO - Epoch 2066 - contrastive_loss/batch: 0.6476\n",
            "2025-11-01 12:00:20,480 - contrastive_pretraining - INFO - Epoch 2066 - learning_rate: 0.0003\n",
            "Epoch 29/100: 100% 72/72 [01:31<00:00,  1.28s/it, Loss=0.0156]\n",
            "2025-11-01 12:00:46,093 - contrastive_pretraining - INFO - Epoch 29 - Average Loss: 0.5853\n",
            "2025-11-01 12:00:46,094 - contrastive_pretraining - INFO - Epoch 28 - contrastive_loss/epoch: 0.5853\n",
            "Epoch 30/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6125]2025-11-01 12:00:48,354 - contrastive_pretraining - INFO - Epoch 2088 - contrastive_loss/batch: 0.6125\n",
            "2025-11-01 12:00:48,356 - contrastive_pretraining - INFO - Epoch 2088 - learning_rate: 0.0003\n",
            "Epoch 30/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4719]2025-11-01 12:01:51,517 - contrastive_pretraining - INFO - Epoch 2138 - contrastive_loss/batch: 0.4719\n",
            "2025-11-01 12:01:51,517 - contrastive_pretraining - INFO - Epoch 2138 - learning_rate: 0.0003\n",
            "Epoch 30/100: 100% 72/72 [01:30<00:00,  1.26s/it, Loss=0.0965]\n",
            "2025-11-01 12:02:17,020 - contrastive_pretraining - INFO - Epoch 30 - Average Loss: 0.5489\n",
            "2025-11-01 12:02:17,020 - contrastive_pretraining - INFO - Epoch 29 - contrastive_loss/epoch: 0.5489\n",
            "Epoch 31/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4750]2025-11-01 12:02:19,483 - contrastive_pretraining - INFO - Epoch 2160 - contrastive_loss/batch: 0.4750\n",
            "2025-11-01 12:02:19,486 - contrastive_pretraining - INFO - Epoch 2160 - learning_rate: 0.0003\n",
            "Epoch 31/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4883]2025-11-01 12:03:22,483 - contrastive_pretraining - INFO - Epoch 2210 - contrastive_loss/batch: 0.4883\n",
            "2025-11-01 12:03:22,484 - contrastive_pretraining - INFO - Epoch 2210 - learning_rate: 0.0003\n",
            "Epoch 31/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.0307]\n",
            "2025-11-01 12:03:48,076 - contrastive_pretraining - INFO - Epoch 31 - Average Loss: 0.5365\n",
            "2025-11-01 12:03:48,076 - contrastive_pretraining - INFO - Epoch 30 - contrastive_loss/epoch: 0.5365\n",
            "Epoch 32/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.6604]2025-11-01 12:03:51,651 - contrastive_pretraining - INFO - Epoch 2232 - contrastive_loss/batch: 0.6604\n",
            "2025-11-01 12:03:51,655 - contrastive_pretraining - INFO - Epoch 2232 - learning_rate: 0.0003\n",
            "Epoch 32/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.7501]2025-11-01 12:04:54,780 - contrastive_pretraining - INFO - Epoch 2282 - contrastive_loss/batch: 0.7501\n",
            "2025-11-01 12:04:54,781 - contrastive_pretraining - INFO - Epoch 2282 - learning_rate: 0.0003\n",
            "Epoch 32/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.1176]\n",
            "2025-11-01 12:05:20,391 - contrastive_pretraining - INFO - Epoch 32 - Average Loss: 0.5383\n",
            "2025-11-01 12:05:20,391 - contrastive_pretraining - INFO - Epoch 31 - contrastive_loss/epoch: 0.5383\n",
            "Epoch 33/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5224]2025-11-01 12:05:22,957 - contrastive_pretraining - INFO - Epoch 2304 - contrastive_loss/batch: 0.5224\n",
            "2025-11-01 12:05:22,958 - contrastive_pretraining - INFO - Epoch 2304 - learning_rate: 0.0003\n",
            "Epoch 33/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4958]2025-11-01 12:06:26,079 - contrastive_pretraining - INFO - Epoch 2354 - contrastive_loss/batch: 0.4958\n",
            "2025-11-01 12:06:26,080 - contrastive_pretraining - INFO - Epoch 2354 - learning_rate: 0.0003\n",
            "Epoch 33/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1097]\n",
            "2025-11-01 12:06:51,659 - contrastive_pretraining - INFO - Epoch 33 - Average Loss: 0.5283\n",
            "2025-11-01 12:06:51,660 - contrastive_pretraining - INFO - Epoch 32 - contrastive_loss/epoch: 0.5283\n",
            "Epoch 34/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5223]2025-11-01 12:06:54,217 - contrastive_pretraining - INFO - Epoch 2376 - contrastive_loss/batch: 0.5223\n",
            "2025-11-01 12:06:54,219 - contrastive_pretraining - INFO - Epoch 2376 - learning_rate: 0.0003\n",
            "Epoch 34/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5119]2025-11-01 12:07:57,252 - contrastive_pretraining - INFO - Epoch 2426 - contrastive_loss/batch: 0.5119\n",
            "2025-11-01 12:07:57,253 - contrastive_pretraining - INFO - Epoch 2426 - learning_rate: 0.0003\n",
            "Epoch 34/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.2076]\n",
            "2025-11-01 12:08:22,908 - contrastive_pretraining - INFO - Epoch 34 - Average Loss: 0.5573\n",
            "2025-11-01 12:08:22,909 - contrastive_pretraining - INFO - Epoch 33 - contrastive_loss/epoch: 0.5573\n",
            "Epoch 35/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.5561]2025-11-01 12:08:26,201 - contrastive_pretraining - INFO - Epoch 2448 - contrastive_loss/batch: 0.5561\n",
            "2025-11-01 12:08:26,202 - contrastive_pretraining - INFO - Epoch 2448 - learning_rate: 0.0003\n",
            "Epoch 35/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.5350]2025-11-01 12:09:29,403 - contrastive_pretraining - INFO - Epoch 2498 - contrastive_loss/batch: 0.5350\n",
            "2025-11-01 12:09:29,404 - contrastive_pretraining - INFO - Epoch 2498 - learning_rate: 0.0003\n",
            "Epoch 35/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0134]\n",
            "2025-11-01 12:09:55,037 - contrastive_pretraining - INFO - Epoch 35 - Average Loss: 0.5240\n",
            "2025-11-01 12:09:55,037 - contrastive_pretraining - INFO - Epoch 34 - contrastive_loss/epoch: 0.5240\n",
            "Epoch 36/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5468]2025-11-01 12:09:57,743 - contrastive_pretraining - INFO - Epoch 2520 - contrastive_loss/batch: 0.5468\n",
            "2025-11-01 12:09:57,743 - contrastive_pretraining - INFO - Epoch 2520 - learning_rate: 0.0003\n",
            "Epoch 36/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.6832]2025-11-01 12:11:00,786 - contrastive_pretraining - INFO - Epoch 2570 - contrastive_loss/batch: 0.6832\n",
            "2025-11-01 12:11:00,786 - contrastive_pretraining - INFO - Epoch 2570 - learning_rate: 0.0003\n",
            "Epoch 36/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0524]\n",
            "2025-11-01 12:11:26,313 - contrastive_pretraining - INFO - Epoch 36 - Average Loss: 0.5322\n",
            "2025-11-01 12:11:26,313 - contrastive_pretraining - INFO - Epoch 35 - contrastive_loss/epoch: 0.5322\n",
            "Epoch 37/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5174]2025-11-01 12:11:28,748 - contrastive_pretraining - INFO - Epoch 2592 - contrastive_loss/batch: 0.5174\n",
            "2025-11-01 12:11:28,750 - contrastive_pretraining - INFO - Epoch 2592 - learning_rate: 0.0003\n",
            "Epoch 37/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5333]2025-11-01 12:12:31,734 - contrastive_pretraining - INFO - Epoch 2642 - contrastive_loss/batch: 0.5333\n",
            "2025-11-01 12:12:31,734 - contrastive_pretraining - INFO - Epoch 2642 - learning_rate: 0.0003\n",
            "Epoch 37/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.0478]\n",
            "2025-11-01 12:12:57,322 - contrastive_pretraining - INFO - Epoch 37 - Average Loss: 0.5312\n",
            "2025-11-01 12:12:57,322 - contrastive_pretraining - INFO - Epoch 36 - contrastive_loss/epoch: 0.5312\n",
            "Epoch 38/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5160]2025-11-01 12:12:59,776 - contrastive_pretraining - INFO - Epoch 2664 - contrastive_loss/batch: 0.5160\n",
            "2025-11-01 12:12:59,777 - contrastive_pretraining - INFO - Epoch 2664 - learning_rate: 0.0003\n",
            "Epoch 38/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.7299]2025-11-01 12:14:02,829 - contrastive_pretraining - INFO - Epoch 2714 - contrastive_loss/batch: 0.7299\n",
            "2025-11-01 12:14:02,830 - contrastive_pretraining - INFO - Epoch 2714 - learning_rate: 0.0003\n",
            "Epoch 38/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0516]\n",
            "2025-11-01 12:14:28,424 - contrastive_pretraining - INFO - Epoch 38 - Average Loss: 0.5568\n",
            "2025-11-01 12:14:28,424 - contrastive_pretraining - INFO - Epoch 37 - contrastive_loss/epoch: 0.5568\n",
            "Epoch 39/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4262]2025-11-01 12:14:30,904 - contrastive_pretraining - INFO - Epoch 2736 - contrastive_loss/batch: 0.4262\n",
            "2025-11-01 12:14:30,906 - contrastive_pretraining - INFO - Epoch 2736 - learning_rate: 0.0003\n",
            "Epoch 39/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5013]2025-11-01 12:15:33,952 - contrastive_pretraining - INFO - Epoch 2786 - contrastive_loss/batch: 0.5013\n",
            "2025-11-01 12:15:33,953 - contrastive_pretraining - INFO - Epoch 2786 - learning_rate: 0.0003\n",
            "Epoch 39/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.0802]\n",
            "2025-11-01 12:15:59,499 - contrastive_pretraining - INFO - Epoch 39 - Average Loss: 0.5107\n",
            "2025-11-01 12:15:59,500 - contrastive_pretraining - INFO - Epoch 38 - contrastive_loss/epoch: 0.5107\n",
            "Epoch 40/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5229]2025-11-01 12:16:02,196 - contrastive_pretraining - INFO - Epoch 2808 - contrastive_loss/batch: 0.5229\n",
            "2025-11-01 12:16:02,197 - contrastive_pretraining - INFO - Epoch 2808 - learning_rate: 0.0003\n",
            "Epoch 40/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4433]2025-11-01 12:17:05,274 - contrastive_pretraining - INFO - Epoch 2858 - contrastive_loss/batch: 0.4433\n",
            "2025-11-01 12:17:05,275 - contrastive_pretraining - INFO - Epoch 2858 - learning_rate: 0.0003\n",
            "Epoch 40/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.3146]\n",
            "2025-11-01 12:17:30,965 - contrastive_pretraining - INFO - Epoch 40 - Average Loss: 0.4968\n",
            "2025-11-01 12:17:30,965 - contrastive_pretraining - INFO - Epoch 39 - contrastive_loss/epoch: 0.4968\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 12:17:36,887 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_40.pth\n",
            "Epoch 41/100:   0% 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 41/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4785]2025-11-01 12:17:39,343 - contrastive_pretraining - INFO - Epoch 2880 - contrastive_loss/batch: 0.4785\n",
            "2025-11-01 12:17:39,345 - contrastive_pretraining - INFO - Epoch 2880 - learning_rate: 0.0003\n",
            "Epoch 41/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4597]2025-11-01 12:18:42,614 - contrastive_pretraining - INFO - Epoch 2930 - contrastive_loss/batch: 0.4597\n",
            "2025-11-01 12:18:42,615 - contrastive_pretraining - INFO - Epoch 2930 - learning_rate: 0.0003\n",
            "Epoch 41/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0281]\n",
            "2025-11-01 12:19:08,086 - contrastive_pretraining - INFO - Epoch 41 - Average Loss: 0.5353\n",
            "2025-11-01 12:19:08,087 - contrastive_pretraining - INFO - Epoch 40 - contrastive_loss/epoch: 0.5353\n",
            "Epoch 42/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5928]2025-11-01 12:19:10,804 - contrastive_pretraining - INFO - Epoch 2952 - contrastive_loss/batch: 0.5928\n",
            "2025-11-01 12:19:10,805 - contrastive_pretraining - INFO - Epoch 2952 - learning_rate: 0.0003\n",
            "Epoch 42/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.6716]2025-11-01 12:20:13,964 - contrastive_pretraining - INFO - Epoch 3002 - contrastive_loss/batch: 0.6716\n",
            "2025-11-01 12:20:13,965 - contrastive_pretraining - INFO - Epoch 3002 - learning_rate: 0.0003\n",
            "Epoch 42/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0303]\n",
            "2025-11-01 12:20:39,484 - contrastive_pretraining - INFO - Epoch 42 - Average Loss: 0.5163\n",
            "2025-11-01 12:20:39,485 - contrastive_pretraining - INFO - Epoch 41 - contrastive_loss/epoch: 0.5163\n",
            "Epoch 43/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5831]2025-11-01 12:20:41,558 - contrastive_pretraining - INFO - Epoch 3024 - contrastive_loss/batch: 0.5831\n",
            "2025-11-01 12:20:41,566 - contrastive_pretraining - INFO - Epoch 3024 - learning_rate: 0.0003\n",
            "Epoch 43/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=0.4837]2025-11-01 12:21:44,616 - contrastive_pretraining - INFO - Epoch 3074 - contrastive_loss/batch: 0.4837\n",
            "2025-11-01 12:21:44,617 - contrastive_pretraining - INFO - Epoch 3074 - learning_rate: 0.0003\n",
            "Epoch 43/100: 100% 72/72 [01:30<00:00,  1.26s/it, Loss=0.0655]\n",
            "2025-11-01 12:22:10,240 - contrastive_pretraining - INFO - Epoch 43 - Average Loss: 0.4945\n",
            "2025-11-01 12:22:10,240 - contrastive_pretraining - INFO - Epoch 42 - contrastive_loss/epoch: 0.4945\n",
            "Epoch 44/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4051]2025-11-01 12:22:12,873 - contrastive_pretraining - INFO - Epoch 3096 - contrastive_loss/batch: 0.4051\n",
            "2025-11-01 12:22:12,874 - contrastive_pretraining - INFO - Epoch 3096 - learning_rate: 0.0003\n",
            "Epoch 44/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4108]2025-11-01 12:23:15,968 - contrastive_pretraining - INFO - Epoch 3146 - contrastive_loss/batch: 0.4108\n",
            "2025-11-01 12:23:15,969 - contrastive_pretraining - INFO - Epoch 3146 - learning_rate: 0.0003\n",
            "Epoch 44/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0237]\n",
            "2025-11-01 12:23:41,544 - contrastive_pretraining - INFO - Epoch 44 - Average Loss: 0.4869\n",
            "2025-11-01 12:23:41,544 - contrastive_pretraining - INFO - Epoch 43 - contrastive_loss/epoch: 0.4869\n",
            "Epoch 45/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4513]2025-11-01 12:23:44,856 - contrastive_pretraining - INFO - Epoch 3168 - contrastive_loss/batch: 0.4513\n",
            "2025-11-01 12:23:44,857 - contrastive_pretraining - INFO - Epoch 3168 - learning_rate: 0.0003\n",
            "Epoch 45/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.4500]2025-11-01 12:24:47,952 - contrastive_pretraining - INFO - Epoch 3218 - contrastive_loss/batch: 0.4500\n",
            "2025-11-01 12:24:47,952 - contrastive_pretraining - INFO - Epoch 3218 - learning_rate: 0.0003\n",
            "Epoch 45/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0657]\n",
            "2025-11-01 12:25:13,588 - contrastive_pretraining - INFO - Epoch 45 - Average Loss: 0.4821\n",
            "2025-11-01 12:25:13,589 - contrastive_pretraining - INFO - Epoch 44 - contrastive_loss/epoch: 0.4821\n",
            "Epoch 46/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5986]2025-11-01 12:25:16,245 - contrastive_pretraining - INFO - Epoch 3240 - contrastive_loss/batch: 0.5986\n",
            "2025-11-01 12:25:16,246 - contrastive_pretraining - INFO - Epoch 3240 - learning_rate: 0.0003\n",
            "Epoch 46/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4919]2025-11-01 12:26:19,299 - contrastive_pretraining - INFO - Epoch 3290 - contrastive_loss/batch: 0.4919\n",
            "2025-11-01 12:26:19,300 - contrastive_pretraining - INFO - Epoch 3290 - learning_rate: 0.0003\n",
            "Epoch 46/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0317]\n",
            "2025-11-01 12:26:44,876 - contrastive_pretraining - INFO - Epoch 46 - Average Loss: 0.4977\n",
            "2025-11-01 12:26:44,876 - contrastive_pretraining - INFO - Epoch 45 - contrastive_loss/epoch: 0.4977\n",
            "Epoch 47/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5516]2025-11-01 12:26:47,111 - contrastive_pretraining - INFO - Epoch 3312 - contrastive_loss/batch: 0.5516\n",
            "2025-11-01 12:26:47,112 - contrastive_pretraining - INFO - Epoch 3312 - learning_rate: 0.0003\n",
            "Epoch 47/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5159]2025-11-01 12:27:50,167 - contrastive_pretraining - INFO - Epoch 3362 - contrastive_loss/batch: 0.5159\n",
            "2025-11-01 12:27:50,167 - contrastive_pretraining - INFO - Epoch 3362 - learning_rate: 0.0003\n",
            "Epoch 47/100: 100% 72/72 [01:30<00:00,  1.26s/it, Loss=0.1375]\n",
            "2025-11-01 12:28:15,754 - contrastive_pretraining - INFO - Epoch 47 - Average Loss: 0.4889\n",
            "2025-11-01 12:28:15,754 - contrastive_pretraining - INFO - Epoch 46 - contrastive_loss/epoch: 0.4889\n",
            "Epoch 48/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4948]2025-11-01 12:28:19,127 - contrastive_pretraining - INFO - Epoch 3384 - contrastive_loss/batch: 0.4948\n",
            "2025-11-01 12:28:19,129 - contrastive_pretraining - INFO - Epoch 3384 - learning_rate: 0.0003\n",
            "Epoch 48/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.3534]2025-11-01 12:29:22,287 - contrastive_pretraining - INFO - Epoch 3434 - contrastive_loss/batch: 0.3534\n",
            "2025-11-01 12:29:22,288 - contrastive_pretraining - INFO - Epoch 3434 - learning_rate: 0.0003\n",
            "Epoch 48/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0126]\n",
            "2025-11-01 12:29:47,881 - contrastive_pretraining - INFO - Epoch 48 - Average Loss: 0.4850\n",
            "2025-11-01 12:29:47,881 - contrastive_pretraining - INFO - Epoch 47 - contrastive_loss/epoch: 0.4850\n",
            "Epoch 49/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4331]2025-11-01 12:29:50,311 - contrastive_pretraining - INFO - Epoch 3456 - contrastive_loss/batch: 0.4331\n",
            "2025-11-01 12:29:50,313 - contrastive_pretraining - INFO - Epoch 3456 - learning_rate: 0.0003\n",
            "Epoch 49/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=0.4952]2025-11-01 12:30:53,349 - contrastive_pretraining - INFO - Epoch 3506 - contrastive_loss/batch: 0.4952\n",
            "2025-11-01 12:30:53,349 - contrastive_pretraining - INFO - Epoch 3506 - learning_rate: 0.0003\n",
            "Epoch 49/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.0645]\n",
            "2025-11-01 12:31:18,907 - contrastive_pretraining - INFO - Epoch 49 - Average Loss: 0.4614\n",
            "2025-11-01 12:31:18,907 - contrastive_pretraining - INFO - Epoch 48 - contrastive_loss/epoch: 0.4614\n",
            "Epoch 50/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5782]2025-11-01 12:31:21,537 - contrastive_pretraining - INFO - Epoch 3528 - contrastive_loss/batch: 0.5782\n",
            "2025-11-01 12:31:21,538 - contrastive_pretraining - INFO - Epoch 3528 - learning_rate: 0.0003\n",
            "Epoch 50/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.3712]2025-11-01 12:32:24,663 - contrastive_pretraining - INFO - Epoch 3578 - contrastive_loss/batch: 0.3712\n",
            "2025-11-01 12:32:24,663 - contrastive_pretraining - INFO - Epoch 3578 - learning_rate: 0.0003\n",
            "Epoch 50/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0262]\n",
            "2025-11-01 12:32:50,275 - contrastive_pretraining - INFO - Epoch 50 - Average Loss: 0.4801\n",
            "2025-11-01 12:32:50,275 - contrastive_pretraining - INFO - Epoch 49 - contrastive_loss/epoch: 0.4801\n",
            "Epoch 51/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4828]2025-11-01 12:32:53,599 - contrastive_pretraining - INFO - Epoch 3600 - contrastive_loss/batch: 0.4828\n",
            "2025-11-01 12:32:53,606 - contrastive_pretraining - INFO - Epoch 3600 - learning_rate: 0.0003\n",
            "Epoch 51/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.4611]2025-11-01 12:33:56,670 - contrastive_pretraining - INFO - Epoch 3650 - contrastive_loss/batch: 0.4611\n",
            "2025-11-01 12:33:56,670 - contrastive_pretraining - INFO - Epoch 3650 - learning_rate: 0.0003\n",
            "Epoch 51/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0313]\n",
            "2025-11-01 12:34:22,287 - contrastive_pretraining - INFO - Epoch 51 - Average Loss: 0.4672\n",
            "2025-11-01 12:34:22,287 - contrastive_pretraining - INFO - Epoch 50 - contrastive_loss/epoch: 0.4672\n",
            "Epoch 52/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4986]2025-11-01 12:34:24,939 - contrastive_pretraining - INFO - Epoch 3672 - contrastive_loss/batch: 0.4986\n",
            "2025-11-01 12:34:24,940 - contrastive_pretraining - INFO - Epoch 3672 - learning_rate: 0.0003\n",
            "Epoch 52/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4482]2025-11-01 12:35:28,131 - contrastive_pretraining - INFO - Epoch 3722 - contrastive_loss/batch: 0.4482\n",
            "2025-11-01 12:35:28,131 - contrastive_pretraining - INFO - Epoch 3722 - learning_rate: 0.0003\n",
            "Epoch 52/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0290]\n",
            "2025-11-01 12:35:53,751 - contrastive_pretraining - INFO - Epoch 52 - Average Loss: 0.4698\n",
            "2025-11-01 12:35:53,751 - contrastive_pretraining - INFO - Epoch 51 - contrastive_loss/epoch: 0.4698\n",
            "Epoch 53/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4551]2025-11-01 12:35:56,484 - contrastive_pretraining - INFO - Epoch 3744 - contrastive_loss/batch: 0.4551\n",
            "2025-11-01 12:35:56,484 - contrastive_pretraining - INFO - Epoch 3744 - learning_rate: 0.0003\n",
            "Epoch 53/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4181]2025-11-01 12:36:59,418 - contrastive_pretraining - INFO - Epoch 3794 - contrastive_loss/batch: 0.4181\n",
            "2025-11-01 12:36:59,419 - contrastive_pretraining - INFO - Epoch 3794 - learning_rate: 0.0003\n",
            "Epoch 53/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0175]\n",
            "2025-11-01 12:37:24,971 - contrastive_pretraining - INFO - Epoch 53 - Average Loss: 0.4521\n",
            "2025-11-01 12:37:24,971 - contrastive_pretraining - INFO - Epoch 52 - contrastive_loss/epoch: 0.4521\n",
            "Epoch 54/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.3779]2025-11-01 12:37:28,406 - contrastive_pretraining - INFO - Epoch 3816 - contrastive_loss/batch: 0.3779\n",
            "2025-11-01 12:37:28,406 - contrastive_pretraining - INFO - Epoch 3816 - learning_rate: 0.0003\n",
            "Epoch 54/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.3595]2025-11-01 12:38:31,526 - contrastive_pretraining - INFO - Epoch 3866 - contrastive_loss/batch: 0.3595\n",
            "2025-11-01 12:38:31,527 - contrastive_pretraining - INFO - Epoch 3866 - learning_rate: 0.0003\n",
            "Epoch 54/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.2256]\n",
            "2025-11-01 12:38:57,132 - contrastive_pretraining - INFO - Epoch 54 - Average Loss: 0.4705\n",
            "2025-11-01 12:38:57,132 - contrastive_pretraining - INFO - Epoch 53 - contrastive_loss/epoch: 0.4705\n",
            "Epoch 55/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4618]2025-11-01 12:38:59,659 - contrastive_pretraining - INFO - Epoch 3888 - contrastive_loss/batch: 0.4618\n",
            "2025-11-01 12:38:59,661 - contrastive_pretraining - INFO - Epoch 3888 - learning_rate: 0.0003\n",
            "Epoch 55/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4335]2025-11-01 12:40:02,643 - contrastive_pretraining - INFO - Epoch 3938 - contrastive_loss/batch: 0.4335\n",
            "2025-11-01 12:40:02,643 - contrastive_pretraining - INFO - Epoch 3938 - learning_rate: 0.0003\n",
            "Epoch 55/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.0865]\n",
            "2025-11-01 12:40:28,165 - contrastive_pretraining - INFO - Epoch 55 - Average Loss: 0.4386\n",
            "2025-11-01 12:40:28,166 - contrastive_pretraining - INFO - Epoch 54 - contrastive_loss/epoch: 0.4386\n",
            "Epoch 56/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5128]2025-11-01 12:40:30,578 - contrastive_pretraining - INFO - Epoch 3960 - contrastive_loss/batch: 0.5128\n",
            "2025-11-01 12:40:30,579 - contrastive_pretraining - INFO - Epoch 3960 - learning_rate: 0.0003\n",
            "Epoch 56/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4053]2025-11-01 12:41:33,593 - contrastive_pretraining - INFO - Epoch 4010 - contrastive_loss/batch: 0.4053\n",
            "2025-11-01 12:41:33,594 - contrastive_pretraining - INFO - Epoch 4010 - learning_rate: 0.0003\n",
            "Epoch 56/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.0965]\n",
            "2025-11-01 12:41:59,226 - contrastive_pretraining - INFO - Epoch 56 - Average Loss: 0.4630\n",
            "2025-11-01 12:41:59,228 - contrastive_pretraining - INFO - Epoch 55 - contrastive_loss/epoch: 0.4630\n",
            "Epoch 57/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5375]2025-11-01 12:42:02,101 - contrastive_pretraining - INFO - Epoch 4032 - contrastive_loss/batch: 0.5375\n",
            "2025-11-01 12:42:02,104 - contrastive_pretraining - INFO - Epoch 4032 - learning_rate: 0.0003\n",
            "Epoch 57/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.4550]2025-11-01 12:43:05,283 - contrastive_pretraining - INFO - Epoch 4082 - contrastive_loss/batch: 0.4550\n",
            "2025-11-01 12:43:05,284 - contrastive_pretraining - INFO - Epoch 4082 - learning_rate: 0.0003\n",
            "Epoch 57/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0625]\n",
            "2025-11-01 12:43:30,844 - contrastive_pretraining - INFO - Epoch 57 - Average Loss: 0.4506\n",
            "2025-11-01 12:43:30,845 - contrastive_pretraining - INFO - Epoch 56 - contrastive_loss/epoch: 0.4506\n",
            "Epoch 58/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3750]2025-11-01 12:43:33,543 - contrastive_pretraining - INFO - Epoch 4104 - contrastive_loss/batch: 0.3750\n",
            "2025-11-01 12:43:33,548 - contrastive_pretraining - INFO - Epoch 4104 - learning_rate: 0.0003\n",
            "Epoch 58/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4788]2025-11-01 12:44:36,673 - contrastive_pretraining - INFO - Epoch 4154 - contrastive_loss/batch: 0.4788\n",
            "2025-11-01 12:44:36,674 - contrastive_pretraining - INFO - Epoch 4154 - learning_rate: 0.0003\n",
            "Epoch 58/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0210]\n",
            "2025-11-01 12:45:02,233 - contrastive_pretraining - INFO - Epoch 58 - Average Loss: 0.4358\n",
            "2025-11-01 12:45:02,233 - contrastive_pretraining - INFO - Epoch 57 - contrastive_loss/epoch: 0.4358\n",
            "Epoch 59/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3818]2025-11-01 12:45:04,559 - contrastive_pretraining - INFO - Epoch 4176 - contrastive_loss/batch: 0.3818\n",
            "2025-11-01 12:45:04,559 - contrastive_pretraining - INFO - Epoch 4176 - learning_rate: 0.0003\n",
            "Epoch 59/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.3516]2025-11-01 12:46:07,566 - contrastive_pretraining - INFO - Epoch 4226 - contrastive_loss/batch: 0.3516\n",
            "2025-11-01 12:46:07,567 - contrastive_pretraining - INFO - Epoch 4226 - learning_rate: 0.0003\n",
            "Epoch 59/100: 100% 72/72 [01:30<00:00,  1.26s/it, Loss=0.0569]\n",
            "2025-11-01 12:46:33,167 - contrastive_pretraining - INFO - Epoch 59 - Average Loss: 0.4337\n",
            "2025-11-01 12:46:33,168 - contrastive_pretraining - INFO - Epoch 58 - contrastive_loss/epoch: 0.4337\n",
            "Epoch 60/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3867]2025-11-01 12:46:35,759 - contrastive_pretraining - INFO - Epoch 4248 - contrastive_loss/batch: 0.3867\n",
            "2025-11-01 12:46:35,759 - contrastive_pretraining - INFO - Epoch 4248 - learning_rate: 0.0003\n",
            "Epoch 60/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.3647]2025-11-01 12:47:38,945 - contrastive_pretraining - INFO - Epoch 4298 - contrastive_loss/batch: 0.3647\n",
            "2025-11-01 12:47:38,945 - contrastive_pretraining - INFO - Epoch 4298 - learning_rate: 0.0003\n",
            "Epoch 60/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0402]\n",
            "2025-11-01 12:48:04,620 - contrastive_pretraining - INFO - Epoch 60 - Average Loss: 0.4309\n",
            "2025-11-01 12:48:04,621 - contrastive_pretraining - INFO - Epoch 59 - contrastive_loss/epoch: 0.4309\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 12:48:10,371 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_60.pth\n",
            "Epoch 61/100:   0% 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 61/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4220]2025-11-01 12:48:13,026 - contrastive_pretraining - INFO - Epoch 4320 - contrastive_loss/batch: 0.4220\n",
            "2025-11-01 12:48:13,027 - contrastive_pretraining - INFO - Epoch 4320 - learning_rate: 0.0003\n",
            "Epoch 61/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4083]2025-11-01 12:49:16,254 - contrastive_pretraining - INFO - Epoch 4370 - contrastive_loss/batch: 0.4083\n",
            "2025-11-01 12:49:16,255 - contrastive_pretraining - INFO - Epoch 4370 - learning_rate: 0.0003\n",
            "Epoch 61/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0679]\n",
            "2025-11-01 12:49:41,749 - contrastive_pretraining - INFO - Epoch 61 - Average Loss: 0.4389\n",
            "2025-11-01 12:49:41,749 - contrastive_pretraining - INFO - Epoch 60 - contrastive_loss/epoch: 0.4389\n",
            "Epoch 62/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3980]2025-11-01 12:49:44,232 - contrastive_pretraining - INFO - Epoch 4392 - contrastive_loss/batch: 0.3980\n",
            "2025-11-01 12:49:44,235 - contrastive_pretraining - INFO - Epoch 4392 - learning_rate: 0.0003\n",
            "Epoch 62/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=0.5726]2025-11-01 12:50:47,360 - contrastive_pretraining - INFO - Epoch 4442 - contrastive_loss/batch: 0.5726\n",
            "2025-11-01 12:50:47,361 - contrastive_pretraining - INFO - Epoch 4442 - learning_rate: 0.0003\n",
            "Epoch 62/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0194]\n",
            "2025-11-01 12:51:13,012 - contrastive_pretraining - INFO - Epoch 62 - Average Loss: 0.4231\n",
            "2025-11-01 12:51:13,012 - contrastive_pretraining - INFO - Epoch 61 - contrastive_loss/epoch: 0.4231\n",
            "Epoch 63/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.3894]2025-11-01 12:51:16,096 - contrastive_pretraining - INFO - Epoch 4464 - contrastive_loss/batch: 0.3894\n",
            "2025-11-01 12:51:16,097 - contrastive_pretraining - INFO - Epoch 4464 - learning_rate: 0.0003\n",
            "Epoch 63/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.4519]2025-11-01 12:52:19,284 - contrastive_pretraining - INFO - Epoch 4514 - contrastive_loss/batch: 0.4519\n",
            "2025-11-01 12:52:19,285 - contrastive_pretraining - INFO - Epoch 4514 - learning_rate: 0.0003\n",
            "Epoch 63/100: 100% 72/72 [01:31<00:00,  1.28s/it, Loss=0.1382]\n",
            "2025-11-01 12:52:44,904 - contrastive_pretraining - INFO - Epoch 63 - Average Loss: 0.4372\n",
            "2025-11-01 12:52:44,905 - contrastive_pretraining - INFO - Epoch 62 - contrastive_loss/epoch: 0.4372\n",
            "Epoch 64/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5529]2025-11-01 12:52:47,518 - contrastive_pretraining - INFO - Epoch 4536 - contrastive_loss/batch: 0.5529\n",
            "2025-11-01 12:52:47,521 - contrastive_pretraining - INFO - Epoch 4536 - learning_rate: 0.0003\n",
            "Epoch 64/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4391]2025-11-01 12:53:50,618 - contrastive_pretraining - INFO - Epoch 4586 - contrastive_loss/batch: 0.4391\n",
            "2025-11-01 12:53:50,619 - contrastive_pretraining - INFO - Epoch 4586 - learning_rate: 0.0003\n",
            "Epoch 64/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0280]\n",
            "2025-11-01 12:54:16,222 - contrastive_pretraining - INFO - Epoch 64 - Average Loss: 0.4345\n",
            "2025-11-01 12:54:16,222 - contrastive_pretraining - INFO - Epoch 63 - contrastive_loss/epoch: 0.4345\n",
            "Epoch 65/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4921]2025-11-01 12:54:18,486 - contrastive_pretraining - INFO - Epoch 4608 - contrastive_loss/batch: 0.4921\n",
            "2025-11-01 12:54:18,486 - contrastive_pretraining - INFO - Epoch 4608 - learning_rate: 0.0003\n",
            "Epoch 65/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.3521]2025-11-01 12:55:21,589 - contrastive_pretraining - INFO - Epoch 4658 - contrastive_loss/batch: 0.3521\n",
            "2025-11-01 12:55:21,590 - contrastive_pretraining - INFO - Epoch 4658 - learning_rate: 0.0003\n",
            "Epoch 65/100: 100% 72/72 [01:30<00:00,  1.26s/it, Loss=0.0466]\n",
            "2025-11-01 12:55:47,159 - contrastive_pretraining - INFO - Epoch 65 - Average Loss: 0.4320\n",
            "2025-11-01 12:55:47,160 - contrastive_pretraining - INFO - Epoch 64 - contrastive_loss/epoch: 0.4320\n",
            "Epoch 66/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5056]2025-11-01 12:55:49,672 - contrastive_pretraining - INFO - Epoch 4680 - contrastive_loss/batch: 0.5056\n",
            "2025-11-01 12:55:49,673 - contrastive_pretraining - INFO - Epoch 4680 - learning_rate: 0.0003\n",
            "Epoch 66/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=0.4459]2025-11-01 12:56:52,821 - contrastive_pretraining - INFO - Epoch 4730 - contrastive_loss/batch: 0.4459\n",
            "2025-11-01 12:56:52,822 - contrastive_pretraining - INFO - Epoch 4730 - learning_rate: 0.0003\n",
            "Epoch 66/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1001]\n",
            "2025-11-01 12:57:18,441 - contrastive_pretraining - INFO - Epoch 66 - Average Loss: 0.4124\n",
            "2025-11-01 12:57:18,441 - contrastive_pretraining - INFO - Epoch 65 - contrastive_loss/epoch: 0.4124\n",
            "Epoch 67/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4092]2025-11-01 12:57:21,938 - contrastive_pretraining - INFO - Epoch 4752 - contrastive_loss/batch: 0.4092\n",
            "2025-11-01 12:57:21,938 - contrastive_pretraining - INFO - Epoch 4752 - learning_rate: 0.0003\n",
            "Epoch 67/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.4382]2025-11-01 12:58:25,009 - contrastive_pretraining - INFO - Epoch 4802 - contrastive_loss/batch: 0.4382\n",
            "2025-11-01 12:58:25,010 - contrastive_pretraining - INFO - Epoch 4802 - learning_rate: 0.0003\n",
            "Epoch 67/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0363]\n",
            "2025-11-01 12:58:50,586 - contrastive_pretraining - INFO - Epoch 67 - Average Loss: 0.4303\n",
            "2025-11-01 12:58:50,586 - contrastive_pretraining - INFO - Epoch 66 - contrastive_loss/epoch: 0.4303\n",
            "Epoch 68/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3690]2025-11-01 12:58:53,202 - contrastive_pretraining - INFO - Epoch 4824 - contrastive_loss/batch: 0.3690\n",
            "2025-11-01 12:58:53,204 - contrastive_pretraining - INFO - Epoch 4824 - learning_rate: 0.0003\n",
            "Epoch 68/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4332]2025-11-01 12:59:56,212 - contrastive_pretraining - INFO - Epoch 4874 - contrastive_loss/batch: 0.4332\n",
            "2025-11-01 12:59:56,213 - contrastive_pretraining - INFO - Epoch 4874 - learning_rate: 0.0003\n",
            "Epoch 68/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0309]\n",
            "2025-11-01 13:00:21,727 - contrastive_pretraining - INFO - Epoch 68 - Average Loss: 0.4343\n",
            "2025-11-01 13:00:21,728 - contrastive_pretraining - INFO - Epoch 67 - contrastive_loss/epoch: 0.4343\n",
            "Epoch 69/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3939]2025-11-01 13:00:24,180 - contrastive_pretraining - INFO - Epoch 4896 - contrastive_loss/batch: 0.3939\n",
            "2025-11-01 13:00:24,181 - contrastive_pretraining - INFO - Epoch 4896 - learning_rate: 0.0003\n",
            "Epoch 69/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5036]2025-11-01 13:01:27,242 - contrastive_pretraining - INFO - Epoch 4946 - contrastive_loss/batch: 0.5036\n",
            "2025-11-01 13:01:27,242 - contrastive_pretraining - INFO - Epoch 4946 - learning_rate: 0.0003\n",
            "Epoch 69/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.0336]\n",
            "2025-11-01 13:01:52,793 - contrastive_pretraining - INFO - Epoch 69 - Average Loss: 0.4288\n",
            "2025-11-01 13:01:52,794 - contrastive_pretraining - INFO - Epoch 68 - contrastive_loss/epoch: 0.4288\n",
            "Epoch 70/100:   0% 0/72 [00:01<?, ?it/s, Loss=0.4721]2025-11-01 13:01:54,793 - contrastive_pretraining - INFO - Epoch 4968 - contrastive_loss/batch: 0.4721\n",
            "2025-11-01 13:01:54,796 - contrastive_pretraining - INFO - Epoch 4968 - learning_rate: 0.0003\n",
            "Epoch 70/100:  69% 50/72 [01:04<00:27,  1.26s/it, Loss=0.3947]2025-11-01 13:02:57,776 - contrastive_pretraining - INFO - Epoch 5018 - contrastive_loss/batch: 0.3947\n",
            "2025-11-01 13:02:57,776 - contrastive_pretraining - INFO - Epoch 5018 - learning_rate: 0.0003\n",
            "Epoch 70/100: 100% 72/72 [01:30<00:00,  1.26s/it, Loss=0.0143]\n",
            "2025-11-01 13:03:23,308 - contrastive_pretraining - INFO - Epoch 70 - Average Loss: 0.4328\n",
            "2025-11-01 13:03:23,309 - contrastive_pretraining - INFO - Epoch 69 - contrastive_loss/epoch: 0.4328\n",
            "Epoch 71/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4494]2025-11-01 13:03:26,664 - contrastive_pretraining - INFO - Epoch 5040 - contrastive_loss/batch: 0.4494\n",
            "2025-11-01 13:03:26,669 - contrastive_pretraining - INFO - Epoch 5040 - learning_rate: 0.0003\n",
            "Epoch 71/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.3948]2025-11-01 13:04:29,785 - contrastive_pretraining - INFO - Epoch 5090 - contrastive_loss/batch: 0.3948\n",
            "2025-11-01 13:04:29,786 - contrastive_pretraining - INFO - Epoch 5090 - learning_rate: 0.0003\n",
            "Epoch 71/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0501]\n",
            "2025-11-01 13:04:55,311 - contrastive_pretraining - INFO - Epoch 71 - Average Loss: 0.4273\n",
            "2025-11-01 13:04:55,311 - contrastive_pretraining - INFO - Epoch 70 - contrastive_loss/epoch: 0.4273\n",
            "Epoch 72/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3322]2025-11-01 13:04:57,979 - contrastive_pretraining - INFO - Epoch 5112 - contrastive_loss/batch: 0.3322\n",
            "2025-11-01 13:04:57,979 - contrastive_pretraining - INFO - Epoch 5112 - learning_rate: 0.0003\n",
            "Epoch 72/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4036]2025-11-01 13:06:00,911 - contrastive_pretraining - INFO - Epoch 5162 - contrastive_loss/batch: 0.4036\n",
            "2025-11-01 13:06:00,911 - contrastive_pretraining - INFO - Epoch 5162 - learning_rate: 0.0003\n",
            "Epoch 72/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0992]\n",
            "2025-11-01 13:06:26,424 - contrastive_pretraining - INFO - Epoch 72 - Average Loss: 0.4201\n",
            "2025-11-01 13:06:26,425 - contrastive_pretraining - INFO - Epoch 71 - contrastive_loss/epoch: 0.4201\n",
            "Epoch 73/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4057]2025-11-01 13:06:28,999 - contrastive_pretraining - INFO - Epoch 5184 - contrastive_loss/batch: 0.4057\n",
            "2025-11-01 13:06:29,000 - contrastive_pretraining - INFO - Epoch 5184 - learning_rate: 0.0003\n",
            "Epoch 73/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4431]2025-11-01 13:07:31,978 - contrastive_pretraining - INFO - Epoch 5234 - contrastive_loss/batch: 0.4431\n",
            "2025-11-01 13:07:31,979 - contrastive_pretraining - INFO - Epoch 5234 - learning_rate: 0.0003\n",
            "Epoch 73/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0418]\n",
            "2025-11-01 13:07:57,634 - contrastive_pretraining - INFO - Epoch 73 - Average Loss: 0.4215\n",
            "2025-11-01 13:07:57,635 - contrastive_pretraining - INFO - Epoch 72 - contrastive_loss/epoch: 0.4215\n",
            "Epoch 74/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4234]2025-11-01 13:08:00,866 - contrastive_pretraining - INFO - Epoch 5256 - contrastive_loss/batch: 0.4234\n",
            "2025-11-01 13:08:00,867 - contrastive_pretraining - INFO - Epoch 5256 - learning_rate: 0.0003\n",
            "Epoch 74/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.4514]2025-11-01 13:09:03,906 - contrastive_pretraining - INFO - Epoch 5306 - contrastive_loss/batch: 0.4514\n",
            "2025-11-01 13:09:03,907 - contrastive_pretraining - INFO - Epoch 5306 - learning_rate: 0.0003\n",
            "Epoch 74/100: 100% 72/72 [01:31<00:00,  1.28s/it, Loss=0.0163]\n",
            "2025-11-01 13:09:29,533 - contrastive_pretraining - INFO - Epoch 74 - Average Loss: 0.4327\n",
            "2025-11-01 13:09:29,534 - contrastive_pretraining - INFO - Epoch 73 - contrastive_loss/epoch: 0.4327\n",
            "Epoch 75/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4446]2025-11-01 13:09:32,302 - contrastive_pretraining - INFO - Epoch 5328 - contrastive_loss/batch: 0.4446\n",
            "2025-11-01 13:09:32,306 - contrastive_pretraining - INFO - Epoch 5328 - learning_rate: 0.0003\n",
            "Epoch 75/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4467]2025-11-01 13:10:35,389 - contrastive_pretraining - INFO - Epoch 5378 - contrastive_loss/batch: 0.4467\n",
            "2025-11-01 13:10:35,389 - contrastive_pretraining - INFO - Epoch 5378 - learning_rate: 0.0003\n",
            "Epoch 75/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1424]\n",
            "2025-11-01 13:11:00,953 - contrastive_pretraining - INFO - Epoch 75 - Average Loss: 0.4120\n",
            "2025-11-01 13:11:00,954 - contrastive_pretraining - INFO - Epoch 74 - contrastive_loss/epoch: 0.4120\n",
            "Epoch 76/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3752]2025-11-01 13:11:03,243 - contrastive_pretraining - INFO - Epoch 5400 - contrastive_loss/batch: 0.3752\n",
            "2025-11-01 13:11:03,244 - contrastive_pretraining - INFO - Epoch 5400 - learning_rate: 0.0003\n",
            "Epoch 76/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5675]2025-11-01 13:12:06,204 - contrastive_pretraining - INFO - Epoch 5450 - contrastive_loss/batch: 0.5675\n",
            "2025-11-01 13:12:06,205 - contrastive_pretraining - INFO - Epoch 5450 - learning_rate: 0.0003\n",
            "Epoch 76/100: 100% 72/72 [01:30<00:00,  1.26s/it, Loss=0.0144]\n",
            "2025-11-01 13:12:31,755 - contrastive_pretraining - INFO - Epoch 76 - Average Loss: 0.3969\n",
            "2025-11-01 13:12:31,755 - contrastive_pretraining - INFO - Epoch 75 - contrastive_loss/epoch: 0.3969\n",
            "Epoch 77/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3438]2025-11-01 13:12:34,194 - contrastive_pretraining - INFO - Epoch 5472 - contrastive_loss/batch: 0.3438\n",
            "2025-11-01 13:12:34,197 - contrastive_pretraining - INFO - Epoch 5472 - learning_rate: 0.0003\n",
            "Epoch 77/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4886]2025-11-01 13:13:37,212 - contrastive_pretraining - INFO - Epoch 5522 - contrastive_loss/batch: 0.4886\n",
            "2025-11-01 13:13:37,213 - contrastive_pretraining - INFO - Epoch 5522 - learning_rate: 0.0003\n",
            "Epoch 77/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.0882]\n",
            "2025-11-01 13:14:02,831 - contrastive_pretraining - INFO - Epoch 77 - Average Loss: 0.4036\n",
            "2025-11-01 13:14:02,831 - contrastive_pretraining - INFO - Epoch 76 - contrastive_loss/epoch: 0.4036\n",
            "Epoch 78/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.3466]2025-11-01 13:14:05,870 - contrastive_pretraining - INFO - Epoch 5544 - contrastive_loss/batch: 0.3466\n",
            "2025-11-01 13:14:05,875 - contrastive_pretraining - INFO - Epoch 5544 - learning_rate: 0.0003\n",
            "Epoch 78/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.4282]2025-11-01 13:15:08,978 - contrastive_pretraining - INFO - Epoch 5594 - contrastive_loss/batch: 0.4282\n",
            "2025-11-01 13:15:08,978 - contrastive_pretraining - INFO - Epoch 5594 - learning_rate: 0.0003\n",
            "Epoch 78/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0157]\n",
            "2025-11-01 13:15:34,570 - contrastive_pretraining - INFO - Epoch 78 - Average Loss: 0.4040\n",
            "2025-11-01 13:15:34,570 - contrastive_pretraining - INFO - Epoch 77 - contrastive_loss/epoch: 0.4040\n",
            "Epoch 79/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4076]2025-11-01 13:15:37,290 - contrastive_pretraining - INFO - Epoch 5616 - contrastive_loss/batch: 0.4076\n",
            "2025-11-01 13:15:37,291 - contrastive_pretraining - INFO - Epoch 5616 - learning_rate: 0.0003\n",
            "Epoch 79/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=0.3467]2025-11-01 13:16:40,433 - contrastive_pretraining - INFO - Epoch 5666 - contrastive_loss/batch: 0.3467\n",
            "2025-11-01 13:16:40,433 - contrastive_pretraining - INFO - Epoch 5666 - learning_rate: 0.0003\n",
            "Epoch 79/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0420]\n",
            "2025-11-01 13:17:06,047 - contrastive_pretraining - INFO - Epoch 79 - Average Loss: 0.4066\n",
            "2025-11-01 13:17:06,047 - contrastive_pretraining - INFO - Epoch 78 - contrastive_loss/epoch: 0.4066\n",
            "Epoch 80/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3501]2025-11-01 13:17:08,497 - contrastive_pretraining - INFO - Epoch 5688 - contrastive_loss/batch: 0.3501\n",
            "2025-11-01 13:17:08,497 - contrastive_pretraining - INFO - Epoch 5688 - learning_rate: 0.0003\n",
            "Epoch 80/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.2934]2025-11-01 13:18:11,505 - contrastive_pretraining - INFO - Epoch 5738 - contrastive_loss/batch: 0.2934\n",
            "2025-11-01 13:18:11,506 - contrastive_pretraining - INFO - Epoch 5738 - learning_rate: 0.0003\n",
            "Epoch 80/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.0804]\n",
            "2025-11-01 13:18:37,063 - contrastive_pretraining - INFO - Epoch 80 - Average Loss: 0.3910\n",
            "2025-11-01 13:18:37,063 - contrastive_pretraining - INFO - Epoch 79 - contrastive_loss/epoch: 0.3910\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 13:18:43,332 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_80.pth\n",
            "Epoch 81/100:   0% 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 81/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4914]2025-11-01 13:18:45,837 - contrastive_pretraining - INFO - Epoch 5760 - contrastive_loss/batch: 0.4914\n",
            "2025-11-01 13:18:45,840 - contrastive_pretraining - INFO - Epoch 5760 - learning_rate: 0.0003\n",
            "Epoch 81/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5152]2025-11-01 13:19:49,148 - contrastive_pretraining - INFO - Epoch 5810 - contrastive_loss/batch: 0.5152\n",
            "2025-11-01 13:19:49,149 - contrastive_pretraining - INFO - Epoch 5810 - learning_rate: 0.0003\n",
            "Epoch 81/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.2436]\n",
            "2025-11-01 13:20:14,651 - contrastive_pretraining - INFO - Epoch 81 - Average Loss: 0.4099\n",
            "2025-11-01 13:20:14,651 - contrastive_pretraining - INFO - Epoch 80 - contrastive_loss/epoch: 0.4099\n",
            "Epoch 82/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4022]2025-11-01 13:20:16,990 - contrastive_pretraining - INFO - Epoch 5832 - contrastive_loss/batch: 0.4022\n",
            "2025-11-01 13:20:16,991 - contrastive_pretraining - INFO - Epoch 5832 - learning_rate: 0.0003\n",
            "Epoch 82/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4078]2025-11-01 13:21:20,151 - contrastive_pretraining - INFO - Epoch 5882 - contrastive_loss/batch: 0.4078\n",
            "2025-11-01 13:21:20,152 - contrastive_pretraining - INFO - Epoch 5882 - learning_rate: 0.0003\n",
            "Epoch 82/100: 100% 72/72 [01:31<00:00,  1.26s/it, Loss=0.0750]\n",
            "2025-11-01 13:21:45,694 - contrastive_pretraining - INFO - Epoch 82 - Average Loss: 0.3934\n",
            "2025-11-01 13:21:45,694 - contrastive_pretraining - INFO - Epoch 81 - contrastive_loss/epoch: 0.3934\n",
            "Epoch 83/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.3433]2025-11-01 13:21:48,958 - contrastive_pretraining - INFO - Epoch 5904 - contrastive_loss/batch: 0.3433\n",
            "2025-11-01 13:21:48,959 - contrastive_pretraining - INFO - Epoch 5904 - learning_rate: 0.0003\n",
            "Epoch 83/100:  69% 50/72 [01:06<00:27,  1.27s/it, Loss=0.3244]2025-11-01 13:22:52,039 - contrastive_pretraining - INFO - Epoch 5954 - contrastive_loss/batch: 0.3244\n",
            "2025-11-01 13:22:52,039 - contrastive_pretraining - INFO - Epoch 5954 - learning_rate: 0.0003\n",
            "Epoch 83/100: 100% 72/72 [01:31<00:00,  1.28s/it, Loss=0.0936]\n",
            "2025-11-01 13:23:17,591 - contrastive_pretraining - INFO - Epoch 83 - Average Loss: 0.3796\n",
            "2025-11-01 13:23:17,591 - contrastive_pretraining - INFO - Epoch 82 - contrastive_loss/epoch: 0.3796\n",
            "Epoch 84/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5235]2025-11-01 13:23:20,232 - contrastive_pretraining - INFO - Epoch 5976 - contrastive_loss/batch: 0.5235\n",
            "2025-11-01 13:23:20,232 - contrastive_pretraining - INFO - Epoch 5976 - learning_rate: 0.0003\n",
            "Epoch 84/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4250]2025-11-01 13:24:23,310 - contrastive_pretraining - INFO - Epoch 6026 - contrastive_loss/batch: 0.4250\n",
            "2025-11-01 13:24:23,310 - contrastive_pretraining - INFO - Epoch 6026 - learning_rate: 0.0003\n",
            "Epoch 84/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0521]\n",
            "2025-11-01 13:24:48,882 - contrastive_pretraining - INFO - Epoch 84 - Average Loss: 0.3776\n",
            "2025-11-01 13:24:48,882 - contrastive_pretraining - INFO - Epoch 83 - contrastive_loss/epoch: 0.3776\n",
            "Epoch 85/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4818]2025-11-01 13:24:51,417 - contrastive_pretraining - INFO - Epoch 6048 - contrastive_loss/batch: 0.4818\n",
            "2025-11-01 13:24:51,418 - contrastive_pretraining - INFO - Epoch 6048 - learning_rate: 0.0003\n",
            "Epoch 85/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=0.3676]2025-11-01 13:25:54,590 - contrastive_pretraining - INFO - Epoch 6098 - contrastive_loss/batch: 0.3676\n",
            "2025-11-01 13:25:54,590 - contrastive_pretraining - INFO - Epoch 6098 - learning_rate: 0.0003\n",
            "Epoch 85/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1269]\n",
            "2025-11-01 13:26:20,238 - contrastive_pretraining - INFO - Epoch 85 - Average Loss: 0.3940\n",
            "2025-11-01 13:26:20,238 - contrastive_pretraining - INFO - Epoch 84 - contrastive_loss/epoch: 0.3940\n",
            "Epoch 86/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.3836]2025-11-01 13:26:23,656 - contrastive_pretraining - INFO - Epoch 6120 - contrastive_loss/batch: 0.3836\n",
            "2025-11-01 13:26:23,659 - contrastive_pretraining - INFO - Epoch 6120 - learning_rate: 0.0003\n",
            "Epoch 86/100:  69% 50/72 [01:06<00:27,  1.27s/it, Loss=0.3973]2025-11-01 13:27:26,749 - contrastive_pretraining - INFO - Epoch 6170 - contrastive_loss/batch: 0.3973\n",
            "2025-11-01 13:27:26,750 - contrastive_pretraining - INFO - Epoch 6170 - learning_rate: 0.0003\n",
            "Epoch 86/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.1349]\n",
            "2025-11-01 13:27:52,327 - contrastive_pretraining - INFO - Epoch 86 - Average Loss: 0.4002\n",
            "2025-11-01 13:27:52,327 - contrastive_pretraining - INFO - Epoch 85 - contrastive_loss/epoch: 0.4002\n",
            "Epoch 87/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3181]2025-11-01 13:27:54,828 - contrastive_pretraining - INFO - Epoch 6192 - contrastive_loss/batch: 0.3181\n",
            "2025-11-01 13:27:54,829 - contrastive_pretraining - INFO - Epoch 6192 - learning_rate: 0.0003\n",
            "Epoch 87/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=0.3247]2025-11-01 13:28:57,964 - contrastive_pretraining - INFO - Epoch 6242 - contrastive_loss/batch: 0.3247\n",
            "2025-11-01 13:28:57,965 - contrastive_pretraining - INFO - Epoch 6242 - learning_rate: 0.0003\n",
            "Epoch 87/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1318]\n",
            "2025-11-01 13:29:23,540 - contrastive_pretraining - INFO - Epoch 87 - Average Loss: 0.3727\n",
            "2025-11-01 13:29:23,541 - contrastive_pretraining - INFO - Epoch 86 - contrastive_loss/epoch: 0.3727\n",
            "Epoch 88/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.2696]2025-11-01 13:29:26,054 - contrastive_pretraining - INFO - Epoch 6264 - contrastive_loss/batch: 0.2696\n",
            "2025-11-01 13:29:26,054 - contrastive_pretraining - INFO - Epoch 6264 - learning_rate: 0.0003\n",
            "Epoch 88/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4865]2025-11-01 13:30:29,135 - contrastive_pretraining - INFO - Epoch 6314 - contrastive_loss/batch: 0.4865\n",
            "2025-11-01 13:30:29,136 - contrastive_pretraining - INFO - Epoch 6314 - learning_rate: 0.0003\n",
            "Epoch 88/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0369]\n",
            "2025-11-01 13:30:54,703 - contrastive_pretraining - INFO - Epoch 88 - Average Loss: 0.3750\n",
            "2025-11-01 13:30:54,703 - contrastive_pretraining - INFO - Epoch 87 - contrastive_loss/epoch: 0.3750\n",
            "Epoch 89/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3495]2025-11-01 13:30:57,499 - contrastive_pretraining - INFO - Epoch 6336 - contrastive_loss/batch: 0.3495\n",
            "2025-11-01 13:30:57,499 - contrastive_pretraining - INFO - Epoch 6336 - learning_rate: 0.0003\n",
            "Epoch 89/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.3463]2025-11-01 13:32:00,543 - contrastive_pretraining - INFO - Epoch 6386 - contrastive_loss/batch: 0.3463\n",
            "2025-11-01 13:32:00,543 - contrastive_pretraining - INFO - Epoch 6386 - learning_rate: 0.0003\n",
            "Epoch 89/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1058]\n",
            "2025-11-01 13:32:26,171 - contrastive_pretraining - INFO - Epoch 89 - Average Loss: 0.3847\n",
            "2025-11-01 13:32:26,171 - contrastive_pretraining - INFO - Epoch 88 - contrastive_loss/epoch: 0.3847\n",
            "Epoch 90/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4030]2025-11-01 13:32:28,719 - contrastive_pretraining - INFO - Epoch 6408 - contrastive_loss/batch: 0.4030\n",
            "2025-11-01 13:32:28,721 - contrastive_pretraining - INFO - Epoch 6408 - learning_rate: 0.0003\n",
            "Epoch 90/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4760]2025-11-01 13:33:31,753 - contrastive_pretraining - INFO - Epoch 6458 - contrastive_loss/batch: 0.4760\n",
            "2025-11-01 13:33:31,753 - contrastive_pretraining - INFO - Epoch 6458 - learning_rate: 0.0003\n",
            "Epoch 90/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0329]\n",
            "2025-11-01 13:33:57,317 - contrastive_pretraining - INFO - Epoch 90 - Average Loss: 0.3843\n",
            "2025-11-01 13:33:57,317 - contrastive_pretraining - INFO - Epoch 89 - contrastive_loss/epoch: 0.3843\n",
            "Epoch 91/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3299]2025-11-01 13:33:59,835 - contrastive_pretraining - INFO - Epoch 6480 - contrastive_loss/batch: 0.3299\n",
            "2025-11-01 13:33:59,836 - contrastive_pretraining - INFO - Epoch 6480 - learning_rate: 0.0003\n",
            "Epoch 91/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4341]2025-11-01 13:35:02,855 - contrastive_pretraining - INFO - Epoch 6530 - contrastive_loss/batch: 0.4341\n",
            "2025-11-01 13:35:02,856 - contrastive_pretraining - INFO - Epoch 6530 - learning_rate: 0.0003\n",
            "Epoch 91/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.1314]\n",
            "2025-11-01 13:35:28,464 - contrastive_pretraining - INFO - Epoch 91 - Average Loss: 0.3684\n",
            "2025-11-01 13:35:28,465 - contrastive_pretraining - INFO - Epoch 90 - contrastive_loss/epoch: 0.3684\n",
            "Epoch 92/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4859]2025-11-01 13:35:31,494 - contrastive_pretraining - INFO - Epoch 6552 - contrastive_loss/batch: 0.4859\n",
            "2025-11-01 13:35:31,495 - contrastive_pretraining - INFO - Epoch 6552 - learning_rate: 0.0003\n",
            "Epoch 92/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.3540]2025-11-01 13:36:34,612 - contrastive_pretraining - INFO - Epoch 6602 - contrastive_loss/batch: 0.3540\n",
            "2025-11-01 13:36:34,613 - contrastive_pretraining - INFO - Epoch 6602 - learning_rate: 0.0003\n",
            "Epoch 92/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0350]\n",
            "2025-11-01 13:37:00,220 - contrastive_pretraining - INFO - Epoch 92 - Average Loss: 0.3641\n",
            "2025-11-01 13:37:00,220 - contrastive_pretraining - INFO - Epoch 91 - contrastive_loss/epoch: 0.3641\n",
            "Epoch 93/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3632]2025-11-01 13:37:02,802 - contrastive_pretraining - INFO - Epoch 6624 - contrastive_loss/batch: 0.3632\n",
            "2025-11-01 13:37:02,802 - contrastive_pretraining - INFO - Epoch 6624 - learning_rate: 0.0003\n",
            "Epoch 93/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.4509]2025-11-01 13:38:05,860 - contrastive_pretraining - INFO - Epoch 6674 - contrastive_loss/batch: 0.4509\n",
            "2025-11-01 13:38:05,860 - contrastive_pretraining - INFO - Epoch 6674 - learning_rate: 0.0003\n",
            "Epoch 93/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0446]\n",
            "2025-11-01 13:38:31,384 - contrastive_pretraining - INFO - Epoch 93 - Average Loss: 0.3643\n",
            "2025-11-01 13:38:31,385 - contrastive_pretraining - INFO - Epoch 92 - contrastive_loss/epoch: 0.3643\n",
            "Epoch 94/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4046]2025-11-01 13:38:33,878 - contrastive_pretraining - INFO - Epoch 6696 - contrastive_loss/batch: 0.4046\n",
            "2025-11-01 13:38:33,878 - contrastive_pretraining - INFO - Epoch 6696 - learning_rate: 0.0003\n",
            "Epoch 94/100:  69% 50/72 [01:05<00:27,  1.27s/it, Loss=0.3149]2025-11-01 13:39:37,068 - contrastive_pretraining - INFO - Epoch 6746 - contrastive_loss/batch: 0.3149\n",
            "2025-11-01 13:39:37,068 - contrastive_pretraining - INFO - Epoch 6746 - learning_rate: 0.0003\n",
            "Epoch 94/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0727]\n",
            "2025-11-01 13:40:02,623 - contrastive_pretraining - INFO - Epoch 94 - Average Loss: 0.3778\n",
            "2025-11-01 13:40:02,623 - contrastive_pretraining - INFO - Epoch 93 - contrastive_loss/epoch: 0.3778\n",
            "Epoch 95/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4329]2025-11-01 13:40:06,015 - contrastive_pretraining - INFO - Epoch 6768 - contrastive_loss/batch: 0.4329\n",
            "2025-11-01 13:40:06,015 - contrastive_pretraining - INFO - Epoch 6768 - learning_rate: 0.0003\n",
            "Epoch 95/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.3831]2025-11-01 13:41:09,181 - contrastive_pretraining - INFO - Epoch 6818 - contrastive_loss/batch: 0.3831\n",
            "2025-11-01 13:41:09,182 - contrastive_pretraining - INFO - Epoch 6818 - learning_rate: 0.0003\n",
            "Epoch 95/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.1201]\n",
            "2025-11-01 13:41:34,796 - contrastive_pretraining - INFO - Epoch 95 - Average Loss: 0.3735\n",
            "2025-11-01 13:41:34,797 - contrastive_pretraining - INFO - Epoch 94 - contrastive_loss/epoch: 0.3735\n",
            "Epoch 96/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4741]2025-11-01 13:41:36,935 - contrastive_pretraining - INFO - Epoch 6840 - contrastive_loss/batch: 0.4741\n",
            "2025-11-01 13:41:36,937 - contrastive_pretraining - INFO - Epoch 6840 - learning_rate: 0.0003\n",
            "Epoch 96/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.2709]2025-11-01 13:42:39,961 - contrastive_pretraining - INFO - Epoch 6890 - contrastive_loss/batch: 0.2709\n",
            "2025-11-01 13:42:39,962 - contrastive_pretraining - INFO - Epoch 6890 - learning_rate: 0.0003\n",
            "Epoch 96/100: 100% 72/72 [01:30<00:00,  1.26s/it, Loss=0.0245]\n",
            "2025-11-01 13:43:05,503 - contrastive_pretraining - INFO - Epoch 96 - Average Loss: 0.3668\n",
            "2025-11-01 13:43:05,504 - contrastive_pretraining - INFO - Epoch 95 - contrastive_loss/epoch: 0.3668\n",
            "Epoch 97/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3173]2025-11-01 13:43:07,955 - contrastive_pretraining - INFO - Epoch 6912 - contrastive_loss/batch: 0.3173\n",
            "2025-11-01 13:43:07,956 - contrastive_pretraining - INFO - Epoch 6912 - learning_rate: 0.0003\n",
            "Epoch 97/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.2922]2025-11-01 13:44:11,063 - contrastive_pretraining - INFO - Epoch 6962 - contrastive_loss/batch: 0.2922\n",
            "2025-11-01 13:44:11,063 - contrastive_pretraining - INFO - Epoch 6962 - learning_rate: 0.0003\n",
            "Epoch 97/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0346]\n",
            "2025-11-01 13:44:36,737 - contrastive_pretraining - INFO - Epoch 97 - Average Loss: 0.3601\n",
            "2025-11-01 13:44:36,737 - contrastive_pretraining - INFO - Epoch 96 - contrastive_loss/epoch: 0.3601\n",
            "Epoch 98/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.3779]2025-11-01 13:44:40,001 - contrastive_pretraining - INFO - Epoch 6984 - contrastive_loss/batch: 0.3779\n",
            "2025-11-01 13:44:40,004 - contrastive_pretraining - INFO - Epoch 6984 - learning_rate: 0.0003\n",
            "Epoch 98/100:  69% 50/72 [01:06<00:27,  1.26s/it, Loss=0.5121]2025-11-01 13:45:43,167 - contrastive_pretraining - INFO - Epoch 7034 - contrastive_loss/batch: 0.5121\n",
            "2025-11-01 13:45:43,168 - contrastive_pretraining - INFO - Epoch 7034 - learning_rate: 0.0003\n",
            "Epoch 98/100: 100% 72/72 [01:31<00:00,  1.28s/it, Loss=0.0577]\n",
            "2025-11-01 13:46:08,730 - contrastive_pretraining - INFO - Epoch 98 - Average Loss: 0.3543\n",
            "2025-11-01 13:46:08,731 - contrastive_pretraining - INFO - Epoch 97 - contrastive_loss/epoch: 0.3543\n",
            "Epoch 99/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3396]2025-11-01 13:46:11,193 - contrastive_pretraining - INFO - Epoch 7056 - contrastive_loss/batch: 0.3396\n",
            "2025-11-01 13:46:11,193 - contrastive_pretraining - INFO - Epoch 7056 - learning_rate: 0.0003\n",
            "Epoch 99/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.5578]2025-11-01 13:47:14,291 - contrastive_pretraining - INFO - Epoch 7106 - contrastive_loss/batch: 0.5578\n",
            "2025-11-01 13:47:14,292 - contrastive_pretraining - INFO - Epoch 7106 - learning_rate: 0.0003\n",
            "Epoch 99/100: 100% 72/72 [01:31<00:00,  1.27s/it, Loss=0.0290]\n",
            "2025-11-01 13:47:39,857 - contrastive_pretraining - INFO - Epoch 99 - Average Loss: 0.3762\n",
            "2025-11-01 13:47:39,857 - contrastive_pretraining - INFO - Epoch 98 - contrastive_loss/epoch: 0.3762\n",
            "Epoch 100/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3949]2025-11-01 13:47:42,312 - contrastive_pretraining - INFO - Epoch 7128 - contrastive_loss/batch: 0.3949\n",
            "2025-11-01 13:47:42,312 - contrastive_pretraining - INFO - Epoch 7128 - learning_rate: 0.0003\n",
            "Epoch 100/100:  69% 50/72 [01:05<00:27,  1.26s/it, Loss=0.3033]2025-11-01 13:48:45,279 - contrastive_pretraining - INFO - Epoch 7178 - contrastive_loss/batch: 0.3033\n",
            "2025-11-01 13:48:45,280 - contrastive_pretraining - INFO - Epoch 7178 - learning_rate: 0.0003\n",
            "Epoch 100/100: 100% 72/72 [01:30<00:00,  1.26s/it, Loss=0.0263]\n",
            "2025-11-01 13:49:10,794 - contrastive_pretraining - INFO - Epoch 100 - Average Loss: 0.3530\n",
            "2025-11-01 13:49:10,794 - contrastive_pretraining - INFO - Epoch 99 - contrastive_loss/epoch: 0.3530\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 13:49:17,341 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_100.pth\n",
            "2025-11-01 13:49:17,865 - contrastive_pretraining - INFO - Contrastive pre-training completed! Model saved to /content/training_output/contrastive_training/models/contrastive_pretrained.pth\n",
            "Contrastive loss plot saved: plots/contrastive_loss.png\n",
            "2025-11-01 13:49:18,429 - contrastive_pretraining - INFO - === Contrastive Pre-training Finished ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the output\n",
        "# !zip -r contrastive_training_output.zip /content/contrastive_training_output"
      ],
      "metadata": {
        "id": "YXy-l8IA38En"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training Phase 2 - Supervised Learning (5-class dataset)"
      ],
      "metadata": {
        "id": "wzbTABHGQ263"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/tselane2110/SSCLNet-Implementation"
      ],
      "metadata": {
        "id": "EVnQlQej2EpC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "# !gdown --fuzzy \"https://drive.google.com/file/d/16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y/view?usp=drive_link\"\n",
        "# !unzip -q /content/Preprocessed-splitted-data.zip"
      ],
      "metadata": {
        "id": "NU8QtVAo2GYL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/SSCLNet-Implementation/train_supervised.py\""
      ],
      "metadata": {
        "id": "vYoNIzhXQT-E",
        "outputId": "1f6e44ed-c14b-4384-e37d-5bf96b557e4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-01 14:25:00.115820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762007100.135357   53053 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762007100.141381   53053 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762007100.156646   53053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762007100.156672   53053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762007100.156676   53053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762007100.156681   53053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-01 14:25:00.161200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✓ All directories created\n",
            "✓ All directories created\n",
            "✓ Random seed set to: 42\n",
            "2025-11-01 14:25:04,928 - supervised_finetuning - INFO - === Starting TWO-STAGE Supervised Fine-tuning ===\n",
            "Keys in file: odict_keys(['encoder.conv1.weight', 'encoder.bn1.weight', 'encoder.bn1.bias', 'encoder.bn1.running_mean', 'encoder.bn1.running_var', 'encoder.bn1.num_batches_tracked', 'encoder.layer1.0.conv1.weight', 'encoder.layer1.0.bn1.weight', 'encoder.layer1.0.bn1.bias', 'encoder.layer1.0.bn1.running_mean', 'encoder.layer1.0.bn1.running_var', 'encoder.layer1.0.bn1.num_batches_tracked', 'encoder.layer1.0.conv2.weight', 'encoder.layer1.0.bn2.weight', 'encoder.layer1.0.bn2.bias', 'encoder.layer1.0.bn2.running_mean', 'encoder.layer1.0.bn2.running_var', 'encoder.layer1.0.bn2.num_batches_tracked', 'encoder.layer1.0.conv3.weight', 'encoder.layer1.0.bn3.weight', 'encoder.layer1.0.bn3.bias', 'encoder.layer1.0.bn3.running_mean', 'encoder.layer1.0.bn3.running_var', 'encoder.layer1.0.bn3.num_batches_tracked', 'encoder.layer1.0.downsample.0.weight', 'encoder.layer1.0.downsample.1.weight', 'encoder.layer1.0.downsample.1.bias', 'encoder.layer1.0.downsample.1.running_mean', 'encoder.layer1.0.downsample.1.running_var', 'encoder.layer1.0.downsample.1.num_batches_tracked', 'encoder.layer1.1.conv1.weight', 'encoder.layer1.1.bn1.weight', 'encoder.layer1.1.bn1.bias', 'encoder.layer1.1.bn1.running_mean', 'encoder.layer1.1.bn1.running_var', 'encoder.layer1.1.bn1.num_batches_tracked', 'encoder.layer1.1.conv2.weight', 'encoder.layer1.1.bn2.weight', 'encoder.layer1.1.bn2.bias', 'encoder.layer1.1.bn2.running_mean', 'encoder.layer1.1.bn2.running_var', 'encoder.layer1.1.bn2.num_batches_tracked', 'encoder.layer1.1.conv3.weight', 'encoder.layer1.1.bn3.weight', 'encoder.layer1.1.bn3.bias', 'encoder.layer1.1.bn3.running_mean', 'encoder.layer1.1.bn3.running_var', 'encoder.layer1.1.bn3.num_batches_tracked', 'encoder.layer1.2.conv1.weight', 'encoder.layer1.2.bn1.weight', 'encoder.layer1.2.bn1.bias', 'encoder.layer1.2.bn1.running_mean', 'encoder.layer1.2.bn1.running_var', 'encoder.layer1.2.bn1.num_batches_tracked', 'encoder.layer1.2.conv2.weight', 'encoder.layer1.2.bn2.weight', 'encoder.layer1.2.bn2.bias', 'encoder.layer1.2.bn2.running_mean', 'encoder.layer1.2.bn2.running_var', 'encoder.layer1.2.bn2.num_batches_tracked', 'encoder.layer1.2.conv3.weight', 'encoder.layer1.2.bn3.weight', 'encoder.layer1.2.bn3.bias', 'encoder.layer1.2.bn3.running_mean', 'encoder.layer1.2.bn3.running_var', 'encoder.layer1.2.bn3.num_batches_tracked', 'encoder.layer2.0.conv1.weight', 'encoder.layer2.0.bn1.weight', 'encoder.layer2.0.bn1.bias', 'encoder.layer2.0.bn1.running_mean', 'encoder.layer2.0.bn1.running_var', 'encoder.layer2.0.bn1.num_batches_tracked', 'encoder.layer2.0.conv2.weight', 'encoder.layer2.0.bn2.weight', 'encoder.layer2.0.bn2.bias', 'encoder.layer2.0.bn2.running_mean', 'encoder.layer2.0.bn2.running_var', 'encoder.layer2.0.bn2.num_batches_tracked', 'encoder.layer2.0.conv3.weight', 'encoder.layer2.0.bn3.weight', 'encoder.layer2.0.bn3.bias', 'encoder.layer2.0.bn3.running_mean', 'encoder.layer2.0.bn3.running_var', 'encoder.layer2.0.bn3.num_batches_tracked', 'encoder.layer2.0.downsample.0.weight', 'encoder.layer2.0.downsample.1.weight', 'encoder.layer2.0.downsample.1.bias', 'encoder.layer2.0.downsample.1.running_mean', 'encoder.layer2.0.downsample.1.running_var', 'encoder.layer2.0.downsample.1.num_batches_tracked', 'encoder.layer2.1.conv1.weight', 'encoder.layer2.1.bn1.weight', 'encoder.layer2.1.bn1.bias', 'encoder.layer2.1.bn1.running_mean', 'encoder.layer2.1.bn1.running_var', 'encoder.layer2.1.bn1.num_batches_tracked', 'encoder.layer2.1.conv2.weight', 'encoder.layer2.1.bn2.weight', 'encoder.layer2.1.bn2.bias', 'encoder.layer2.1.bn2.running_mean', 'encoder.layer2.1.bn2.running_var', 'encoder.layer2.1.bn2.num_batches_tracked', 'encoder.layer2.1.conv3.weight', 'encoder.layer2.1.bn3.weight', 'encoder.layer2.1.bn3.bias', 'encoder.layer2.1.bn3.running_mean', 'encoder.layer2.1.bn3.running_var', 'encoder.layer2.1.bn3.num_batches_tracked', 'encoder.layer2.2.conv1.weight', 'encoder.layer2.2.bn1.weight', 'encoder.layer2.2.bn1.bias', 'encoder.layer2.2.bn1.running_mean', 'encoder.layer2.2.bn1.running_var', 'encoder.layer2.2.bn1.num_batches_tracked', 'encoder.layer2.2.conv2.weight', 'encoder.layer2.2.bn2.weight', 'encoder.layer2.2.bn2.bias', 'encoder.layer2.2.bn2.running_mean', 'encoder.layer2.2.bn2.running_var', 'encoder.layer2.2.bn2.num_batches_tracked', 'encoder.layer2.2.conv3.weight', 'encoder.layer2.2.bn3.weight', 'encoder.layer2.2.bn3.bias', 'encoder.layer2.2.bn3.running_mean', 'encoder.layer2.2.bn3.running_var', 'encoder.layer2.2.bn3.num_batches_tracked', 'encoder.layer2.3.conv1.weight', 'encoder.layer2.3.bn1.weight', 'encoder.layer2.3.bn1.bias', 'encoder.layer2.3.bn1.running_mean', 'encoder.layer2.3.bn1.running_var', 'encoder.layer2.3.bn1.num_batches_tracked', 'encoder.layer2.3.conv2.weight', 'encoder.layer2.3.bn2.weight', 'encoder.layer2.3.bn2.bias', 'encoder.layer2.3.bn2.running_mean', 'encoder.layer2.3.bn2.running_var', 'encoder.layer2.3.bn2.num_batches_tracked', 'encoder.layer2.3.conv3.weight', 'encoder.layer2.3.bn3.weight', 'encoder.layer2.3.bn3.bias', 'encoder.layer2.3.bn3.running_mean', 'encoder.layer2.3.bn3.running_var', 'encoder.layer2.3.bn3.num_batches_tracked', 'encoder.layer3.0.conv1.weight', 'encoder.layer3.0.bn1.weight', 'encoder.layer3.0.bn1.bias', 'encoder.layer3.0.bn1.running_mean', 'encoder.layer3.0.bn1.running_var', 'encoder.layer3.0.bn1.num_batches_tracked', 'encoder.layer3.0.conv2.weight', 'encoder.layer3.0.bn2.weight', 'encoder.layer3.0.bn2.bias', 'encoder.layer3.0.bn2.running_mean', 'encoder.layer3.0.bn2.running_var', 'encoder.layer3.0.bn2.num_batches_tracked', 'encoder.layer3.0.conv3.weight', 'encoder.layer3.0.bn3.weight', 'encoder.layer3.0.bn3.bias', 'encoder.layer3.0.bn3.running_mean', 'encoder.layer3.0.bn3.running_var', 'encoder.layer3.0.bn3.num_batches_tracked', 'encoder.layer3.0.downsample.0.weight', 'encoder.layer3.0.downsample.1.weight', 'encoder.layer3.0.downsample.1.bias', 'encoder.layer3.0.downsample.1.running_mean', 'encoder.layer3.0.downsample.1.running_var', 'encoder.layer3.0.downsample.1.num_batches_tracked', 'encoder.layer3.1.conv1.weight', 'encoder.layer3.1.bn1.weight', 'encoder.layer3.1.bn1.bias', 'encoder.layer3.1.bn1.running_mean', 'encoder.layer3.1.bn1.running_var', 'encoder.layer3.1.bn1.num_batches_tracked', 'encoder.layer3.1.conv2.weight', 'encoder.layer3.1.bn2.weight', 'encoder.layer3.1.bn2.bias', 'encoder.layer3.1.bn2.running_mean', 'encoder.layer3.1.bn2.running_var', 'encoder.layer3.1.bn2.num_batches_tracked', 'encoder.layer3.1.conv3.weight', 'encoder.layer3.1.bn3.weight', 'encoder.layer3.1.bn3.bias', 'encoder.layer3.1.bn3.running_mean', 'encoder.layer3.1.bn3.running_var', 'encoder.layer3.1.bn3.num_batches_tracked', 'encoder.layer3.2.conv1.weight', 'encoder.layer3.2.bn1.weight', 'encoder.layer3.2.bn1.bias', 'encoder.layer3.2.bn1.running_mean', 'encoder.layer3.2.bn1.running_var', 'encoder.layer3.2.bn1.num_batches_tracked', 'encoder.layer3.2.conv2.weight', 'encoder.layer3.2.bn2.weight', 'encoder.layer3.2.bn2.bias', 'encoder.layer3.2.bn2.running_mean', 'encoder.layer3.2.bn2.running_var', 'encoder.layer3.2.bn2.num_batches_tracked', 'encoder.layer3.2.conv3.weight', 'encoder.layer3.2.bn3.weight', 'encoder.layer3.2.bn3.bias', 'encoder.layer3.2.bn3.running_mean', 'encoder.layer3.2.bn3.running_var', 'encoder.layer3.2.bn3.num_batches_tracked', 'encoder.layer3.3.conv1.weight', 'encoder.layer3.3.bn1.weight', 'encoder.layer3.3.bn1.bias', 'encoder.layer3.3.bn1.running_mean', 'encoder.layer3.3.bn1.running_var', 'encoder.layer3.3.bn1.num_batches_tracked', 'encoder.layer3.3.conv2.weight', 'encoder.layer3.3.bn2.weight', 'encoder.layer3.3.bn2.bias', 'encoder.layer3.3.bn2.running_mean', 'encoder.layer3.3.bn2.running_var', 'encoder.layer3.3.bn2.num_batches_tracked', 'encoder.layer3.3.conv3.weight', 'encoder.layer3.3.bn3.weight', 'encoder.layer3.3.bn3.bias', 'encoder.layer3.3.bn3.running_mean', 'encoder.layer3.3.bn3.running_var', 'encoder.layer3.3.bn3.num_batches_tracked', 'encoder.layer3.4.conv1.weight', 'encoder.layer3.4.bn1.weight', 'encoder.layer3.4.bn1.bias', 'encoder.layer3.4.bn1.running_mean', 'encoder.layer3.4.bn1.running_var', 'encoder.layer3.4.bn1.num_batches_tracked', 'encoder.layer3.4.conv2.weight', 'encoder.layer3.4.bn2.weight', 'encoder.layer3.4.bn2.bias', 'encoder.layer3.4.bn2.running_mean', 'encoder.layer3.4.bn2.running_var', 'encoder.layer3.4.bn2.num_batches_tracked', 'encoder.layer3.4.conv3.weight', 'encoder.layer3.4.bn3.weight', 'encoder.layer3.4.bn3.bias', 'encoder.layer3.4.bn3.running_mean', 'encoder.layer3.4.bn3.running_var', 'encoder.layer3.4.bn3.num_batches_tracked', 'encoder.layer3.5.conv1.weight', 'encoder.layer3.5.bn1.weight', 'encoder.layer3.5.bn1.bias', 'encoder.layer3.5.bn1.running_mean', 'encoder.layer3.5.bn1.running_var', 'encoder.layer3.5.bn1.num_batches_tracked', 'encoder.layer3.5.conv2.weight', 'encoder.layer3.5.bn2.weight', 'encoder.layer3.5.bn2.bias', 'encoder.layer3.5.bn2.running_mean', 'encoder.layer3.5.bn2.running_var', 'encoder.layer3.5.bn2.num_batches_tracked', 'encoder.layer3.5.conv3.weight', 'encoder.layer3.5.bn3.weight', 'encoder.layer3.5.bn3.bias', 'encoder.layer3.5.bn3.running_mean', 'encoder.layer3.5.bn3.running_var', 'encoder.layer3.5.bn3.num_batches_tracked', 'encoder.layer4.0.conv1.weight', 'encoder.layer4.0.bn1.weight', 'encoder.layer4.0.bn1.bias', 'encoder.layer4.0.bn1.running_mean', 'encoder.layer4.0.bn1.running_var', 'encoder.layer4.0.bn1.num_batches_tracked', 'encoder.layer4.0.conv2.weight', 'encoder.layer4.0.bn2.weight', 'encoder.layer4.0.bn2.bias', 'encoder.layer4.0.bn2.running_mean', 'encoder.layer4.0.bn2.running_var', 'encoder.layer4.0.bn2.num_batches_tracked', 'encoder.layer4.0.conv3.weight', 'encoder.layer4.0.bn3.weight', 'encoder.layer4.0.bn3.bias', 'encoder.layer4.0.bn3.running_mean', 'encoder.layer4.0.bn3.running_var', 'encoder.layer4.0.bn3.num_batches_tracked', 'encoder.layer4.0.downsample.0.weight', 'encoder.layer4.0.downsample.1.weight', 'encoder.layer4.0.downsample.1.bias', 'encoder.layer4.0.downsample.1.running_mean', 'encoder.layer4.0.downsample.1.running_var', 'encoder.layer4.0.downsample.1.num_batches_tracked', 'encoder.layer4.1.conv1.weight', 'encoder.layer4.1.bn1.weight', 'encoder.layer4.1.bn1.bias', 'encoder.layer4.1.bn1.running_mean', 'encoder.layer4.1.bn1.running_var', 'encoder.layer4.1.bn1.num_batches_tracked', 'encoder.layer4.1.conv2.weight', 'encoder.layer4.1.bn2.weight', 'encoder.layer4.1.bn2.bias', 'encoder.layer4.1.bn2.running_mean', 'encoder.layer4.1.bn2.running_var', 'encoder.layer4.1.bn2.num_batches_tracked', 'encoder.layer4.1.conv3.weight', 'encoder.layer4.1.bn3.weight', 'encoder.layer4.1.bn3.bias', 'encoder.layer4.1.bn3.running_mean', 'encoder.layer4.1.bn3.running_var', 'encoder.layer4.1.bn3.num_batches_tracked', 'encoder.layer4.2.conv1.weight', 'encoder.layer4.2.bn1.weight', 'encoder.layer4.2.bn1.bias', 'encoder.layer4.2.bn1.running_mean', 'encoder.layer4.2.bn1.running_var', 'encoder.layer4.2.bn1.num_batches_tracked', 'encoder.layer4.2.conv2.weight', 'encoder.layer4.2.bn2.weight', 'encoder.layer4.2.bn2.bias', 'encoder.layer4.2.bn2.running_mean', 'encoder.layer4.2.bn2.running_var', 'encoder.layer4.2.bn2.num_batches_tracked', 'encoder.layer4.2.conv3.weight', 'encoder.layer4.2.bn3.weight', 'encoder.layer4.2.bn3.bias', 'encoder.layer4.2.bn3.running_mean', 'encoder.layer4.2.bn3.running_var', 'encoder.layer4.2.bn3.num_batches_tracked', 'projection_head.mlp.0.weight', 'projection_head.mlp.0.bias', 'projection_head.mlp.2.weight', 'projection_head.mlp.2.bias', 'projection_head.mlp.4.weight', 'projection_head.mlp.4.bias', 'projection_head.mlp.6.weight', 'projection_head.mlp.6.bias', 'projection_head.mlp.9.weight', 'projection_head.mlp.9.bias', 'classifier.classifier.0.weight', 'classifier.classifier.0.bias', 'classifier.classifier.2.weight', 'classifier.classifier.2.bias', 'classifier.classifier.4.weight', 'classifier.classifier.4.bias', 'classifier.classifier.6.weight', 'classifier.classifier.6.bias', 'classifier.classifier.8.weight', 'classifier.classifier.8.bias', 'classifier.classifier.10.weight', 'classifier.classifier.10.bias', 'classifier.classifier.12.weight', 'classifier.classifier.12.bias', 'classifier.classifier.14.weight', 'classifier.classifier.14.bias'])\n",
            "✓ It's a model file - loading directly\n",
            "2025-11-01 14:25:06,024 - supervised_finetuning - INFO - ✓ Loaded pre-trained weights from contrastive training!\n",
            "================================================================================\n",
            "MODEL SUMMARY\n",
            "================================================================================\n",
            "Model: SSCLNet\n",
            "Total trainable parameters: 25,669,541\n",
            "Model size: 98.12 MB\n",
            "\n",
            "Layer breakdown:\n",
            "  encoder.conv1.weight: 3,136\n",
            "  encoder.bn1.weight: 64\n",
            "  encoder.bn1.bias: 64\n",
            "  encoder.layer1.0.conv1.weight: 4,096\n",
            "  encoder.layer1.0.bn1.weight: 64\n",
            "  encoder.layer1.0.bn1.bias: 64\n",
            "  encoder.layer1.0.conv2.weight: 36,864\n",
            "  encoder.layer1.0.bn2.weight: 64\n",
            "  encoder.layer1.0.bn2.bias: 64\n",
            "  encoder.layer1.0.conv3.weight: 16,384\n",
            "  encoder.layer1.0.bn3.weight: 256\n",
            "  encoder.layer1.0.bn3.bias: 256\n",
            "  encoder.layer1.0.downsample.0.weight: 16,384\n",
            "  encoder.layer1.0.downsample.1.weight: 256\n",
            "  encoder.layer1.0.downsample.1.bias: 256\n",
            "  encoder.layer1.1.conv1.weight: 16,384\n",
            "  encoder.layer1.1.bn1.weight: 64\n",
            "  encoder.layer1.1.bn1.bias: 64\n",
            "  encoder.layer1.1.conv2.weight: 36,864\n",
            "  encoder.layer1.1.bn2.weight: 64\n",
            "  encoder.layer1.1.bn2.bias: 64\n",
            "  encoder.layer1.1.conv3.weight: 16,384\n",
            "  encoder.layer1.1.bn3.weight: 256\n",
            "  encoder.layer1.1.bn3.bias: 256\n",
            "  encoder.layer1.2.conv1.weight: 16,384\n",
            "  encoder.layer1.2.bn1.weight: 64\n",
            "  encoder.layer1.2.bn1.bias: 64\n",
            "  encoder.layer1.2.conv2.weight: 36,864\n",
            "  encoder.layer1.2.bn2.weight: 64\n",
            "  encoder.layer1.2.bn2.bias: 64\n",
            "  encoder.layer1.2.conv3.weight: 16,384\n",
            "  encoder.layer1.2.bn3.weight: 256\n",
            "  encoder.layer1.2.bn3.bias: 256\n",
            "  encoder.layer2.0.conv1.weight: 32,768\n",
            "  encoder.layer2.0.bn1.weight: 128\n",
            "  encoder.layer2.0.bn1.bias: 128\n",
            "  encoder.layer2.0.conv2.weight: 147,456\n",
            "  encoder.layer2.0.bn2.weight: 128\n",
            "  encoder.layer2.0.bn2.bias: 128\n",
            "  encoder.layer2.0.conv3.weight: 65,536\n",
            "  encoder.layer2.0.bn3.weight: 512\n",
            "  encoder.layer2.0.bn3.bias: 512\n",
            "  encoder.layer2.0.downsample.0.weight: 131,072\n",
            "  encoder.layer2.0.downsample.1.weight: 512\n",
            "  encoder.layer2.0.downsample.1.bias: 512\n",
            "  encoder.layer2.1.conv1.weight: 65,536\n",
            "  encoder.layer2.1.bn1.weight: 128\n",
            "  encoder.layer2.1.bn1.bias: 128\n",
            "  encoder.layer2.1.conv2.weight: 147,456\n",
            "  encoder.layer2.1.bn2.weight: 128\n",
            "  encoder.layer2.1.bn2.bias: 128\n",
            "  encoder.layer2.1.conv3.weight: 65,536\n",
            "  encoder.layer2.1.bn3.weight: 512\n",
            "  encoder.layer2.1.bn3.bias: 512\n",
            "  encoder.layer2.2.conv1.weight: 65,536\n",
            "  encoder.layer2.2.bn1.weight: 128\n",
            "  encoder.layer2.2.bn1.bias: 128\n",
            "  encoder.layer2.2.conv2.weight: 147,456\n",
            "  encoder.layer2.2.bn2.weight: 128\n",
            "  encoder.layer2.2.bn2.bias: 128\n",
            "  encoder.layer2.2.conv3.weight: 65,536\n",
            "  encoder.layer2.2.bn3.weight: 512\n",
            "  encoder.layer2.2.bn3.bias: 512\n",
            "  encoder.layer2.3.conv1.weight: 65,536\n",
            "  encoder.layer2.3.bn1.weight: 128\n",
            "  encoder.layer2.3.bn1.bias: 128\n",
            "  encoder.layer2.3.conv2.weight: 147,456\n",
            "  encoder.layer2.3.bn2.weight: 128\n",
            "  encoder.layer2.3.bn2.bias: 128\n",
            "  encoder.layer2.3.conv3.weight: 65,536\n",
            "  encoder.layer2.3.bn3.weight: 512\n",
            "  encoder.layer2.3.bn3.bias: 512\n",
            "  encoder.layer3.0.conv1.weight: 131,072\n",
            "  encoder.layer3.0.bn1.weight: 256\n",
            "  encoder.layer3.0.bn1.bias: 256\n",
            "  encoder.layer3.0.conv2.weight: 589,824\n",
            "  encoder.layer3.0.bn2.weight: 256\n",
            "  encoder.layer3.0.bn2.bias: 256\n",
            "  encoder.layer3.0.conv3.weight: 262,144\n",
            "  encoder.layer3.0.bn3.weight: 1,024\n",
            "  encoder.layer3.0.bn3.bias: 1,024\n",
            "  encoder.layer3.0.downsample.0.weight: 524,288\n",
            "  encoder.layer3.0.downsample.1.weight: 1,024\n",
            "  encoder.layer3.0.downsample.1.bias: 1,024\n",
            "  encoder.layer3.1.conv1.weight: 262,144\n",
            "  encoder.layer3.1.bn1.weight: 256\n",
            "  encoder.layer3.1.bn1.bias: 256\n",
            "  encoder.layer3.1.conv2.weight: 589,824\n",
            "  encoder.layer3.1.bn2.weight: 256\n",
            "  encoder.layer3.1.bn2.bias: 256\n",
            "  encoder.layer3.1.conv3.weight: 262,144\n",
            "  encoder.layer3.1.bn3.weight: 1,024\n",
            "  encoder.layer3.1.bn3.bias: 1,024\n",
            "  encoder.layer3.2.conv1.weight: 262,144\n",
            "  encoder.layer3.2.bn1.weight: 256\n",
            "  encoder.layer3.2.bn1.bias: 256\n",
            "  encoder.layer3.2.conv2.weight: 589,824\n",
            "  encoder.layer3.2.bn2.weight: 256\n",
            "  encoder.layer3.2.bn2.bias: 256\n",
            "  encoder.layer3.2.conv3.weight: 262,144\n",
            "  encoder.layer3.2.bn3.weight: 1,024\n",
            "  encoder.layer3.2.bn3.bias: 1,024\n",
            "  encoder.layer3.3.conv1.weight: 262,144\n",
            "  encoder.layer3.3.bn1.weight: 256\n",
            "  encoder.layer3.3.bn1.bias: 256\n",
            "  encoder.layer3.3.conv2.weight: 589,824\n",
            "  encoder.layer3.3.bn2.weight: 256\n",
            "  encoder.layer3.3.bn2.bias: 256\n",
            "  encoder.layer3.3.conv3.weight: 262,144\n",
            "  encoder.layer3.3.bn3.weight: 1,024\n",
            "  encoder.layer3.3.bn3.bias: 1,024\n",
            "  encoder.layer3.4.conv1.weight: 262,144\n",
            "  encoder.layer3.4.bn1.weight: 256\n",
            "  encoder.layer3.4.bn1.bias: 256\n",
            "  encoder.layer3.4.conv2.weight: 589,824\n",
            "  encoder.layer3.4.bn2.weight: 256\n",
            "  encoder.layer3.4.bn2.bias: 256\n",
            "  encoder.layer3.4.conv3.weight: 262,144\n",
            "  encoder.layer3.4.bn3.weight: 1,024\n",
            "  encoder.layer3.4.bn3.bias: 1,024\n",
            "  encoder.layer3.5.conv1.weight: 262,144\n",
            "  encoder.layer3.5.bn1.weight: 256\n",
            "  encoder.layer3.5.bn1.bias: 256\n",
            "  encoder.layer3.5.conv2.weight: 589,824\n",
            "  encoder.layer3.5.bn2.weight: 256\n",
            "  encoder.layer3.5.bn2.bias: 256\n",
            "  encoder.layer3.5.conv3.weight: 262,144\n",
            "  encoder.layer3.5.bn3.weight: 1,024\n",
            "  encoder.layer3.5.bn3.bias: 1,024\n",
            "  encoder.layer4.0.conv1.weight: 524,288\n",
            "  encoder.layer4.0.bn1.weight: 512\n",
            "  encoder.layer4.0.bn1.bias: 512\n",
            "  encoder.layer4.0.conv2.weight: 2,359,296\n",
            "  encoder.layer4.0.bn2.weight: 512\n",
            "  encoder.layer4.0.bn2.bias: 512\n",
            "  encoder.layer4.0.conv3.weight: 1,048,576\n",
            "  encoder.layer4.0.bn3.weight: 2,048\n",
            "  encoder.layer4.0.bn3.bias: 2,048\n",
            "  encoder.layer4.0.downsample.0.weight: 2,097,152\n",
            "  encoder.layer4.0.downsample.1.weight: 2,048\n",
            "  encoder.layer4.0.downsample.1.bias: 2,048\n",
            "  encoder.layer4.1.conv1.weight: 1,048,576\n",
            "  encoder.layer4.1.bn1.weight: 512\n",
            "  encoder.layer4.1.bn1.bias: 512\n",
            "  encoder.layer4.1.conv2.weight: 2,359,296\n",
            "  encoder.layer4.1.bn2.weight: 512\n",
            "  encoder.layer4.1.bn2.bias: 512\n",
            "  encoder.layer4.1.conv3.weight: 1,048,576\n",
            "  encoder.layer4.1.bn3.weight: 2,048\n",
            "  encoder.layer4.1.bn3.bias: 2,048\n",
            "  encoder.layer4.2.conv1.weight: 1,048,576\n",
            "  encoder.layer4.2.bn1.weight: 512\n",
            "  encoder.layer4.2.bn1.bias: 512\n",
            "  encoder.layer4.2.conv2.weight: 2,359,296\n",
            "  encoder.layer4.2.bn2.weight: 512\n",
            "  encoder.layer4.2.bn2.bias: 512\n",
            "  encoder.layer4.2.conv3.weight: 1,048,576\n",
            "  encoder.layer4.2.bn3.weight: 2,048\n",
            "  encoder.layer4.2.bn3.bias: 2,048\n",
            "  projection_head.mlp.0.weight: 1,048,576\n",
            "  projection_head.mlp.0.bias: 512\n",
            "  projection_head.mlp.2.weight: 262,144\n",
            "  projection_head.mlp.2.bias: 512\n",
            "  projection_head.mlp.4.weight: 131,072\n",
            "  projection_head.mlp.4.bias: 256\n",
            "  projection_head.mlp.6.weight: 65,536\n",
            "  projection_head.mlp.6.bias: 256\n",
            "  projection_head.mlp.9.weight: 8,192\n",
            "  projection_head.mlp.9.bias: 32\n",
            "  classifier.classifier.0.weight: 524,288\n",
            "  classifier.classifier.0.bias: 256\n",
            "  classifier.classifier.2.weight: 65,536\n",
            "  classifier.classifier.2.bias: 256\n",
            "  classifier.classifier.4.weight: 32,768\n",
            "  classifier.classifier.4.bias: 128\n",
            "  classifier.classifier.6.weight: 16,384\n",
            "  classifier.classifier.6.bias: 128\n",
            "  classifier.classifier.8.weight: 8,192\n",
            "  classifier.classifier.8.bias: 64\n",
            "  classifier.classifier.10.weight: 2,048\n",
            "  classifier.classifier.10.bias: 32\n",
            "  classifier.classifier.12.weight: 512\n",
            "  classifier.classifier.12.bias: 16\n",
            "  classifier.classifier.14.weight: 80\n",
            "  classifier.classifier.14.bias: 5\n",
            "================================================================================\n",
            "2025-11-01 14:25:06,086 - supervised_finetuning - INFO - ✓ Encoder frozen - only classifier will be trained\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Created train DataLoader:\n",
            "  - Batch size: 64\n",
            "  - Shuffle: True\n",
            "  - Workers: 4\n",
            "  - Classes: ['Glioblastoma', 'glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
            "  - Number of samples: 1287\n",
            "==================================================\n",
            "DATASET ANALYSIS\n",
            "==================================================\n",
            "Total samples: 1287\n",
            "Class distribution:\n",
            "  pituitary_tumor: 288 (22.4%)\n",
            "  meningioma_tumor: 330 (25.6%)\n",
            "  no_tumor: 193 (15.0%)\n",
            "  Glioblastoma: 194 (15.1%)\n",
            "  glioma_tumor: 282 (21.9%)\n",
            "2025-11-01 14:25:08,029 - supervised_finetuning - INFO - Training with 1287 labeled images\n",
            "2025-11-01 14:25:08,029 - supervised_finetuning - INFO - === STAGE 1: Extracting Label Features ===\n",
            "Extracting features:   0% 0/21 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Extracting features: 100% 21/21 [00:04<00:00,  4.97it/s]\n",
            "2025-11-01 14:25:12,270 - supervised_finetuning - INFO - ✓ Extracted 1287 label features with shape torch.Size([1287, 2048])\n",
            "2025-11-01 14:25:12,270 - supervised_finetuning - INFO - ✓ Created feature dataset for classifier training\n",
            "2025-11-01 14:25:12,270 - supervised_finetuning - INFO - === STAGE 2: Training Classifier on Label Features ===\n",
            "Classifier Epoch 1/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.6196, Acc=26.56%]2025-11-01 14:25:12,397 - supervised_finetuning - INFO - Epoch 0 - supervised_loss/batch: 1.6196\n",
            "2025-11-01 14:25:12,397 - supervised_finetuning - INFO - Epoch 0 - supervised_accuracy/batch: 26.5625\n",
            "2025-11-01 14:25:12,398 - supervised_finetuning - INFO - Epoch 0 - learning_rate: 0.0001\n",
            "Classifier Epoch 1/100:   5% 1/21 [00:00<00:02,  7.87it/s, Loss=1.5926, Acc=29.69%]2025-11-01 14:25:12,447 - supervised_finetuning - INFO - Epoch 10 - supervised_loss/batch: 1.5926\n",
            "2025-11-01 14:25:12,447 - supervised_finetuning - INFO - Epoch 10 - supervised_accuracy/batch: 29.6875\n",
            "2025-11-01 14:25:12,447 - supervised_finetuning - INFO - Epoch 10 - learning_rate: 0.0001\n",
            "Classifier Epoch 1/100:   5% 1/21 [00:00<00:02,  7.87it/s, Loss=1.6147, Acc=28.57%]2025-11-01 14:25:12,486 - supervised_finetuning - INFO - Epoch 20 - supervised_loss/batch: 1.6147\n",
            "2025-11-01 14:25:12,486 - supervised_finetuning - INFO - Epoch 20 - supervised_accuracy/batch: 28.5714\n",
            "2025-11-01 14:25:12,486 - supervised_finetuning - INFO - Epoch 20 - learning_rate: 0.0001\n",
            "Classifier Epoch 1/100: 100% 21/21 [00:00<00:00, 97.32it/s, Loss=1.6147, Acc=28.57%]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "2025-11-01 14:25:12,498 - supervised_finetuning - INFO - Epoch 1:\n",
            "2025-11-01 14:25:12,498 - supervised_finetuning - INFO -   Train Loss: 1.6226, Train Acc: 21.91%\n",
            "2025-11-01 14:25:12,499 - supervised_finetuning - INFO -   Train F1: 7.88%, Train Recall: 21.91%\n",
            "2025-11-01 14:25:12,499 - supervised_finetuning - INFO -   Train Precision: 4.80%\n",
            "2025-11-01 14:25:12,499 - supervised_finetuning - INFO - Epoch 0 - supervised_loss/epoch: 1.6226\n",
            "2025-11-01 14:25:12,499 - supervised_finetuning - INFO - Epoch 0 - supervised_accuracy/epoch: 21.9114\n",
            "2025-11-01 14:25:12,499 - supervised_finetuning - INFO - Epoch 0 - supervised_f1/epoch: 7.8764\n",
            "2025-11-01 14:25:12,499 - supervised_finetuning - INFO - Epoch 0 - supervised_recall/epoch: 21.9114\n",
            "2025-11-01 14:25:12,500 - supervised_finetuning - INFO - Epoch 0 - supervised_precision/epoch: 4.8011\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:13,097 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 21.91%\n",
            "Classifier Epoch 2/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.6280, Acc=17.19%]2025-11-01 14:25:13,103 - supervised_finetuning - INFO - Epoch 21 - supervised_loss/batch: 1.6280\n",
            "2025-11-01 14:25:13,103 - supervised_finetuning - INFO - Epoch 21 - supervised_accuracy/batch: 17.1875\n",
            "2025-11-01 14:25:13,103 - supervised_finetuning - INFO - Epoch 21 - learning_rate: 0.0001\n",
            "Classifier Epoch 2/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.5607, Acc=26.56%]2025-11-01 14:25:13,141 - supervised_finetuning - INFO - Epoch 31 - supervised_loss/batch: 1.5607\n",
            "2025-11-01 14:25:13,141 - supervised_finetuning - INFO - Epoch 31 - supervised_accuracy/batch: 26.5625\n",
            "2025-11-01 14:25:13,141 - supervised_finetuning - INFO - Epoch 31 - learning_rate: 0.0001\n",
            "Classifier Epoch 2/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.4952, Acc=42.86%]2025-11-01 14:25:13,177 - supervised_finetuning - INFO - Epoch 41 - supervised_loss/batch: 1.4952\n",
            "2025-11-01 14:25:13,177 - supervised_finetuning - INFO - Epoch 41 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:25:13,177 - supervised_finetuning - INFO - Epoch 41 - learning_rate: 0.0001\n",
            "Classifier Epoch 2/100: 100% 21/21 [00:00<00:00, 264.39it/s, Loss=1.4952, Acc=42.86%]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "2025-11-01 14:25:13,191 - supervised_finetuning - INFO - Epoch 2:\n",
            "2025-11-01 14:25:13,191 - supervised_finetuning - INFO -   Train Loss: 1.6050, Train Acc: 21.91%\n",
            "2025-11-01 14:25:13,191 - supervised_finetuning - INFO -   Train F1: 7.88%, Train Recall: 21.91%\n",
            "2025-11-01 14:25:13,191 - supervised_finetuning - INFO -   Train Precision: 4.80%\n",
            "2025-11-01 14:25:13,191 - supervised_finetuning - INFO - Epoch 1 - supervised_loss/epoch: 1.6050\n",
            "2025-11-01 14:25:13,191 - supervised_finetuning - INFO - Epoch 1 - supervised_accuracy/epoch: 21.9114\n",
            "2025-11-01 14:25:13,192 - supervised_finetuning - INFO - Epoch 1 - supervised_f1/epoch: 7.8764\n",
            "2025-11-01 14:25:13,192 - supervised_finetuning - INFO - Epoch 1 - supervised_recall/epoch: 21.9114\n",
            "2025-11-01 14:25:13,192 - supervised_finetuning - INFO - Epoch 1 - supervised_precision/epoch: 4.8011\n",
            "Classifier Epoch 3/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.5995, Acc=25.00%]2025-11-01 14:25:13,198 - supervised_finetuning - INFO - Epoch 42 - supervised_loss/batch: 1.5995\n",
            "2025-11-01 14:25:13,198 - supervised_finetuning - INFO - Epoch 42 - supervised_accuracy/batch: 25.0000\n",
            "2025-11-01 14:25:13,199 - supervised_finetuning - INFO - Epoch 42 - learning_rate: 0.0001\n",
            "Classifier Epoch 3/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.5959, Acc=25.00%]2025-11-01 14:25:13,236 - supervised_finetuning - INFO - Epoch 52 - supervised_loss/batch: 1.5959\n",
            "2025-11-01 14:25:13,237 - supervised_finetuning - INFO - Epoch 52 - supervised_accuracy/batch: 25.0000\n",
            "2025-11-01 14:25:13,237 - supervised_finetuning - INFO - Epoch 52 - learning_rate: 0.0001\n",
            "Classifier Epoch 3/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.5985, Acc=28.57%]2025-11-01 14:25:13,275 - supervised_finetuning - INFO - Epoch 62 - supervised_loss/batch: 1.5985\n",
            "2025-11-01 14:25:13,275 - supervised_finetuning - INFO - Epoch 62 - supervised_accuracy/batch: 28.5714\n",
            "2025-11-01 14:25:13,275 - supervised_finetuning - INFO - Epoch 62 - learning_rate: 0.0001\n",
            "Classifier Epoch 3/100: 100% 21/21 [00:00<00:00, 252.96it/s, Loss=1.5985, Acc=28.57%]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "2025-11-01 14:25:13,287 - supervised_finetuning - INFO - Epoch 3:\n",
            "2025-11-01 14:25:13,287 - supervised_finetuning - INFO -   Train Loss: 1.5851, Train Acc: 29.29%\n",
            "2025-11-01 14:25:13,287 - supervised_finetuning - INFO -   Train F1: 17.32%, Train Recall: 29.29%\n",
            "2025-11-01 14:25:13,287 - supervised_finetuning - INFO -   Train Precision: 15.50%\n",
            "2025-11-01 14:25:13,287 - supervised_finetuning - INFO - Epoch 2 - supervised_loss/epoch: 1.5851\n",
            "2025-11-01 14:25:13,287 - supervised_finetuning - INFO - Epoch 2 - supervised_accuracy/epoch: 29.2929\n",
            "2025-11-01 14:25:13,288 - supervised_finetuning - INFO - Epoch 2 - supervised_f1/epoch: 17.3199\n",
            "2025-11-01 14:25:13,288 - supervised_finetuning - INFO - Epoch 2 - supervised_recall/epoch: 29.2929\n",
            "2025-11-01 14:25:13,288 - supervised_finetuning - INFO - Epoch 2 - supervised_precision/epoch: 15.4951\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:14,142 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 29.29%\n",
            "Classifier Epoch 4/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.5127, Acc=39.06%]2025-11-01 14:25:14,152 - supervised_finetuning - INFO - Epoch 63 - supervised_loss/batch: 1.5127\n",
            "2025-11-01 14:25:14,152 - supervised_finetuning - INFO - Epoch 63 - supervised_accuracy/batch: 39.0625\n",
            "2025-11-01 14:25:14,152 - supervised_finetuning - INFO - Epoch 63 - learning_rate: 0.0001\n",
            "Classifier Epoch 4/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.5502, Acc=35.94%]2025-11-01 14:25:14,195 - supervised_finetuning - INFO - Epoch 73 - supervised_loss/batch: 1.5502\n",
            "2025-11-01 14:25:14,195 - supervised_finetuning - INFO - Epoch 73 - supervised_accuracy/batch: 35.9375\n",
            "2025-11-01 14:25:14,196 - supervised_finetuning - INFO - Epoch 73 - learning_rate: 0.0001\n",
            "Classifier Epoch 4/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.6286, Acc=14.29%]2025-11-01 14:25:14,231 - supervised_finetuning - INFO - Epoch 83 - supervised_loss/batch: 1.6286\n",
            "2025-11-01 14:25:14,231 - supervised_finetuning - INFO - Epoch 83 - supervised_accuracy/batch: 14.2857\n",
            "2025-11-01 14:25:14,231 - supervised_finetuning - INFO - Epoch 83 - learning_rate: 0.0001\n",
            "Classifier Epoch 4/100: 100% 21/21 [00:00<00:00, 236.91it/s, Loss=1.6286, Acc=14.29%]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "2025-11-01 14:25:14,242 - supervised_finetuning - INFO - Epoch 4:\n",
            "2025-11-01 14:25:14,242 - supervised_finetuning - INFO -   Train Loss: 1.5273, Train Acc: 36.99%\n",
            "2025-11-01 14:25:14,242 - supervised_finetuning - INFO -   Train F1: 28.38%, Train Recall: 36.99%\n",
            "2025-11-01 14:25:14,242 - supervised_finetuning - INFO -   Train Precision: 32.35%\n",
            "2025-11-01 14:25:14,242 - supervised_finetuning - INFO - Epoch 3 - supervised_loss/epoch: 1.5273\n",
            "2025-11-01 14:25:14,243 - supervised_finetuning - INFO - Epoch 3 - supervised_accuracy/epoch: 36.9852\n",
            "2025-11-01 14:25:14,243 - supervised_finetuning - INFO - Epoch 3 - supervised_f1/epoch: 28.3819\n",
            "2025-11-01 14:25:14,243 - supervised_finetuning - INFO - Epoch 3 - supervised_recall/epoch: 36.9852\n",
            "2025-11-01 14:25:14,243 - supervised_finetuning - INFO - Epoch 3 - supervised_precision/epoch: 32.3462\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:15,544 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 36.99%\n",
            "Classifier Epoch 5/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.5280, Acc=32.81%]2025-11-01 14:25:15,552 - supervised_finetuning - INFO - Epoch 84 - supervised_loss/batch: 1.5280\n",
            "2025-11-01 14:25:15,553 - supervised_finetuning - INFO - Epoch 84 - supervised_accuracy/batch: 32.8125\n",
            "2025-11-01 14:25:15,553 - supervised_finetuning - INFO - Epoch 84 - learning_rate: 0.0001\n",
            "Classifier Epoch 5/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.3803, Acc=48.44%]2025-11-01 14:25:15,607 - supervised_finetuning - INFO - Epoch 94 - supervised_loss/batch: 1.3803\n",
            "2025-11-01 14:25:15,607 - supervised_finetuning - INFO - Epoch 94 - supervised_accuracy/batch: 48.4375\n",
            "2025-11-01 14:25:15,607 - supervised_finetuning - INFO - Epoch 94 - learning_rate: 0.0001\n",
            "Classifier Epoch 5/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.1981, Acc=71.43%]2025-11-01 14:25:15,645 - supervised_finetuning - INFO - Epoch 104 - supervised_loss/batch: 1.1981\n",
            "2025-11-01 14:25:15,646 - supervised_finetuning - INFO - Epoch 104 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:15,646 - supervised_finetuning - INFO - Epoch 104 - learning_rate: 0.0001\n",
            "Classifier Epoch 5/100: 100% 21/21 [00:00<00:00, 207.63it/s, Loss=1.1981, Acc=71.43%]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "2025-11-01 14:25:15,665 - supervised_finetuning - INFO - Epoch 5:\n",
            "2025-11-01 14:25:15,665 - supervised_finetuning - INFO -   Train Loss: 1.3993, Train Acc: 43.28%\n",
            "2025-11-01 14:25:15,665 - supervised_finetuning - INFO -   Train F1: 33.78%, Train Recall: 43.28%\n",
            "2025-11-01 14:25:15,665 - supervised_finetuning - INFO -   Train Precision: 30.10%\n",
            "2025-11-01 14:25:15,665 - supervised_finetuning - INFO - Epoch 4 - supervised_loss/epoch: 1.3993\n",
            "2025-11-01 14:25:15,665 - supervised_finetuning - INFO - Epoch 4 - supervised_accuracy/epoch: 43.2789\n",
            "2025-11-01 14:25:15,666 - supervised_finetuning - INFO - Epoch 4 - supervised_f1/epoch: 33.7760\n",
            "2025-11-01 14:25:15,666 - supervised_finetuning - INFO - Epoch 4 - supervised_recall/epoch: 43.2789\n",
            "2025-11-01 14:25:15,666 - supervised_finetuning - INFO - Epoch 4 - supervised_precision/epoch: 30.0980\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:17,237 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 43.28%\n",
            "Classifier Epoch 6/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.2917, Acc=54.69%]2025-11-01 14:25:17,250 - supervised_finetuning - INFO - Epoch 105 - supervised_loss/batch: 1.2917\n",
            "2025-11-01 14:25:17,250 - supervised_finetuning - INFO - Epoch 105 - supervised_accuracy/batch: 54.6875\n",
            "2025-11-01 14:25:17,251 - supervised_finetuning - INFO - Epoch 105 - learning_rate: 0.0001\n",
            "Classifier Epoch 6/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.2997, Acc=48.44%]2025-11-01 14:25:17,299 - supervised_finetuning - INFO - Epoch 115 - supervised_loss/batch: 1.2997\n",
            "2025-11-01 14:25:17,299 - supervised_finetuning - INFO - Epoch 115 - supervised_accuracy/batch: 48.4375\n",
            "2025-11-01 14:25:17,299 - supervised_finetuning - INFO - Epoch 115 - learning_rate: 0.0001\n",
            "Classifier Epoch 6/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.5481, Acc=28.57%]2025-11-01 14:25:17,339 - supervised_finetuning - INFO - Epoch 125 - supervised_loss/batch: 1.5481\n",
            "2025-11-01 14:25:17,340 - supervised_finetuning - INFO - Epoch 125 - supervised_accuracy/batch: 28.5714\n",
            "2025-11-01 14:25:17,340 - supervised_finetuning - INFO - Epoch 125 - learning_rate: 0.0001\n",
            "Classifier Epoch 6/100: 100% 21/21 [00:00<00:00, 206.76it/s, Loss=1.5481, Acc=28.57%]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "2025-11-01 14:25:17,356 - supervised_finetuning - INFO - Epoch 6:\n",
            "2025-11-01 14:25:17,356 - supervised_finetuning - INFO -   Train Loss: 1.3064, Train Acc: 46.23%\n",
            "2025-11-01 14:25:17,356 - supervised_finetuning - INFO -   Train F1: 35.66%, Train Recall: 46.23%\n",
            "2025-11-01 14:25:17,356 - supervised_finetuning - INFO -   Train Precision: 42.76%\n",
            "2025-11-01 14:25:17,356 - supervised_finetuning - INFO - Epoch 5 - supervised_loss/epoch: 1.3064\n",
            "2025-11-01 14:25:17,357 - supervised_finetuning - INFO - Epoch 5 - supervised_accuracy/epoch: 46.2315\n",
            "2025-11-01 14:25:17,357 - supervised_finetuning - INFO - Epoch 5 - supervised_f1/epoch: 35.6632\n",
            "2025-11-01 14:25:17,357 - supervised_finetuning - INFO - Epoch 5 - supervised_recall/epoch: 46.2315\n",
            "2025-11-01 14:25:17,357 - supervised_finetuning - INFO - Epoch 5 - supervised_precision/epoch: 42.7587\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:19,246 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 46.23%\n",
            "Classifier Epoch 7/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.2565, Acc=46.88%]2025-11-01 14:25:19,262 - supervised_finetuning - INFO - Epoch 126 - supervised_loss/batch: 1.2565\n",
            "2025-11-01 14:25:19,262 - supervised_finetuning - INFO - Epoch 126 - supervised_accuracy/batch: 46.8750\n",
            "2025-11-01 14:25:19,262 - supervised_finetuning - INFO - Epoch 126 - learning_rate: 0.0001\n",
            "Classifier Epoch 7/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.2710, Acc=40.62%]2025-11-01 14:25:19,312 - supervised_finetuning - INFO - Epoch 136 - supervised_loss/batch: 1.2710\n",
            "2025-11-01 14:25:19,312 - supervised_finetuning - INFO - Epoch 136 - supervised_accuracy/batch: 40.6250\n",
            "2025-11-01 14:25:19,312 - supervised_finetuning - INFO - Epoch 136 - learning_rate: 0.0001\n",
            "Classifier Epoch 7/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.2302, Acc=42.86%]2025-11-01 14:25:19,347 - supervised_finetuning - INFO - Epoch 146 - supervised_loss/batch: 1.2302\n",
            "2025-11-01 14:25:19,347 - supervised_finetuning - INFO - Epoch 146 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:25:19,347 - supervised_finetuning - INFO - Epoch 146 - learning_rate: 0.0001\n",
            "Classifier Epoch 7/100: 100% 21/21 [00:00<00:00, 207.49it/s, Loss=1.2302, Acc=42.86%]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "2025-11-01 14:25:19,358 - supervised_finetuning - INFO - Epoch 7:\n",
            "2025-11-01 14:25:19,358 - supervised_finetuning - INFO -   Train Loss: 1.2181, Train Acc: 49.03%\n",
            "2025-11-01 14:25:19,358 - supervised_finetuning - INFO -   Train F1: 40.99%, Train Recall: 49.03%\n",
            "2025-11-01 14:25:19,358 - supervised_finetuning - INFO -   Train Precision: 41.00%\n",
            "2025-11-01 14:25:19,358 - supervised_finetuning - INFO - Epoch 6 - supervised_loss/epoch: 1.2181\n",
            "2025-11-01 14:25:19,359 - supervised_finetuning - INFO - Epoch 6 - supervised_accuracy/epoch: 49.0287\n",
            "2025-11-01 14:25:19,359 - supervised_finetuning - INFO - Epoch 6 - supervised_f1/epoch: 40.9878\n",
            "2025-11-01 14:25:19,359 - supervised_finetuning - INFO - Epoch 6 - supervised_recall/epoch: 49.0287\n",
            "2025-11-01 14:25:19,359 - supervised_finetuning - INFO - Epoch 6 - supervised_precision/epoch: 40.9973\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:20,409 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 49.03%\n",
            "Classifier Epoch 8/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.1502, Acc=57.81%]2025-11-01 14:25:20,415 - supervised_finetuning - INFO - Epoch 147 - supervised_loss/batch: 1.1502\n",
            "2025-11-01 14:25:20,415 - supervised_finetuning - INFO - Epoch 147 - supervised_accuracy/batch: 57.8125\n",
            "2025-11-01 14:25:20,415 - supervised_finetuning - INFO - Epoch 147 - learning_rate: 0.0001\n",
            "Classifier Epoch 8/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.2044, Acc=50.00%]2025-11-01 14:25:20,465 - supervised_finetuning - INFO - Epoch 157 - supervised_loss/batch: 1.2044\n",
            "2025-11-01 14:25:20,465 - supervised_finetuning - INFO - Epoch 157 - supervised_accuracy/batch: 50.0000\n",
            "2025-11-01 14:25:20,466 - supervised_finetuning - INFO - Epoch 157 - learning_rate: 0.0001\n",
            "Classifier Epoch 8/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.1186, Acc=42.86%]2025-11-01 14:25:20,504 - supervised_finetuning - INFO - Epoch 167 - supervised_loss/batch: 1.1186\n",
            "2025-11-01 14:25:20,505 - supervised_finetuning - INFO - Epoch 167 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:25:20,505 - supervised_finetuning - INFO - Epoch 167 - learning_rate: 0.0001\n",
            "Classifier Epoch 8/100: 100% 21/21 [00:00<00:00, 219.50it/s, Loss=1.1186, Acc=42.86%]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "2025-11-01 14:25:20,516 - supervised_finetuning - INFO - Epoch 8:\n",
            "2025-11-01 14:25:20,516 - supervised_finetuning - INFO -   Train Loss: 1.1633, Train Acc: 49.88%\n",
            "2025-11-01 14:25:20,516 - supervised_finetuning - INFO -   Train F1: 42.07%, Train Recall: 49.88%\n",
            "2025-11-01 14:25:20,517 - supervised_finetuning - INFO -   Train Precision: 42.21%\n",
            "2025-11-01 14:25:20,517 - supervised_finetuning - INFO - Epoch 7 - supervised_loss/epoch: 1.1633\n",
            "2025-11-01 14:25:20,517 - supervised_finetuning - INFO - Epoch 7 - supervised_accuracy/epoch: 49.8834\n",
            "2025-11-01 14:25:20,517 - supervised_finetuning - INFO - Epoch 7 - supervised_f1/epoch: 42.0699\n",
            "2025-11-01 14:25:20,517 - supervised_finetuning - INFO - Epoch 7 - supervised_recall/epoch: 49.8834\n",
            "2025-11-01 14:25:20,518 - supervised_finetuning - INFO - Epoch 7 - supervised_precision/epoch: 42.2113\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:21,663 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 49.88%\n",
            "Classifier Epoch 9/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.1606, Acc=48.44%]2025-11-01 14:25:21,672 - supervised_finetuning - INFO - Epoch 168 - supervised_loss/batch: 1.1606\n",
            "2025-11-01 14:25:21,672 - supervised_finetuning - INFO - Epoch 168 - supervised_accuracy/batch: 48.4375\n",
            "2025-11-01 14:25:21,672 - supervised_finetuning - INFO - Epoch 168 - learning_rate: 0.0001\n",
            "Classifier Epoch 9/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.2156, Acc=48.44%]2025-11-01 14:25:21,708 - supervised_finetuning - INFO - Epoch 178 - supervised_loss/batch: 1.2156\n",
            "2025-11-01 14:25:21,708 - supervised_finetuning - INFO - Epoch 178 - supervised_accuracy/batch: 48.4375\n",
            "2025-11-01 14:25:21,709 - supervised_finetuning - INFO - Epoch 178 - learning_rate: 0.0001\n",
            "Classifier Epoch 9/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0192, Acc=57.14%]2025-11-01 14:25:21,746 - supervised_finetuning - INFO - Epoch 188 - supervised_loss/batch: 1.0192\n",
            "2025-11-01 14:25:21,746 - supervised_finetuning - INFO - Epoch 188 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:21,747 - supervised_finetuning - INFO - Epoch 188 - learning_rate: 0.0001\n",
            "Classifier Epoch 9/100: 100% 21/21 [00:00<00:00, 253.27it/s, Loss=1.0192, Acc=57.14%]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "2025-11-01 14:25:21,764 - supervised_finetuning - INFO - Epoch 9:\n",
            "2025-11-01 14:25:21,764 - supervised_finetuning - INFO -   Train Loss: 1.1178, Train Acc: 51.90%\n",
            "2025-11-01 14:25:21,764 - supervised_finetuning - INFO -   Train F1: 46.74%, Train Recall: 51.90%\n",
            "2025-11-01 14:25:21,764 - supervised_finetuning - INFO -   Train Precision: 43.09%\n",
            "2025-11-01 14:25:21,764 - supervised_finetuning - INFO - Epoch 8 - supervised_loss/epoch: 1.1178\n",
            "2025-11-01 14:25:21,764 - supervised_finetuning - INFO - Epoch 8 - supervised_accuracy/epoch: 51.9037\n",
            "2025-11-01 14:25:21,765 - supervised_finetuning - INFO - Epoch 8 - supervised_f1/epoch: 46.7450\n",
            "2025-11-01 14:25:21,765 - supervised_finetuning - INFO - Epoch 8 - supervised_recall/epoch: 51.9037\n",
            "2025-11-01 14:25:21,765 - supervised_finetuning - INFO - Epoch 8 - supervised_precision/epoch: 43.0937\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:22,897 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 51.90%\n",
            "Classifier Epoch 10/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0323, Acc=54.69%]2025-11-01 14:25:22,902 - supervised_finetuning - INFO - Epoch 189 - supervised_loss/batch: 1.0323\n",
            "2025-11-01 14:25:22,902 - supervised_finetuning - INFO - Epoch 189 - supervised_accuracy/batch: 54.6875\n",
            "2025-11-01 14:25:22,903 - supervised_finetuning - INFO - Epoch 189 - learning_rate: 0.0001\n",
            "Classifier Epoch 10/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0083, Acc=60.94%]2025-11-01 14:25:22,953 - supervised_finetuning - INFO - Epoch 199 - supervised_loss/batch: 1.0083\n",
            "2025-11-01 14:25:22,954 - supervised_finetuning - INFO - Epoch 199 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:25:22,954 - supervised_finetuning - INFO - Epoch 199 - learning_rate: 0.0001\n",
            "Classifier Epoch 10/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0205, Acc=57.14%]2025-11-01 14:25:22,989 - supervised_finetuning - INFO - Epoch 209 - supervised_loss/batch: 1.0205\n",
            "2025-11-01 14:25:22,989 - supervised_finetuning - INFO - Epoch 209 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:22,990 - supervised_finetuning - INFO - Epoch 209 - learning_rate: 0.0001\n",
            "Classifier Epoch 10/100: 100% 21/21 [00:00<00:00, 226.76it/s, Loss=1.0205, Acc=57.14%]\n",
            "2025-11-01 14:25:23,000 - supervised_finetuning - INFO - Epoch 10:\n",
            "2025-11-01 14:25:23,000 - supervised_finetuning - INFO -   Train Loss: 1.0748, Train Acc: 55.01%\n",
            "2025-11-01 14:25:23,000 - supervised_finetuning - INFO -   Train F1: 50.69%, Train Recall: 55.01%\n",
            "2025-11-01 14:25:23,000 - supervised_finetuning - INFO -   Train Precision: 58.04%\n",
            "2025-11-01 14:25:23,001 - supervised_finetuning - INFO - Epoch 9 - supervised_loss/epoch: 1.0748\n",
            "2025-11-01 14:25:23,001 - supervised_finetuning - INFO - Epoch 9 - supervised_accuracy/epoch: 55.0117\n",
            "2025-11-01 14:25:23,001 - supervised_finetuning - INFO - Epoch 9 - supervised_f1/epoch: 50.6927\n",
            "2025-11-01 14:25:23,001 - supervised_finetuning - INFO - Epoch 9 - supervised_recall/epoch: 55.0117\n",
            "2025-11-01 14:25:23,002 - supervised_finetuning - INFO - Epoch 9 - supervised_precision/epoch: 58.0407\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:24,205 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 55.01%\n",
            "Classifier Epoch 11/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9618, Acc=57.81%]2025-11-01 14:25:24,214 - supervised_finetuning - INFO - Epoch 210 - supervised_loss/batch: 0.9618\n",
            "2025-11-01 14:25:24,214 - supervised_finetuning - INFO - Epoch 210 - supervised_accuracy/batch: 57.8125\n",
            "2025-11-01 14:25:24,214 - supervised_finetuning - INFO - Epoch 210 - learning_rate: 0.0001\n",
            "Classifier Epoch 11/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.2289, Acc=46.88%]2025-11-01 14:25:24,264 - supervised_finetuning - INFO - Epoch 220 - supervised_loss/batch: 1.2289\n",
            "2025-11-01 14:25:24,264 - supervised_finetuning - INFO - Epoch 220 - supervised_accuracy/batch: 46.8750\n",
            "2025-11-01 14:25:24,264 - supervised_finetuning - INFO - Epoch 220 - learning_rate: 0.0001\n",
            "Classifier Epoch 11/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6827, Acc=71.43%]2025-11-01 14:25:24,300 - supervised_finetuning - INFO - Epoch 230 - supervised_loss/batch: 0.6827\n",
            "2025-11-01 14:25:24,300 - supervised_finetuning - INFO - Epoch 230 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:24,301 - supervised_finetuning - INFO - Epoch 230 - learning_rate: 0.0001\n",
            "Classifier Epoch 11/100: 100% 21/21 [00:00<00:00, 222.05it/s, Loss=0.6827, Acc=71.43%]\n",
            "2025-11-01 14:25:24,312 - supervised_finetuning - INFO - Epoch 11:\n",
            "2025-11-01 14:25:24,312 - supervised_finetuning - INFO -   Train Loss: 1.0319, Train Acc: 56.95%\n",
            "2025-11-01 14:25:24,312 - supervised_finetuning - INFO -   Train F1: 54.22%, Train Recall: 56.95%\n",
            "2025-11-01 14:25:24,312 - supervised_finetuning - INFO -   Train Precision: 58.65%\n",
            "2025-11-01 14:25:24,312 - supervised_finetuning - INFO - Epoch 10 - supervised_loss/epoch: 1.0319\n",
            "2025-11-01 14:25:24,312 - supervised_finetuning - INFO - Epoch 10 - supervised_accuracy/epoch: 56.9542\n",
            "2025-11-01 14:25:24,313 - supervised_finetuning - INFO - Epoch 10 - supervised_f1/epoch: 54.2202\n",
            "2025-11-01 14:25:24,313 - supervised_finetuning - INFO - Epoch 10 - supervised_recall/epoch: 56.9542\n",
            "2025-11-01 14:25:24,313 - supervised_finetuning - INFO - Epoch 10 - supervised_precision/epoch: 58.6452\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:26,057 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 56.95%\n",
            "Classifier Epoch 12/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0182, Acc=64.06%]2025-11-01 14:25:26,062 - supervised_finetuning - INFO - Epoch 231 - supervised_loss/batch: 1.0182\n",
            "2025-11-01 14:25:26,063 - supervised_finetuning - INFO - Epoch 231 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:25:26,063 - supervised_finetuning - INFO - Epoch 231 - learning_rate: 0.0001\n",
            "Classifier Epoch 12/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0361, Acc=62.50%]2025-11-01 14:25:26,113 - supervised_finetuning - INFO - Epoch 241 - supervised_loss/batch: 1.0361\n",
            "2025-11-01 14:25:26,113 - supervised_finetuning - INFO - Epoch 241 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:26,113 - supervised_finetuning - INFO - Epoch 241 - learning_rate: 0.0001\n",
            "Classifier Epoch 12/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8523, Acc=71.43%]2025-11-01 14:25:26,153 - supervised_finetuning - INFO - Epoch 251 - supervised_loss/batch: 0.8523\n",
            "2025-11-01 14:25:26,153 - supervised_finetuning - INFO - Epoch 251 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:26,153 - supervised_finetuning - INFO - Epoch 251 - learning_rate: 0.0001\n",
            "Classifier Epoch 12/100: 100% 21/21 [00:00<00:00, 217.58it/s, Loss=0.8523, Acc=71.43%]\n",
            "2025-11-01 14:25:26,164 - supervised_finetuning - INFO - Epoch 12:\n",
            "2025-11-01 14:25:26,164 - supervised_finetuning - INFO -   Train Loss: 0.9965, Train Acc: 59.29%\n",
            "2025-11-01 14:25:26,164 - supervised_finetuning - INFO -   Train F1: 57.30%, Train Recall: 59.29%\n",
            "2025-11-01 14:25:26,164 - supervised_finetuning - INFO -   Train Precision: 58.98%\n",
            "2025-11-01 14:25:26,164 - supervised_finetuning - INFO - Epoch 11 - supervised_loss/epoch: 0.9965\n",
            "2025-11-01 14:25:26,164 - supervised_finetuning - INFO - Epoch 11 - supervised_accuracy/epoch: 59.2852\n",
            "2025-11-01 14:25:26,165 - supervised_finetuning - INFO - Epoch 11 - supervised_f1/epoch: 57.2989\n",
            "2025-11-01 14:25:26,165 - supervised_finetuning - INFO - Epoch 11 - supervised_recall/epoch: 59.2852\n",
            "2025-11-01 14:25:26,165 - supervised_finetuning - INFO - Epoch 11 - supervised_precision/epoch: 58.9761\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:29,461 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 59.29%\n",
            "Classifier Epoch 13/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9876, Acc=62.50%]2025-11-01 14:25:29,470 - supervised_finetuning - INFO - Epoch 252 - supervised_loss/batch: 0.9876\n",
            "2025-11-01 14:25:29,470 - supervised_finetuning - INFO - Epoch 252 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:29,470 - supervised_finetuning - INFO - Epoch 252 - learning_rate: 0.0001\n",
            "Classifier Epoch 13/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0102, Acc=57.81%]2025-11-01 14:25:29,516 - supervised_finetuning - INFO - Epoch 262 - supervised_loss/batch: 1.0102\n",
            "2025-11-01 14:25:29,516 - supervised_finetuning - INFO - Epoch 262 - supervised_accuracy/batch: 57.8125\n",
            "2025-11-01 14:25:29,516 - supervised_finetuning - INFO - Epoch 262 - learning_rate: 0.0001\n",
            "Classifier Epoch 13/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7413, Acc=71.43%]2025-11-01 14:25:29,560 - supervised_finetuning - INFO - Epoch 272 - supervised_loss/batch: 0.7413\n",
            "2025-11-01 14:25:29,560 - supervised_finetuning - INFO - Epoch 272 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:29,560 - supervised_finetuning - INFO - Epoch 272 - learning_rate: 0.0001\n",
            "Classifier Epoch 13/100: 100% 21/21 [00:00<00:00, 212.97it/s, Loss=0.7413, Acc=71.43%]\n",
            "2025-11-01 14:25:29,577 - supervised_finetuning - INFO - Epoch 13:\n",
            "2025-11-01 14:25:29,578 - supervised_finetuning - INFO -   Train Loss: 0.9712, Train Acc: 60.30%\n",
            "2025-11-01 14:25:29,578 - supervised_finetuning - INFO -   Train F1: 59.23%, Train Recall: 60.30%\n",
            "2025-11-01 14:25:29,578 - supervised_finetuning - INFO -   Train Precision: 59.52%\n",
            "2025-11-01 14:25:29,578 - supervised_finetuning - INFO - Epoch 12 - supervised_loss/epoch: 0.9712\n",
            "2025-11-01 14:25:29,578 - supervised_finetuning - INFO - Epoch 12 - supervised_accuracy/epoch: 60.2953\n",
            "2025-11-01 14:25:29,579 - supervised_finetuning - INFO - Epoch 12 - supervised_f1/epoch: 59.2293\n",
            "2025-11-01 14:25:29,579 - supervised_finetuning - INFO - Epoch 12 - supervised_recall/epoch: 60.2953\n",
            "2025-11-01 14:25:29,579 - supervised_finetuning - INFO - Epoch 12 - supervised_precision/epoch: 59.5221\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:30,712 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 60.30%\n",
            "Classifier Epoch 14/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9653, Acc=59.38%]2025-11-01 14:25:30,719 - supervised_finetuning - INFO - Epoch 273 - supervised_loss/batch: 0.9653\n",
            "2025-11-01 14:25:30,719 - supervised_finetuning - INFO - Epoch 273 - supervised_accuracy/batch: 59.3750\n",
            "2025-11-01 14:25:30,720 - supervised_finetuning - INFO - Epoch 273 - learning_rate: 0.0001\n",
            "Classifier Epoch 14/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9881, Acc=53.12%]2025-11-01 14:25:30,780 - supervised_finetuning - INFO - Epoch 283 - supervised_loss/batch: 0.9881\n",
            "2025-11-01 14:25:30,781 - supervised_finetuning - INFO - Epoch 283 - supervised_accuracy/batch: 53.1250\n",
            "2025-11-01 14:25:30,782 - supervised_finetuning - INFO - Epoch 283 - learning_rate: 0.0001\n",
            "Classifier Epoch 14/100:  71% 15/21 [00:00<00:00, 147.78it/s, Loss=0.9304, Acc=57.14%]2025-11-01 14:25:30,843 - supervised_finetuning - INFO - Epoch 293 - supervised_loss/batch: 0.9304\n",
            "2025-11-01 14:25:30,843 - supervised_finetuning - INFO - Epoch 293 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:30,843 - supervised_finetuning - INFO - Epoch 293 - learning_rate: 0.0001\n",
            "Classifier Epoch 14/100: 100% 21/21 [00:00<00:00, 160.17it/s, Loss=0.9304, Acc=57.14%]\n",
            "2025-11-01 14:25:30,863 - supervised_finetuning - INFO - Epoch 14:\n",
            "2025-11-01 14:25:30,863 - supervised_finetuning - INFO -   Train Loss: 0.9673, Train Acc: 59.83%\n",
            "2025-11-01 14:25:30,863 - supervised_finetuning - INFO -   Train F1: 59.11%, Train Recall: 59.83%\n",
            "2025-11-01 14:25:30,864 - supervised_finetuning - INFO -   Train Precision: 59.20%\n",
            "2025-11-01 14:25:30,864 - supervised_finetuning - INFO - Epoch 13 - supervised_loss/epoch: 0.9673\n",
            "2025-11-01 14:25:30,864 - supervised_finetuning - INFO - Epoch 13 - supervised_accuracy/epoch: 59.8291\n",
            "2025-11-01 14:25:30,864 - supervised_finetuning - INFO - Epoch 13 - supervised_f1/epoch: 59.1068\n",
            "2025-11-01 14:25:30,864 - supervised_finetuning - INFO - Epoch 13 - supervised_recall/epoch: 59.8291\n",
            "2025-11-01 14:25:30,864 - supervised_finetuning - INFO - Epoch 13 - supervised_precision/epoch: 59.1951\n",
            "Classifier Epoch 15/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8562, Acc=64.06%]2025-11-01 14:25:30,870 - supervised_finetuning - INFO - Epoch 294 - supervised_loss/batch: 0.8562\n",
            "2025-11-01 14:25:30,870 - supervised_finetuning - INFO - Epoch 294 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:25:30,870 - supervised_finetuning - INFO - Epoch 294 - learning_rate: 0.0001\n",
            "Classifier Epoch 15/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8740, Acc=60.94%]2025-11-01 14:25:30,919 - supervised_finetuning - INFO - Epoch 304 - supervised_loss/batch: 0.8740\n",
            "2025-11-01 14:25:30,920 - supervised_finetuning - INFO - Epoch 304 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:25:30,920 - supervised_finetuning - INFO - Epoch 304 - learning_rate: 0.0001\n",
            "Classifier Epoch 15/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6921, Acc=85.71%]2025-11-01 14:25:30,968 - supervised_finetuning - INFO - Epoch 314 - supervised_loss/batch: 0.6921\n",
            "2025-11-01 14:25:30,968 - supervised_finetuning - INFO - Epoch 314 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:25:30,968 - supervised_finetuning - INFO - Epoch 314 - learning_rate: 0.0001\n",
            "Classifier Epoch 15/100: 100% 21/21 [00:00<00:00, 202.15it/s, Loss=0.6921, Acc=85.71%]\n",
            "2025-11-01 14:25:30,986 - supervised_finetuning - INFO - Epoch 15:\n",
            "2025-11-01 14:25:30,986 - supervised_finetuning - INFO -   Train Loss: 0.9362, Train Acc: 61.07%\n",
            "2025-11-01 14:25:30,987 - supervised_finetuning - INFO -   Train F1: 60.08%, Train Recall: 61.07%\n",
            "2025-11-01 14:25:30,987 - supervised_finetuning - INFO -   Train Precision: 59.89%\n",
            "2025-11-01 14:25:30,987 - supervised_finetuning - INFO - Epoch 14 - supervised_loss/epoch: 0.9362\n",
            "2025-11-01 14:25:30,987 - supervised_finetuning - INFO - Epoch 14 - supervised_accuracy/epoch: 61.0723\n",
            "2025-11-01 14:25:30,987 - supervised_finetuning - INFO - Epoch 14 - supervised_f1/epoch: 60.0779\n",
            "2025-11-01 14:25:30,987 - supervised_finetuning - INFO - Epoch 14 - supervised_recall/epoch: 61.0723\n",
            "2025-11-01 14:25:30,987 - supervised_finetuning - INFO - Epoch 14 - supervised_precision/epoch: 59.8902\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:32,322 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 61.07%\n",
            "Classifier Epoch 16/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8452, Acc=70.31%]2025-11-01 14:25:32,341 - supervised_finetuning - INFO - Epoch 315 - supervised_loss/batch: 0.8452\n",
            "2025-11-01 14:25:32,342 - supervised_finetuning - INFO - Epoch 315 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:25:32,343 - supervised_finetuning - INFO - Epoch 315 - learning_rate: 0.0001\n",
            "Classifier Epoch 16/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9140, Acc=62.50%]2025-11-01 14:25:32,404 - supervised_finetuning - INFO - Epoch 325 - supervised_loss/batch: 0.9140\n",
            "2025-11-01 14:25:32,404 - supervised_finetuning - INFO - Epoch 325 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:32,404 - supervised_finetuning - INFO - Epoch 325 - learning_rate: 0.0001\n",
            "Classifier Epoch 16/100:  76% 16/21 [00:00<00:00, 153.93it/s, Loss=0.8698, Acc=71.43%]2025-11-01 14:25:32,456 - supervised_finetuning - INFO - Epoch 335 - supervised_loss/batch: 0.8698\n",
            "2025-11-01 14:25:32,457 - supervised_finetuning - INFO - Epoch 335 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:32,457 - supervised_finetuning - INFO - Epoch 335 - learning_rate: 0.0001\n",
            "Classifier Epoch 16/100: 100% 21/21 [00:00<00:00, 159.71it/s, Loss=0.8698, Acc=71.43%]\n",
            "2025-11-01 14:25:32,474 - supervised_finetuning - INFO - Epoch 16:\n",
            "2025-11-01 14:25:32,475 - supervised_finetuning - INFO -   Train Loss: 0.9352, Train Acc: 62.24%\n",
            "2025-11-01 14:25:32,475 - supervised_finetuning - INFO -   Train F1: 61.51%, Train Recall: 62.24%\n",
            "2025-11-01 14:25:32,475 - supervised_finetuning - INFO -   Train Precision: 61.14%\n",
            "2025-11-01 14:25:32,475 - supervised_finetuning - INFO - Epoch 15 - supervised_loss/epoch: 0.9352\n",
            "2025-11-01 14:25:32,475 - supervised_finetuning - INFO - Epoch 15 - supervised_accuracy/epoch: 62.2378\n",
            "2025-11-01 14:25:32,475 - supervised_finetuning - INFO - Epoch 15 - supervised_f1/epoch: 61.5108\n",
            "2025-11-01 14:25:32,476 - supervised_finetuning - INFO - Epoch 15 - supervised_recall/epoch: 62.2378\n",
            "2025-11-01 14:25:32,476 - supervised_finetuning - INFO - Epoch 15 - supervised_precision/epoch: 61.1426\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:35,222 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 62.24%\n",
            "Classifier Epoch 17/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0870, Acc=53.12%]2025-11-01 14:25:35,228 - supervised_finetuning - INFO - Epoch 336 - supervised_loss/batch: 1.0870\n",
            "2025-11-01 14:25:35,228 - supervised_finetuning - INFO - Epoch 336 - supervised_accuracy/batch: 53.1250\n",
            "2025-11-01 14:25:35,228 - supervised_finetuning - INFO - Epoch 336 - learning_rate: 0.0001\n",
            "Classifier Epoch 17/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9640, Acc=62.50%]2025-11-01 14:25:35,271 - supervised_finetuning - INFO - Epoch 346 - supervised_loss/batch: 0.9640\n",
            "2025-11-01 14:25:35,271 - supervised_finetuning - INFO - Epoch 346 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:35,272 - supervised_finetuning - INFO - Epoch 346 - learning_rate: 0.0001\n",
            "Classifier Epoch 17/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7102, Acc=57.14%]2025-11-01 14:25:35,309 - supervised_finetuning - INFO - Epoch 356 - supervised_loss/batch: 0.7102\n",
            "2025-11-01 14:25:35,309 - supervised_finetuning - INFO - Epoch 356 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:35,309 - supervised_finetuning - INFO - Epoch 356 - learning_rate: 0.0001\n",
            "Classifier Epoch 17/100: 100% 21/21 [00:00<00:00, 241.41it/s, Loss=0.7102, Acc=57.14%]\n",
            "2025-11-01 14:25:35,321 - supervised_finetuning - INFO - Epoch 17:\n",
            "2025-11-01 14:25:35,321 - supervised_finetuning - INFO -   Train Loss: 0.9273, Train Acc: 61.54%\n",
            "2025-11-01 14:25:35,321 - supervised_finetuning - INFO -   Train F1: 60.81%, Train Recall: 61.54%\n",
            "2025-11-01 14:25:35,321 - supervised_finetuning - INFO -   Train Precision: 60.48%\n",
            "2025-11-01 14:25:35,321 - supervised_finetuning - INFO - Epoch 16 - supervised_loss/epoch: 0.9273\n",
            "2025-11-01 14:25:35,322 - supervised_finetuning - INFO - Epoch 16 - supervised_accuracy/epoch: 61.5385\n",
            "2025-11-01 14:25:35,322 - supervised_finetuning - INFO - Epoch 16 - supervised_f1/epoch: 60.8053\n",
            "2025-11-01 14:25:35,323 - supervised_finetuning - INFO - Epoch 16 - supervised_recall/epoch: 61.5385\n",
            "2025-11-01 14:25:35,323 - supervised_finetuning - INFO - Epoch 16 - supervised_precision/epoch: 60.4811\n",
            "Classifier Epoch 18/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9066, Acc=60.94%]2025-11-01 14:25:35,327 - supervised_finetuning - INFO - Epoch 357 - supervised_loss/batch: 0.9066\n",
            "2025-11-01 14:25:35,328 - supervised_finetuning - INFO - Epoch 357 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:25:35,328 - supervised_finetuning - INFO - Epoch 357 - learning_rate: 0.0001\n",
            "Classifier Epoch 18/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9345, Acc=60.94%]2025-11-01 14:25:35,369 - supervised_finetuning - INFO - Epoch 367 - supervised_loss/batch: 0.9345\n",
            "2025-11-01 14:25:35,369 - supervised_finetuning - INFO - Epoch 367 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:25:35,369 - supervised_finetuning - INFO - Epoch 367 - learning_rate: 0.0001\n",
            "Classifier Epoch 18/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7526, Acc=71.43%]2025-11-01 14:25:35,421 - supervised_finetuning - INFO - Epoch 377 - supervised_loss/batch: 0.7526\n",
            "2025-11-01 14:25:35,421 - supervised_finetuning - INFO - Epoch 377 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:35,421 - supervised_finetuning - INFO - Epoch 377 - learning_rate: 0.0001\n",
            "Classifier Epoch 18/100: 100% 21/21 [00:00<00:00, 213.74it/s, Loss=0.7526, Acc=71.43%]\n",
            "2025-11-01 14:25:35,432 - supervised_finetuning - INFO - Epoch 18:\n",
            "2025-11-01 14:25:35,433 - supervised_finetuning - INFO -   Train Loss: 0.9259, Train Acc: 61.93%\n",
            "2025-11-01 14:25:35,433 - supervised_finetuning - INFO -   Train F1: 61.03%, Train Recall: 61.93%\n",
            "2025-11-01 14:25:35,433 - supervised_finetuning - INFO -   Train Precision: 60.74%\n",
            "2025-11-01 14:25:35,433 - supervised_finetuning - INFO - Epoch 17 - supervised_loss/epoch: 0.9259\n",
            "2025-11-01 14:25:35,433 - supervised_finetuning - INFO - Epoch 17 - supervised_accuracy/epoch: 61.9270\n",
            "2025-11-01 14:25:35,433 - supervised_finetuning - INFO - Epoch 17 - supervised_f1/epoch: 61.0254\n",
            "2025-11-01 14:25:35,434 - supervised_finetuning - INFO - Epoch 17 - supervised_recall/epoch: 61.9270\n",
            "2025-11-01 14:25:35,434 - supervised_finetuning - INFO - Epoch 17 - supervised_precision/epoch: 60.7396\n",
            "Classifier Epoch 19/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0584, Acc=59.38%]2025-11-01 14:25:35,439 - supervised_finetuning - INFO - Epoch 378 - supervised_loss/batch: 1.0584\n",
            "2025-11-01 14:25:35,439 - supervised_finetuning - INFO - Epoch 378 - supervised_accuracy/batch: 59.3750\n",
            "2025-11-01 14:25:35,439 - supervised_finetuning - INFO - Epoch 378 - learning_rate: 0.0001\n",
            "Classifier Epoch 19/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0117, Acc=60.94%]2025-11-01 14:25:35,479 - supervised_finetuning - INFO - Epoch 388 - supervised_loss/batch: 1.0117\n",
            "2025-11-01 14:25:35,479 - supervised_finetuning - INFO - Epoch 388 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:25:35,479 - supervised_finetuning - INFO - Epoch 388 - learning_rate: 0.0001\n",
            "Classifier Epoch 19/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0394, Acc=57.14%]2025-11-01 14:25:35,521 - supervised_finetuning - INFO - Epoch 398 - supervised_loss/batch: 1.0394\n",
            "2025-11-01 14:25:35,521 - supervised_finetuning - INFO - Epoch 398 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:35,521 - supervised_finetuning - INFO - Epoch 398 - learning_rate: 0.0001\n",
            "Classifier Epoch 19/100: 100% 21/21 [00:00<00:00, 240.25it/s, Loss=1.0394, Acc=57.14%]\n",
            "2025-11-01 14:25:35,532 - supervised_finetuning - INFO - Epoch 19:\n",
            "2025-11-01 14:25:35,532 - supervised_finetuning - INFO -   Train Loss: 0.9244, Train Acc: 62.70%\n",
            "2025-11-01 14:25:35,533 - supervised_finetuning - INFO -   Train F1: 61.85%, Train Recall: 62.70%\n",
            "2025-11-01 14:25:35,533 - supervised_finetuning - INFO -   Train Precision: 61.52%\n",
            "2025-11-01 14:25:35,533 - supervised_finetuning - INFO - Epoch 18 - supervised_loss/epoch: 0.9244\n",
            "2025-11-01 14:25:35,533 - supervised_finetuning - INFO - Epoch 18 - supervised_accuracy/epoch: 62.7040\n",
            "2025-11-01 14:25:35,533 - supervised_finetuning - INFO - Epoch 18 - supervised_f1/epoch: 61.8490\n",
            "2025-11-01 14:25:35,534 - supervised_finetuning - INFO - Epoch 18 - supervised_recall/epoch: 62.7040\n",
            "2025-11-01 14:25:35,534 - supervised_finetuning - INFO - Epoch 18 - supervised_precision/epoch: 61.5167\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:36,530 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 62.70%\n",
            "Classifier Epoch 20/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9114, Acc=70.31%]2025-11-01 14:25:36,547 - supervised_finetuning - INFO - Epoch 399 - supervised_loss/batch: 0.9114\n",
            "2025-11-01 14:25:36,548 - supervised_finetuning - INFO - Epoch 399 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:25:36,548 - supervised_finetuning - INFO - Epoch 399 - learning_rate: 0.0001\n",
            "Classifier Epoch 20/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8606, Acc=71.88%]2025-11-01 14:25:36,591 - supervised_finetuning - INFO - Epoch 409 - supervised_loss/batch: 0.8606\n",
            "2025-11-01 14:25:36,591 - supervised_finetuning - INFO - Epoch 409 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:25:36,591 - supervised_finetuning - INFO - Epoch 409 - learning_rate: 0.0001\n",
            "Classifier Epoch 20/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9126, Acc=57.14%]2025-11-01 14:25:36,627 - supervised_finetuning - INFO - Epoch 419 - supervised_loss/batch: 0.9126\n",
            "2025-11-01 14:25:36,627 - supervised_finetuning - INFO - Epoch 419 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:36,628 - supervised_finetuning - INFO - Epoch 419 - learning_rate: 0.0001\n",
            "Classifier Epoch 20/100: 100% 21/21 [00:00<00:00, 215.11it/s, Loss=0.9126, Acc=57.14%]\n",
            "2025-11-01 14:25:36,638 - supervised_finetuning - INFO - Epoch 20:\n",
            "2025-11-01 14:25:36,639 - supervised_finetuning - INFO -   Train Loss: 0.9118, Train Acc: 63.33%\n",
            "2025-11-01 14:25:36,639 - supervised_finetuning - INFO -   Train F1: 62.71%, Train Recall: 63.33%\n",
            "2025-11-01 14:25:36,639 - supervised_finetuning - INFO -   Train Precision: 62.78%\n",
            "2025-11-01 14:25:36,639 - supervised_finetuning - INFO - Epoch 19 - supervised_loss/epoch: 0.9118\n",
            "2025-11-01 14:25:36,639 - supervised_finetuning - INFO - Epoch 19 - supervised_accuracy/epoch: 63.3256\n",
            "2025-11-01 14:25:36,639 - supervised_finetuning - INFO - Epoch 19 - supervised_f1/epoch: 62.7059\n",
            "2025-11-01 14:25:36,640 - supervised_finetuning - INFO - Epoch 19 - supervised_recall/epoch: 63.3256\n",
            "2025-11-01 14:25:36,640 - supervised_finetuning - INFO - Epoch 19 - supervised_precision/epoch: 62.7788\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:40,153 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 63.33%\n",
            "Classifier Epoch 21/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8489, Acc=65.62%]2025-11-01 14:25:40,158 - supervised_finetuning - INFO - Epoch 420 - supervised_loss/batch: 0.8489\n",
            "2025-11-01 14:25:40,158 - supervised_finetuning - INFO - Epoch 420 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:25:40,158 - supervised_finetuning - INFO - Epoch 420 - learning_rate: 0.0001\n",
            "Classifier Epoch 21/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7253, Acc=75.00%]2025-11-01 14:25:40,195 - supervised_finetuning - INFO - Epoch 430 - supervised_loss/batch: 0.7253\n",
            "2025-11-01 14:25:40,195 - supervised_finetuning - INFO - Epoch 430 - supervised_accuracy/batch: 75.0000\n",
            "2025-11-01 14:25:40,196 - supervised_finetuning - INFO - Epoch 430 - learning_rate: 0.0001\n",
            "Classifier Epoch 21/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5344, Acc=100.00%]2025-11-01 14:25:40,241 - supervised_finetuning - INFO - Epoch 440 - supervised_loss/batch: 0.5344\n",
            "2025-11-01 14:25:40,241 - supervised_finetuning - INFO - Epoch 440 - supervised_accuracy/batch: 100.0000\n",
            "2025-11-01 14:25:40,241 - supervised_finetuning - INFO - Epoch 440 - learning_rate: 0.0001\n",
            "Classifier Epoch 21/100: 100% 21/21 [00:00<00:00, 237.77it/s, Loss=0.5344, Acc=100.00%]\n",
            "2025-11-01 14:25:40,255 - supervised_finetuning - INFO - Epoch 21:\n",
            "2025-11-01 14:25:40,255 - supervised_finetuning - INFO -   Train Loss: 0.8934, Train Acc: 62.24%\n",
            "2025-11-01 14:25:40,255 - supervised_finetuning - INFO -   Train F1: 61.70%, Train Recall: 62.24%\n",
            "2025-11-01 14:25:40,255 - supervised_finetuning - INFO -   Train Precision: 61.46%\n",
            "2025-11-01 14:25:40,255 - supervised_finetuning - INFO - Epoch 20 - supervised_loss/epoch: 0.8934\n",
            "2025-11-01 14:25:40,256 - supervised_finetuning - INFO - Epoch 20 - supervised_accuracy/epoch: 62.2378\n",
            "2025-11-01 14:25:40,256 - supervised_finetuning - INFO - Epoch 20 - supervised_f1/epoch: 61.7012\n",
            "2025-11-01 14:25:40,256 - supervised_finetuning - INFO - Epoch 20 - supervised_recall/epoch: 62.2378\n",
            "2025-11-01 14:25:40,256 - supervised_finetuning - INFO - Epoch 20 - supervised_precision/epoch: 61.4629\n",
            "Classifier Epoch 22/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7049, Acc=73.44%]2025-11-01 14:25:40,261 - supervised_finetuning - INFO - Epoch 441 - supervised_loss/batch: 0.7049\n",
            "2025-11-01 14:25:40,261 - supervised_finetuning - INFO - Epoch 441 - supervised_accuracy/batch: 73.4375\n",
            "2025-11-01 14:25:40,262 - supervised_finetuning - INFO - Epoch 441 - learning_rate: 0.0001\n",
            "Classifier Epoch 22/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9914, Acc=62.50%]2025-11-01 14:25:40,300 - supervised_finetuning - INFO - Epoch 451 - supervised_loss/batch: 0.9914\n",
            "2025-11-01 14:25:40,300 - supervised_finetuning - INFO - Epoch 451 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:40,300 - supervised_finetuning - INFO - Epoch 451 - learning_rate: 0.0001\n",
            "Classifier Epoch 22/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.1872, Acc=57.14%]2025-11-01 14:25:40,336 - supervised_finetuning - INFO - Epoch 461 - supervised_loss/batch: 1.1872\n",
            "2025-11-01 14:25:40,336 - supervised_finetuning - INFO - Epoch 461 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:40,336 - supervised_finetuning - INFO - Epoch 461 - learning_rate: 0.0001\n",
            "Classifier Epoch 22/100: 100% 21/21 [00:00<00:00, 262.62it/s, Loss=1.1872, Acc=57.14%]\n",
            "2025-11-01 14:25:40,347 - supervised_finetuning - INFO - Epoch 22:\n",
            "2025-11-01 14:25:40,347 - supervised_finetuning - INFO -   Train Loss: 0.9162, Train Acc: 63.33%\n",
            "2025-11-01 14:25:40,347 - supervised_finetuning - INFO -   Train F1: 62.79%, Train Recall: 63.33%\n",
            "2025-11-01 14:25:40,347 - supervised_finetuning - INFO -   Train Precision: 62.60%\n",
            "2025-11-01 14:25:40,347 - supervised_finetuning - INFO - Epoch 21 - supervised_loss/epoch: 0.9162\n",
            "2025-11-01 14:25:40,347 - supervised_finetuning - INFO - Epoch 21 - supervised_accuracy/epoch: 63.3256\n",
            "2025-11-01 14:25:40,348 - supervised_finetuning - INFO - Epoch 21 - supervised_f1/epoch: 62.7858\n",
            "2025-11-01 14:25:40,348 - supervised_finetuning - INFO - Epoch 21 - supervised_recall/epoch: 63.3256\n",
            "2025-11-01 14:25:40,348 - supervised_finetuning - INFO - Epoch 21 - supervised_precision/epoch: 62.6034\n",
            "Classifier Epoch 23/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0016, Acc=62.50%]2025-11-01 14:25:40,352 - supervised_finetuning - INFO - Epoch 462 - supervised_loss/batch: 1.0016\n",
            "2025-11-01 14:25:40,352 - supervised_finetuning - INFO - Epoch 462 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:40,353 - supervised_finetuning - INFO - Epoch 462 - learning_rate: 0.0001\n",
            "Classifier Epoch 23/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9220, Acc=60.94%]2025-11-01 14:25:40,390 - supervised_finetuning - INFO - Epoch 472 - supervised_loss/batch: 0.9220\n",
            "2025-11-01 14:25:40,390 - supervised_finetuning - INFO - Epoch 472 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:25:40,390 - supervised_finetuning - INFO - Epoch 472 - learning_rate: 0.0001\n",
            "Classifier Epoch 23/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.5628, Acc=57.14%]2025-11-01 14:25:40,432 - supervised_finetuning - INFO - Epoch 482 - supervised_loss/batch: 1.5628\n",
            "2025-11-01 14:25:40,432 - supervised_finetuning - INFO - Epoch 482 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:40,432 - supervised_finetuning - INFO - Epoch 482 - learning_rate: 0.0001\n",
            "Classifier Epoch 23/100: 100% 21/21 [00:00<00:00, 249.95it/s, Loss=1.5628, Acc=57.14%]\n",
            "2025-11-01 14:25:40,443 - supervised_finetuning - INFO - Epoch 23:\n",
            "2025-11-01 14:25:40,443 - supervised_finetuning - INFO -   Train Loss: 0.9190, Train Acc: 63.87%\n",
            "2025-11-01 14:25:40,443 - supervised_finetuning - INFO -   Train F1: 63.22%, Train Recall: 63.87%\n",
            "2025-11-01 14:25:40,443 - supervised_finetuning - INFO -   Train Precision: 62.90%\n",
            "2025-11-01 14:25:40,443 - supervised_finetuning - INFO - Epoch 22 - supervised_loss/epoch: 0.9190\n",
            "2025-11-01 14:25:40,444 - supervised_finetuning - INFO - Epoch 22 - supervised_accuracy/epoch: 63.8695\n",
            "2025-11-01 14:25:40,444 - supervised_finetuning - INFO - Epoch 22 - supervised_f1/epoch: 63.2243\n",
            "2025-11-01 14:25:40,445 - supervised_finetuning - INFO - Epoch 22 - supervised_recall/epoch: 63.8695\n",
            "2025-11-01 14:25:40,445 - supervised_finetuning - INFO - Epoch 22 - supervised_precision/epoch: 62.8991\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:41,348 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 63.87%\n",
            "Classifier Epoch 24/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8139, Acc=70.31%]2025-11-01 14:25:41,354 - supervised_finetuning - INFO - Epoch 483 - supervised_loss/batch: 0.8139\n",
            "2025-11-01 14:25:41,355 - supervised_finetuning - INFO - Epoch 483 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:25:41,355 - supervised_finetuning - INFO - Epoch 483 - learning_rate: 0.0001\n",
            "Classifier Epoch 24/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9462, Acc=62.50%]2025-11-01 14:25:41,394 - supervised_finetuning - INFO - Epoch 493 - supervised_loss/batch: 0.9462\n",
            "2025-11-01 14:25:41,394 - supervised_finetuning - INFO - Epoch 493 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:41,394 - supervised_finetuning - INFO - Epoch 493 - learning_rate: 0.0001\n",
            "Classifier Epoch 24/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.3943, Acc=100.00%]2025-11-01 14:25:41,447 - supervised_finetuning - INFO - Epoch 503 - supervised_loss/batch: 0.3943\n",
            "2025-11-01 14:25:41,447 - supervised_finetuning - INFO - Epoch 503 - supervised_accuracy/batch: 100.0000\n",
            "2025-11-01 14:25:41,447 - supervised_finetuning - INFO - Epoch 503 - learning_rate: 0.0001\n",
            "Classifier Epoch 24/100: 100% 21/21 [00:00<00:00, 213.02it/s, Loss=0.3943, Acc=100.00%]\n",
            "2025-11-01 14:25:41,465 - supervised_finetuning - INFO - Epoch 24:\n",
            "2025-11-01 14:25:41,465 - supervised_finetuning - INFO -   Train Loss: 0.8701, Train Acc: 63.95%\n",
            "2025-11-01 14:25:41,465 - supervised_finetuning - INFO -   Train F1: 63.30%, Train Recall: 63.95%\n",
            "2025-11-01 14:25:41,465 - supervised_finetuning - INFO -   Train Precision: 62.99%\n",
            "2025-11-01 14:25:41,465 - supervised_finetuning - INFO - Epoch 23 - supervised_loss/epoch: 0.8701\n",
            "2025-11-01 14:25:41,466 - supervised_finetuning - INFO - Epoch 23 - supervised_accuracy/epoch: 63.9472\n",
            "2025-11-01 14:25:41,466 - supervised_finetuning - INFO - Epoch 23 - supervised_f1/epoch: 63.3002\n",
            "2025-11-01 14:25:41,466 - supervised_finetuning - INFO - Epoch 23 - supervised_recall/epoch: 63.9472\n",
            "2025-11-01 14:25:41,467 - supervised_finetuning - INFO - Epoch 23 - supervised_precision/epoch: 62.9872\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:42,669 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 63.95%\n",
            "Classifier Epoch 25/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8532, Acc=54.69%]2025-11-01 14:25:42,687 - supervised_finetuning - INFO - Epoch 504 - supervised_loss/batch: 0.8532\n",
            "2025-11-01 14:25:42,687 - supervised_finetuning - INFO - Epoch 504 - supervised_accuracy/batch: 54.6875\n",
            "2025-11-01 14:25:42,691 - supervised_finetuning - INFO - Epoch 504 - learning_rate: 0.0001\n",
            "Classifier Epoch 25/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7985, Acc=70.31%]2025-11-01 14:25:42,749 - supervised_finetuning - INFO - Epoch 514 - supervised_loss/batch: 0.7985\n",
            "2025-11-01 14:25:42,749 - supervised_finetuning - INFO - Epoch 514 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:25:42,749 - supervised_finetuning - INFO - Epoch 514 - learning_rate: 0.0001\n",
            "Classifier Epoch 25/100:  76% 16/21 [00:00<00:00, 152.39it/s, Loss=0.7274, Acc=71.43%]2025-11-01 14:25:42,802 - supervised_finetuning - INFO - Epoch 524 - supervised_loss/batch: 0.7274\n",
            "2025-11-01 14:25:42,802 - supervised_finetuning - INFO - Epoch 524 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:42,803 - supervised_finetuning - INFO - Epoch 524 - learning_rate: 0.0001\n",
            "Classifier Epoch 25/100: 100% 21/21 [00:00<00:00, 158.11it/s, Loss=0.7274, Acc=71.43%]\n",
            "2025-11-01 14:25:42,828 - supervised_finetuning - INFO - Epoch 25:\n",
            "2025-11-01 14:25:42,828 - supervised_finetuning - INFO -   Train Loss: 0.8844, Train Acc: 63.33%\n",
            "2025-11-01 14:25:42,828 - supervised_finetuning - INFO -   Train F1: 62.74%, Train Recall: 63.33%\n",
            "2025-11-01 14:25:42,831 - supervised_finetuning - INFO -   Train Precision: 62.39%\n",
            "2025-11-01 14:25:42,831 - supervised_finetuning - INFO - Epoch 24 - supervised_loss/epoch: 0.8844\n",
            "2025-11-01 14:25:42,831 - supervised_finetuning - INFO - Epoch 24 - supervised_accuracy/epoch: 63.3256\n",
            "2025-11-01 14:25:42,832 - supervised_finetuning - INFO - Epoch 24 - supervised_f1/epoch: 62.7416\n",
            "2025-11-01 14:25:42,834 - supervised_finetuning - INFO - Epoch 24 - supervised_recall/epoch: 63.3256\n",
            "2025-11-01 14:25:42,834 - supervised_finetuning - INFO - Epoch 24 - supervised_precision/epoch: 62.3854\n",
            "Classifier Epoch 26/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9162, Acc=64.06%]2025-11-01 14:25:42,842 - supervised_finetuning - INFO - Epoch 525 - supervised_loss/batch: 0.9162\n",
            "2025-11-01 14:25:42,842 - supervised_finetuning - INFO - Epoch 525 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:25:42,842 - supervised_finetuning - INFO - Epoch 525 - learning_rate: 0.0001\n",
            "Classifier Epoch 26/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8931, Acc=64.06%]2025-11-01 14:25:42,895 - supervised_finetuning - INFO - Epoch 535 - supervised_loss/batch: 0.8931\n",
            "2025-11-01 14:25:42,895 - supervised_finetuning - INFO - Epoch 535 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:25:42,896 - supervised_finetuning - INFO - Epoch 535 - learning_rate: 0.0001\n",
            "Classifier Epoch 26/100:  90% 19/21 [00:00<00:00, 186.62it/s, Loss=0.8923, Acc=42.86%]2025-11-01 14:25:42,958 - supervised_finetuning - INFO - Epoch 545 - supervised_loss/batch: 0.8923\n",
            "2025-11-01 14:25:42,959 - supervised_finetuning - INFO - Epoch 545 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:25:42,959 - supervised_finetuning - INFO - Epoch 545 - learning_rate: 0.0001\n",
            "Classifier Epoch 26/100: 100% 21/21 [00:00<00:00, 167.18it/s, Loss=0.8923, Acc=42.86%]\n",
            "2025-11-01 14:25:42,990 - supervised_finetuning - INFO - Epoch 26:\n",
            "2025-11-01 14:25:42,990 - supervised_finetuning - INFO -   Train Loss: 0.8840, Train Acc: 64.02%\n",
            "2025-11-01 14:25:42,990 - supervised_finetuning - INFO -   Train F1: 63.31%, Train Recall: 64.02%\n",
            "2025-11-01 14:25:42,990 - supervised_finetuning - INFO -   Train Precision: 63.40%\n",
            "2025-11-01 14:25:42,990 - supervised_finetuning - INFO - Epoch 25 - supervised_loss/epoch: 0.8840\n",
            "2025-11-01 14:25:42,990 - supervised_finetuning - INFO - Epoch 25 - supervised_accuracy/epoch: 64.0249\n",
            "2025-11-01 14:25:42,991 - supervised_finetuning - INFO - Epoch 25 - supervised_f1/epoch: 63.3140\n",
            "2025-11-01 14:25:42,991 - supervised_finetuning - INFO - Epoch 25 - supervised_recall/epoch: 64.0249\n",
            "2025-11-01 14:25:42,991 - supervised_finetuning - INFO - Epoch 25 - supervised_precision/epoch: 63.3987\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:45,926 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 64.02%\n",
            "Classifier Epoch 27/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9492, Acc=60.94%]2025-11-01 14:25:45,937 - supervised_finetuning - INFO - Epoch 546 - supervised_loss/batch: 0.9492\n",
            "2025-11-01 14:25:45,939 - supervised_finetuning - INFO - Epoch 546 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:25:45,940 - supervised_finetuning - INFO - Epoch 546 - learning_rate: 0.0001\n",
            "Classifier Epoch 27/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7188, Acc=71.88%]2025-11-01 14:25:45,983 - supervised_finetuning - INFO - Epoch 556 - supervised_loss/batch: 0.7188\n",
            "2025-11-01 14:25:45,983 - supervised_finetuning - INFO - Epoch 556 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:25:45,984 - supervised_finetuning - INFO - Epoch 556 - learning_rate: 0.0001\n",
            "Classifier Epoch 27/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5305, Acc=100.00%]2025-11-01 14:25:46,020 - supervised_finetuning - INFO - Epoch 566 - supervised_loss/batch: 0.5305\n",
            "2025-11-01 14:25:46,020 - supervised_finetuning - INFO - Epoch 566 - supervised_accuracy/batch: 100.0000\n",
            "2025-11-01 14:25:46,020 - supervised_finetuning - INFO - Epoch 566 - learning_rate: 0.0001\n",
            "Classifier Epoch 27/100: 100% 21/21 [00:00<00:00, 223.86it/s, Loss=0.5305, Acc=100.00%]\n",
            "2025-11-01 14:25:46,034 - supervised_finetuning - INFO - Epoch 27:\n",
            "2025-11-01 14:25:46,034 - supervised_finetuning - INFO -   Train Loss: 0.8716, Train Acc: 63.79%\n",
            "2025-11-01 14:25:46,035 - supervised_finetuning - INFO -   Train F1: 63.11%, Train Recall: 63.79%\n",
            "2025-11-01 14:25:46,035 - supervised_finetuning - INFO -   Train Precision: 62.86%\n",
            "2025-11-01 14:25:46,035 - supervised_finetuning - INFO - Epoch 26 - supervised_loss/epoch: 0.8716\n",
            "2025-11-01 14:25:46,035 - supervised_finetuning - INFO - Epoch 26 - supervised_accuracy/epoch: 63.7918\n",
            "2025-11-01 14:25:46,035 - supervised_finetuning - INFO - Epoch 26 - supervised_f1/epoch: 63.1131\n",
            "2025-11-01 14:25:46,035 - supervised_finetuning - INFO - Epoch 26 - supervised_recall/epoch: 63.7918\n",
            "2025-11-01 14:25:46,036 - supervised_finetuning - INFO - Epoch 26 - supervised_precision/epoch: 62.8567\n",
            "Classifier Epoch 28/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8725, Acc=62.50%]2025-11-01 14:25:46,040 - supervised_finetuning - INFO - Epoch 567 - supervised_loss/batch: 0.8725\n",
            "2025-11-01 14:25:46,040 - supervised_finetuning - INFO - Epoch 567 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:46,041 - supervised_finetuning - INFO - Epoch 567 - learning_rate: 0.0001\n",
            "Classifier Epoch 28/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7717, Acc=76.56%]2025-11-01 14:25:46,077 - supervised_finetuning - INFO - Epoch 577 - supervised_loss/batch: 0.7717\n",
            "2025-11-01 14:25:46,077 - supervised_finetuning - INFO - Epoch 577 - supervised_accuracy/batch: 76.5625\n",
            "2025-11-01 14:25:46,078 - supervised_finetuning - INFO - Epoch 577 - learning_rate: 0.0001\n",
            "Classifier Epoch 28/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5959, Acc=71.43%]2025-11-01 14:25:46,114 - supervised_finetuning - INFO - Epoch 587 - supervised_loss/batch: 0.5959\n",
            "2025-11-01 14:25:46,115 - supervised_finetuning - INFO - Epoch 587 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:46,115 - supervised_finetuning - INFO - Epoch 587 - learning_rate: 0.0001\n",
            "Classifier Epoch 28/100: 100% 21/21 [00:00<00:00, 265.38it/s, Loss=0.5959, Acc=71.43%]\n",
            "2025-11-01 14:25:46,126 - supervised_finetuning - INFO - Epoch 28:\n",
            "2025-11-01 14:25:46,126 - supervised_finetuning - INFO -   Train Loss: 0.8689, Train Acc: 65.27%\n",
            "2025-11-01 14:25:46,126 - supervised_finetuning - INFO -   Train F1: 64.63%, Train Recall: 65.27%\n",
            "2025-11-01 14:25:46,126 - supervised_finetuning - INFO -   Train Precision: 64.55%\n",
            "2025-11-01 14:25:46,126 - supervised_finetuning - INFO - Epoch 27 - supervised_loss/epoch: 0.8689\n",
            "2025-11-01 14:25:46,127 - supervised_finetuning - INFO - Epoch 27 - supervised_accuracy/epoch: 65.2681\n",
            "2025-11-01 14:25:46,127 - supervised_finetuning - INFO - Epoch 27 - supervised_f1/epoch: 64.6274\n",
            "2025-11-01 14:25:46,127 - supervised_finetuning - INFO - Epoch 27 - supervised_recall/epoch: 65.2681\n",
            "2025-11-01 14:25:46,127 - supervised_finetuning - INFO - Epoch 27 - supervised_precision/epoch: 64.5451\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:47,166 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 65.27%\n",
            "Classifier Epoch 29/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7776, Acc=64.06%]2025-11-01 14:25:47,174 - supervised_finetuning - INFO - Epoch 588 - supervised_loss/batch: 0.7776\n",
            "2025-11-01 14:25:47,174 - supervised_finetuning - INFO - Epoch 588 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:25:47,174 - supervised_finetuning - INFO - Epoch 588 - learning_rate: 0.0001\n",
            "Classifier Epoch 29/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8994, Acc=60.94%]2025-11-01 14:25:47,233 - supervised_finetuning - INFO - Epoch 598 - supervised_loss/batch: 0.8994\n",
            "2025-11-01 14:25:47,233 - supervised_finetuning - INFO - Epoch 598 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:25:47,234 - supervised_finetuning - INFO - Epoch 598 - learning_rate: 0.0001\n",
            "Classifier Epoch 29/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9041, Acc=71.43%]2025-11-01 14:25:47,270 - supervised_finetuning - INFO - Epoch 608 - supervised_loss/batch: 0.9041\n",
            "2025-11-01 14:25:47,270 - supervised_finetuning - INFO - Epoch 608 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:47,270 - supervised_finetuning - INFO - Epoch 608 - learning_rate: 0.0001\n",
            "Classifier Epoch 29/100: 100% 21/21 [00:00<00:00, 201.91it/s, Loss=0.9041, Acc=71.43%]\n",
            "2025-11-01 14:25:47,282 - supervised_finetuning - INFO - Epoch 29:\n",
            "2025-11-01 14:25:47,282 - supervised_finetuning - INFO -   Train Loss: 0.8674, Train Acc: 64.49%\n",
            "2025-11-01 14:25:47,282 - supervised_finetuning - INFO -   Train F1: 63.89%, Train Recall: 64.49%\n",
            "2025-11-01 14:25:47,282 - supervised_finetuning - INFO -   Train Precision: 63.67%\n",
            "2025-11-01 14:25:47,282 - supervised_finetuning - INFO - Epoch 28 - supervised_loss/epoch: 0.8674\n",
            "2025-11-01 14:25:47,282 - supervised_finetuning - INFO - Epoch 28 - supervised_accuracy/epoch: 64.4911\n",
            "2025-11-01 14:25:47,283 - supervised_finetuning - INFO - Epoch 28 - supervised_f1/epoch: 63.8944\n",
            "2025-11-01 14:25:47,283 - supervised_finetuning - INFO - Epoch 28 - supervised_recall/epoch: 64.4911\n",
            "2025-11-01 14:25:47,283 - supervised_finetuning - INFO - Epoch 28 - supervised_precision/epoch: 63.6704\n",
            "Classifier Epoch 30/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7831, Acc=70.31%]2025-11-01 14:25:47,288 - supervised_finetuning - INFO - Epoch 609 - supervised_loss/batch: 0.7831\n",
            "2025-11-01 14:25:47,288 - supervised_finetuning - INFO - Epoch 609 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:25:47,288 - supervised_finetuning - INFO - Epoch 609 - learning_rate: 0.0001\n",
            "Classifier Epoch 30/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8261, Acc=64.06%]2025-11-01 14:25:47,326 - supervised_finetuning - INFO - Epoch 619 - supervised_loss/batch: 0.8261\n",
            "2025-11-01 14:25:47,326 - supervised_finetuning - INFO - Epoch 619 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:25:47,326 - supervised_finetuning - INFO - Epoch 619 - learning_rate: 0.0001\n",
            "Classifier Epoch 30/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8373, Acc=57.14%]2025-11-01 14:25:47,365 - supervised_finetuning - INFO - Epoch 629 - supervised_loss/batch: 0.8373\n",
            "2025-11-01 14:25:47,365 - supervised_finetuning - INFO - Epoch 629 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:47,366 - supervised_finetuning - INFO - Epoch 629 - learning_rate: 0.0001\n",
            "Classifier Epoch 30/100: 100% 21/21 [00:00<00:00, 254.76it/s, Loss=0.8373, Acc=57.14%]\n",
            "2025-11-01 14:25:47,376 - supervised_finetuning - INFO - Epoch 30:\n",
            "2025-11-01 14:25:47,376 - supervised_finetuning - INFO -   Train Loss: 0.8605, Train Acc: 64.80%\n",
            "2025-11-01 14:25:47,376 - supervised_finetuning - INFO -   Train F1: 64.21%, Train Recall: 64.80%\n",
            "2025-11-01 14:25:47,376 - supervised_finetuning - INFO -   Train Precision: 63.96%\n",
            "2025-11-01 14:25:47,376 - supervised_finetuning - INFO - Epoch 29 - supervised_loss/epoch: 0.8605\n",
            "2025-11-01 14:25:47,376 - supervised_finetuning - INFO - Epoch 29 - supervised_accuracy/epoch: 64.8019\n",
            "2025-11-01 14:25:47,377 - supervised_finetuning - INFO - Epoch 29 - supervised_f1/epoch: 64.2110\n",
            "2025-11-01 14:25:47,377 - supervised_finetuning - INFO - Epoch 29 - supervised_recall/epoch: 64.8019\n",
            "2025-11-01 14:25:47,377 - supervised_finetuning - INFO - Epoch 29 - supervised_precision/epoch: 63.9553\n",
            "Classifier Epoch 31/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8148, Acc=70.31%]2025-11-01 14:25:47,382 - supervised_finetuning - INFO - Epoch 630 - supervised_loss/batch: 0.8148\n",
            "2025-11-01 14:25:47,382 - supervised_finetuning - INFO - Epoch 630 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:25:47,382 - supervised_finetuning - INFO - Epoch 630 - learning_rate: 0.0001\n",
            "Classifier Epoch 31/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9012, Acc=62.50%]2025-11-01 14:25:47,421 - supervised_finetuning - INFO - Epoch 640 - supervised_loss/batch: 0.9012\n",
            "2025-11-01 14:25:47,421 - supervised_finetuning - INFO - Epoch 640 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:47,421 - supervised_finetuning - INFO - Epoch 640 - learning_rate: 0.0001\n",
            "Classifier Epoch 31/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7214, Acc=57.14%]2025-11-01 14:25:47,465 - supervised_finetuning - INFO - Epoch 650 - supervised_loss/batch: 0.7214\n",
            "2025-11-01 14:25:47,465 - supervised_finetuning - INFO - Epoch 650 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:47,465 - supervised_finetuning - INFO - Epoch 650 - learning_rate: 0.0001\n",
            "Classifier Epoch 31/100: 100% 21/21 [00:00<00:00, 239.06it/s, Loss=0.7214, Acc=57.14%]\n",
            "2025-11-01 14:25:47,475 - supervised_finetuning - INFO - Epoch 31:\n",
            "2025-11-01 14:25:47,475 - supervised_finetuning - INFO -   Train Loss: 0.8561, Train Acc: 63.64%\n",
            "2025-11-01 14:25:47,476 - supervised_finetuning - INFO -   Train F1: 62.82%, Train Recall: 63.64%\n",
            "2025-11-01 14:25:47,476 - supervised_finetuning - INFO -   Train Precision: 63.16%\n",
            "2025-11-01 14:25:47,476 - supervised_finetuning - INFO - Epoch 30 - supervised_loss/epoch: 0.8561\n",
            "2025-11-01 14:25:47,476 - supervised_finetuning - INFO - Epoch 30 - supervised_accuracy/epoch: 63.6364\n",
            "2025-11-01 14:25:47,476 - supervised_finetuning - INFO - Epoch 30 - supervised_f1/epoch: 62.8237\n",
            "2025-11-01 14:25:47,476 - supervised_finetuning - INFO - Epoch 30 - supervised_recall/epoch: 63.6364\n",
            "2025-11-01 14:25:47,477 - supervised_finetuning - INFO - Epoch 30 - supervised_precision/epoch: 63.1640\n",
            "Classifier Epoch 32/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9861, Acc=65.62%]2025-11-01 14:25:47,481 - supervised_finetuning - INFO - Epoch 651 - supervised_loss/batch: 0.9861\n",
            "2025-11-01 14:25:47,481 - supervised_finetuning - INFO - Epoch 651 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:25:47,482 - supervised_finetuning - INFO - Epoch 651 - learning_rate: 0.0001\n",
            "Classifier Epoch 32/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8488, Acc=67.19%]2025-11-01 14:25:47,519 - supervised_finetuning - INFO - Epoch 661 - supervised_loss/batch: 0.8488\n",
            "2025-11-01 14:25:47,519 - supervised_finetuning - INFO - Epoch 661 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:25:47,519 - supervised_finetuning - INFO - Epoch 661 - learning_rate: 0.0001\n",
            "Classifier Epoch 32/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7322, Acc=71.43%]2025-11-01 14:25:47,558 - supervised_finetuning - INFO - Epoch 671 - supervised_loss/batch: 0.7322\n",
            "2025-11-01 14:25:47,558 - supervised_finetuning - INFO - Epoch 671 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:47,558 - supervised_finetuning - INFO - Epoch 671 - learning_rate: 0.0001\n",
            "Classifier Epoch 32/100: 100% 21/21 [00:00<00:00, 257.48it/s, Loss=0.7322, Acc=71.43%]\n",
            "2025-11-01 14:25:47,569 - supervised_finetuning - INFO - Epoch 32:\n",
            "2025-11-01 14:25:47,569 - supervised_finetuning - INFO -   Train Loss: 0.8591, Train Acc: 64.10%\n",
            "2025-11-01 14:25:47,569 - supervised_finetuning - INFO -   Train F1: 63.57%, Train Recall: 64.10%\n",
            "2025-11-01 14:25:47,569 - supervised_finetuning - INFO -   Train Precision: 63.34%\n",
            "2025-11-01 14:25:47,569 - supervised_finetuning - INFO - Epoch 31 - supervised_loss/epoch: 0.8591\n",
            "2025-11-01 14:25:47,570 - supervised_finetuning - INFO - Epoch 31 - supervised_accuracy/epoch: 64.1026\n",
            "2025-11-01 14:25:47,570 - supervised_finetuning - INFO - Epoch 31 - supervised_f1/epoch: 63.5660\n",
            "2025-11-01 14:25:47,570 - supervised_finetuning - INFO - Epoch 31 - supervised_recall/epoch: 64.1026\n",
            "2025-11-01 14:25:47,570 - supervised_finetuning - INFO - Epoch 31 - supervised_precision/epoch: 63.3375\n",
            "Classifier Epoch 33/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8249, Acc=65.62%]2025-11-01 14:25:47,575 - supervised_finetuning - INFO - Epoch 672 - supervised_loss/batch: 0.8249\n",
            "2025-11-01 14:25:47,575 - supervised_finetuning - INFO - Epoch 672 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:25:47,575 - supervised_finetuning - INFO - Epoch 672 - learning_rate: 0.0001\n",
            "Classifier Epoch 33/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8556, Acc=71.88%]2025-11-01 14:25:47,613 - supervised_finetuning - INFO - Epoch 682 - supervised_loss/batch: 0.8556\n",
            "2025-11-01 14:25:47,614 - supervised_finetuning - INFO - Epoch 682 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:25:47,614 - supervised_finetuning - INFO - Epoch 682 - learning_rate: 0.0001\n",
            "Classifier Epoch 33/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9822, Acc=57.14%]2025-11-01 14:25:47,658 - supervised_finetuning - INFO - Epoch 692 - supervised_loss/batch: 0.9822\n",
            "2025-11-01 14:25:47,658 - supervised_finetuning - INFO - Epoch 692 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:47,658 - supervised_finetuning - INFO - Epoch 692 - learning_rate: 0.0001\n",
            "Classifier Epoch 33/100: 100% 21/21 [00:00<00:00, 239.30it/s, Loss=0.9822, Acc=57.14%]\n",
            "2025-11-01 14:25:47,670 - supervised_finetuning - INFO - Epoch 33:\n",
            "2025-11-01 14:25:47,670 - supervised_finetuning - INFO -   Train Loss: 0.8576, Train Acc: 65.11%\n",
            "2025-11-01 14:25:47,670 - supervised_finetuning - INFO -   Train F1: 64.52%, Train Recall: 65.11%\n",
            "2025-11-01 14:25:47,670 - supervised_finetuning - INFO -   Train Precision: 64.19%\n",
            "2025-11-01 14:25:47,670 - supervised_finetuning - INFO - Epoch 32 - supervised_loss/epoch: 0.8576\n",
            "2025-11-01 14:25:47,670 - supervised_finetuning - INFO - Epoch 32 - supervised_accuracy/epoch: 65.1127\n",
            "2025-11-01 14:25:47,671 - supervised_finetuning - INFO - Epoch 32 - supervised_f1/epoch: 64.5156\n",
            "2025-11-01 14:25:47,671 - supervised_finetuning - INFO - Epoch 32 - supervised_recall/epoch: 65.1127\n",
            "2025-11-01 14:25:47,671 - supervised_finetuning - INFO - Epoch 32 - supervised_precision/epoch: 64.1936\n",
            "Classifier Epoch 34/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7647, Acc=75.00%]2025-11-01 14:25:47,676 - supervised_finetuning - INFO - Epoch 693 - supervised_loss/batch: 0.7647\n",
            "2025-11-01 14:25:47,676 - supervised_finetuning - INFO - Epoch 693 - supervised_accuracy/batch: 75.0000\n",
            "2025-11-01 14:25:47,676 - supervised_finetuning - INFO - Epoch 693 - learning_rate: 0.0001\n",
            "Classifier Epoch 34/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6517, Acc=75.00%]2025-11-01 14:25:47,715 - supervised_finetuning - INFO - Epoch 703 - supervised_loss/batch: 0.6517\n",
            "2025-11-01 14:25:47,715 - supervised_finetuning - INFO - Epoch 703 - supervised_accuracy/batch: 75.0000\n",
            "2025-11-01 14:25:47,715 - supervised_finetuning - INFO - Epoch 703 - learning_rate: 0.0001\n",
            "Classifier Epoch 34/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8395, Acc=57.14%]2025-11-01 14:25:47,754 - supervised_finetuning - INFO - Epoch 713 - supervised_loss/batch: 0.8395\n",
            "2025-11-01 14:25:47,754 - supervised_finetuning - INFO - Epoch 713 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:47,754 - supervised_finetuning - INFO - Epoch 713 - learning_rate: 0.0001\n",
            "Classifier Epoch 34/100: 100% 21/21 [00:00<00:00, 253.26it/s, Loss=0.8395, Acc=57.14%]\n",
            "2025-11-01 14:25:47,766 - supervised_finetuning - INFO - Epoch 34:\n",
            "2025-11-01 14:25:47,766 - supervised_finetuning - INFO -   Train Loss: 0.8722, Train Acc: 62.78%\n",
            "2025-11-01 14:25:47,766 - supervised_finetuning - INFO -   Train F1: 61.81%, Train Recall: 62.78%\n",
            "2025-11-01 14:25:47,766 - supervised_finetuning - INFO -   Train Precision: 62.11%\n",
            "2025-11-01 14:25:47,766 - supervised_finetuning - INFO - Epoch 33 - supervised_loss/epoch: 0.8722\n",
            "2025-11-01 14:25:47,767 - supervised_finetuning - INFO - Epoch 33 - supervised_accuracy/epoch: 62.7817\n",
            "2025-11-01 14:25:47,767 - supervised_finetuning - INFO - Epoch 33 - supervised_f1/epoch: 61.8102\n",
            "2025-11-01 14:25:47,767 - supervised_finetuning - INFO - Epoch 33 - supervised_recall/epoch: 62.7817\n",
            "2025-11-01 14:25:47,767 - supervised_finetuning - INFO - Epoch 33 - supervised_precision/epoch: 62.1085\n",
            "Classifier Epoch 35/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7107, Acc=71.88%]2025-11-01 14:25:47,772 - supervised_finetuning - INFO - Epoch 714 - supervised_loss/batch: 0.7107\n",
            "2025-11-01 14:25:47,772 - supervised_finetuning - INFO - Epoch 714 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:25:47,772 - supervised_finetuning - INFO - Epoch 714 - learning_rate: 0.0001\n",
            "Classifier Epoch 35/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8694, Acc=64.06%]2025-11-01 14:25:47,809 - supervised_finetuning - INFO - Epoch 724 - supervised_loss/batch: 0.8694\n",
            "2025-11-01 14:25:47,810 - supervised_finetuning - INFO - Epoch 724 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:25:47,810 - supervised_finetuning - INFO - Epoch 724 - learning_rate: 0.0001\n",
            "Classifier Epoch 35/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.6311, Acc=42.86%]2025-11-01 14:25:47,859 - supervised_finetuning - INFO - Epoch 734 - supervised_loss/batch: 1.6311\n",
            "2025-11-01 14:25:47,860 - supervised_finetuning - INFO - Epoch 734 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:25:47,860 - supervised_finetuning - INFO - Epoch 734 - learning_rate: 0.0001\n",
            "Classifier Epoch 35/100: 100% 21/21 [00:00<00:00, 226.37it/s, Loss=1.6311, Acc=42.86%]\n",
            "2025-11-01 14:25:47,870 - supervised_finetuning - INFO - Epoch 35:\n",
            "2025-11-01 14:25:47,870 - supervised_finetuning - INFO -   Train Loss: 0.8874, Train Acc: 64.18%\n",
            "2025-11-01 14:25:47,870 - supervised_finetuning - INFO -   Train F1: 63.60%, Train Recall: 64.18%\n",
            "2025-11-01 14:25:47,870 - supervised_finetuning - INFO -   Train Precision: 63.49%\n",
            "2025-11-01 14:25:47,871 - supervised_finetuning - INFO - Epoch 34 - supervised_loss/epoch: 0.8874\n",
            "2025-11-01 14:25:47,871 - supervised_finetuning - INFO - Epoch 34 - supervised_accuracy/epoch: 64.1803\n",
            "2025-11-01 14:25:47,871 - supervised_finetuning - INFO - Epoch 34 - supervised_f1/epoch: 63.5988\n",
            "2025-11-01 14:25:47,871 - supervised_finetuning - INFO - Epoch 34 - supervised_recall/epoch: 64.1803\n",
            "2025-11-01 14:25:47,871 - supervised_finetuning - INFO - Epoch 34 - supervised_precision/epoch: 63.4860\n",
            "Classifier Epoch 36/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0030, Acc=51.56%]2025-11-01 14:25:47,876 - supervised_finetuning - INFO - Epoch 735 - supervised_loss/batch: 1.0030\n",
            "2025-11-01 14:25:47,876 - supervised_finetuning - INFO - Epoch 735 - supervised_accuracy/batch: 51.5625\n",
            "2025-11-01 14:25:47,876 - supervised_finetuning - INFO - Epoch 735 - learning_rate: 0.0001\n",
            "Classifier Epoch 36/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9213, Acc=65.62%]2025-11-01 14:25:47,913 - supervised_finetuning - INFO - Epoch 745 - supervised_loss/batch: 0.9213\n",
            "2025-11-01 14:25:47,913 - supervised_finetuning - INFO - Epoch 745 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:25:47,913 - supervised_finetuning - INFO - Epoch 745 - learning_rate: 0.0001\n",
            "Classifier Epoch 36/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.2988, Acc=100.00%]2025-11-01 14:25:47,956 - supervised_finetuning - INFO - Epoch 755 - supervised_loss/batch: 0.2988\n",
            "2025-11-01 14:25:47,956 - supervised_finetuning - INFO - Epoch 755 - supervised_accuracy/batch: 100.0000\n",
            "2025-11-01 14:25:47,957 - supervised_finetuning - INFO - Epoch 755 - learning_rate: 0.0001\n",
            "Classifier Epoch 36/100: 100% 21/21 [00:00<00:00, 246.63it/s, Loss=0.2988, Acc=100.00%]\n",
            "2025-11-01 14:25:47,968 - supervised_finetuning - INFO - Epoch 36:\n",
            "2025-11-01 14:25:47,968 - supervised_finetuning - INFO -   Train Loss: 0.8293, Train Acc: 65.66%\n",
            "2025-11-01 14:25:47,968 - supervised_finetuning - INFO -   Train F1: 65.22%, Train Recall: 65.66%\n",
            "2025-11-01 14:25:47,968 - supervised_finetuning - INFO -   Train Precision: 65.05%\n",
            "2025-11-01 14:25:47,968 - supervised_finetuning - INFO - Epoch 35 - supervised_loss/epoch: 0.8293\n",
            "2025-11-01 14:25:47,969 - supervised_finetuning - INFO - Epoch 35 - supervised_accuracy/epoch: 65.6566\n",
            "2025-11-01 14:25:47,969 - supervised_finetuning - INFO - Epoch 35 - supervised_f1/epoch: 65.2151\n",
            "2025-11-01 14:25:47,969 - supervised_finetuning - INFO - Epoch 35 - supervised_recall/epoch: 65.6566\n",
            "2025-11-01 14:25:47,969 - supervised_finetuning - INFO - Epoch 35 - supervised_precision/epoch: 65.0495\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:53,788 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 65.66%\n",
            "Classifier Epoch 37/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6152, Acc=75.00%]2025-11-01 14:25:53,796 - supervised_finetuning - INFO - Epoch 756 - supervised_loss/batch: 0.6152\n",
            "2025-11-01 14:25:53,797 - supervised_finetuning - INFO - Epoch 756 - supervised_accuracy/batch: 75.0000\n",
            "2025-11-01 14:25:53,797 - supervised_finetuning - INFO - Epoch 756 - learning_rate: 0.0001\n",
            "Classifier Epoch 37/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8542, Acc=67.19%]2025-11-01 14:25:53,835 - supervised_finetuning - INFO - Epoch 766 - supervised_loss/batch: 0.8542\n",
            "2025-11-01 14:25:53,835 - supervised_finetuning - INFO - Epoch 766 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:25:53,836 - supervised_finetuning - INFO - Epoch 766 - learning_rate: 0.0001\n",
            "Classifier Epoch 37/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.3733, Acc=100.00%]2025-11-01 14:25:53,876 - supervised_finetuning - INFO - Epoch 776 - supervised_loss/batch: 0.3733\n",
            "2025-11-01 14:25:53,876 - supervised_finetuning - INFO - Epoch 776 - supervised_accuracy/batch: 100.0000\n",
            "2025-11-01 14:25:53,877 - supervised_finetuning - INFO - Epoch 776 - learning_rate: 0.0001\n",
            "Classifier Epoch 37/100: 100% 21/21 [00:00<00:00, 238.39it/s, Loss=0.3733, Acc=100.00%]\n",
            "2025-11-01 14:25:53,888 - supervised_finetuning - INFO - Epoch 37:\n",
            "2025-11-01 14:25:53,888 - supervised_finetuning - INFO -   Train Loss: 0.8198, Train Acc: 66.51%\n",
            "2025-11-01 14:25:53,888 - supervised_finetuning - INFO -   Train F1: 66.11%, Train Recall: 66.51%\n",
            "2025-11-01 14:25:53,889 - supervised_finetuning - INFO -   Train Precision: 65.89%\n",
            "2025-11-01 14:25:53,889 - supervised_finetuning - INFO - Epoch 36 - supervised_loss/epoch: 0.8198\n",
            "2025-11-01 14:25:53,889 - supervised_finetuning - INFO - Epoch 36 - supervised_accuracy/epoch: 66.5113\n",
            "2025-11-01 14:25:53,889 - supervised_finetuning - INFO - Epoch 36 - supervised_f1/epoch: 66.1096\n",
            "2025-11-01 14:25:53,889 - supervised_finetuning - INFO - Epoch 36 - supervised_recall/epoch: 66.5113\n",
            "2025-11-01 14:25:53,890 - supervised_finetuning - INFO - Epoch 36 - supervised_precision/epoch: 65.8870\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:25:55,024 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 66.51%\n",
            "Classifier Epoch 38/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7663, Acc=71.88%]2025-11-01 14:25:55,034 - supervised_finetuning - INFO - Epoch 777 - supervised_loss/batch: 0.7663\n",
            "2025-11-01 14:25:55,034 - supervised_finetuning - INFO - Epoch 777 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:25:55,034 - supervised_finetuning - INFO - Epoch 777 - learning_rate: 0.0001\n",
            "Classifier Epoch 38/100:  48% 10/21 [00:00<00:00, 95.31it/s, Loss=0.7770, Acc=71.88%]2025-11-01 14:25:55,136 - supervised_finetuning - INFO - Epoch 787 - supervised_loss/batch: 0.7770\n",
            "2025-11-01 14:25:55,136 - supervised_finetuning - INFO - Epoch 787 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:25:55,136 - supervised_finetuning - INFO - Epoch 787 - learning_rate: 0.0001\n",
            "Classifier Epoch 38/100:  48% 10/21 [00:00<00:00, 95.31it/s, Loss=1.4789, Acc=57.14%]2025-11-01 14:25:55,191 - supervised_finetuning - INFO - Epoch 797 - supervised_loss/batch: 1.4789\n",
            "2025-11-01 14:25:55,191 - supervised_finetuning - INFO - Epoch 797 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:55,193 - supervised_finetuning - INFO - Epoch 797 - learning_rate: 0.0001\n",
            "Classifier Epoch 38/100: 100% 21/21 [00:00<00:00, 124.86it/s, Loss=1.4789, Acc=57.14%]\n",
            "2025-11-01 14:25:55,212 - supervised_finetuning - INFO - Epoch 38:\n",
            "2025-11-01 14:25:55,212 - supervised_finetuning - INFO -   Train Loss: 0.8698, Train Acc: 64.96%\n",
            "2025-11-01 14:25:55,212 - supervised_finetuning - INFO -   Train F1: 64.40%, Train Recall: 64.96%\n",
            "2025-11-01 14:25:55,212 - supervised_finetuning - INFO -   Train Precision: 64.56%\n",
            "2025-11-01 14:25:55,213 - supervised_finetuning - INFO - Epoch 37 - supervised_loss/epoch: 0.8698\n",
            "2025-11-01 14:25:55,213 - supervised_finetuning - INFO - Epoch 37 - supervised_accuracy/epoch: 64.9573\n",
            "2025-11-01 14:25:55,213 - supervised_finetuning - INFO - Epoch 37 - supervised_f1/epoch: 64.3958\n",
            "2025-11-01 14:25:55,213 - supervised_finetuning - INFO - Epoch 37 - supervised_recall/epoch: 64.9573\n",
            "2025-11-01 14:25:55,213 - supervised_finetuning - INFO - Epoch 37 - supervised_precision/epoch: 64.5643\n",
            "Classifier Epoch 39/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8433, Acc=60.94%]2025-11-01 14:25:55,220 - supervised_finetuning - INFO - Epoch 798 - supervised_loss/batch: 0.8433\n",
            "2025-11-01 14:25:55,221 - supervised_finetuning - INFO - Epoch 798 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:25:55,221 - supervised_finetuning - INFO - Epoch 798 - learning_rate: 0.0001\n",
            "Classifier Epoch 39/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7086, Acc=76.56%]2025-11-01 14:25:55,271 - supervised_finetuning - INFO - Epoch 808 - supervised_loss/batch: 0.7086\n",
            "2025-11-01 14:25:55,271 - supervised_finetuning - INFO - Epoch 808 - supervised_accuracy/batch: 76.5625\n",
            "2025-11-01 14:25:55,271 - supervised_finetuning - INFO - Epoch 808 - learning_rate: 0.0001\n",
            "Classifier Epoch 39/100:  81% 17/21 [00:00<00:00, 165.90it/s, Loss=0.9200, Acc=71.43%]2025-11-01 14:25:55,334 - supervised_finetuning - INFO - Epoch 818 - supervised_loss/batch: 0.9200\n",
            "2025-11-01 14:25:55,334 - supervised_finetuning - INFO - Epoch 818 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:55,334 - supervised_finetuning - INFO - Epoch 818 - learning_rate: 0.0001\n",
            "Classifier Epoch 39/100: 100% 21/21 [00:00<00:00, 173.16it/s, Loss=0.9200, Acc=71.43%]\n",
            "2025-11-01 14:25:55,352 - supervised_finetuning - INFO - Epoch 39:\n",
            "2025-11-01 14:25:55,352 - supervised_finetuning - INFO -   Train Loss: 0.8430, Train Acc: 65.73%\n",
            "2025-11-01 14:25:55,352 - supervised_finetuning - INFO -   Train F1: 65.08%, Train Recall: 65.73%\n",
            "2025-11-01 14:25:55,352 - supervised_finetuning - INFO -   Train Precision: 64.85%\n",
            "2025-11-01 14:25:55,353 - supervised_finetuning - INFO - Epoch 38 - supervised_loss/epoch: 0.8430\n",
            "2025-11-01 14:25:55,353 - supervised_finetuning - INFO - Epoch 38 - supervised_accuracy/epoch: 65.7343\n",
            "2025-11-01 14:25:55,353 - supervised_finetuning - INFO - Epoch 38 - supervised_f1/epoch: 65.0796\n",
            "2025-11-01 14:25:55,353 - supervised_finetuning - INFO - Epoch 38 - supervised_recall/epoch: 65.7343\n",
            "2025-11-01 14:25:55,354 - supervised_finetuning - INFO - Epoch 38 - supervised_precision/epoch: 64.8468\n",
            "Classifier Epoch 40/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7668, Acc=67.19%]2025-11-01 14:25:55,360 - supervised_finetuning - INFO - Epoch 819 - supervised_loss/batch: 0.7668\n",
            "2025-11-01 14:25:55,361 - supervised_finetuning - INFO - Epoch 819 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:25:55,361 - supervised_finetuning - INFO - Epoch 819 - learning_rate: 0.0001\n",
            "Classifier Epoch 40/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8939, Acc=59.38%]2025-11-01 14:25:55,410 - supervised_finetuning - INFO - Epoch 829 - supervised_loss/batch: 0.8939\n",
            "2025-11-01 14:25:55,410 - supervised_finetuning - INFO - Epoch 829 - supervised_accuracy/batch: 59.3750\n",
            "2025-11-01 14:25:55,410 - supervised_finetuning - INFO - Epoch 829 - learning_rate: 0.0001\n",
            "Classifier Epoch 40/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9219, Acc=57.14%]2025-11-01 14:25:55,456 - supervised_finetuning - INFO - Epoch 839 - supervised_loss/batch: 0.9219\n",
            "2025-11-01 14:25:55,457 - supervised_finetuning - INFO - Epoch 839 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:55,457 - supervised_finetuning - INFO - Epoch 839 - learning_rate: 0.0001\n",
            "Classifier Epoch 40/100: 100% 21/21 [00:00<00:00, 203.29it/s, Loss=0.9219, Acc=57.14%]\n",
            "2025-11-01 14:25:55,475 - supervised_finetuning - INFO - Epoch 40:\n",
            "2025-11-01 14:25:55,475 - supervised_finetuning - INFO -   Train Loss: 0.8467, Train Acc: 66.51%\n",
            "2025-11-01 14:25:55,475 - supervised_finetuning - INFO -   Train F1: 65.82%, Train Recall: 66.51%\n",
            "2025-11-01 14:25:55,475 - supervised_finetuning - INFO -   Train Precision: 65.52%\n",
            "2025-11-01 14:25:55,476 - supervised_finetuning - INFO - Epoch 39 - supervised_loss/epoch: 0.8467\n",
            "2025-11-01 14:25:55,476 - supervised_finetuning - INFO - Epoch 39 - supervised_accuracy/epoch: 66.5113\n",
            "2025-11-01 14:25:55,476 - supervised_finetuning - INFO - Epoch 39 - supervised_f1/epoch: 65.8154\n",
            "2025-11-01 14:25:55,476 - supervised_finetuning - INFO - Epoch 39 - supervised_recall/epoch: 66.5113\n",
            "2025-11-01 14:25:55,476 - supervised_finetuning - INFO - Epoch 39 - supervised_precision/epoch: 65.5216\n",
            "Classifier Epoch 41/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8944, Acc=68.75%]2025-11-01 14:25:55,485 - supervised_finetuning - INFO - Epoch 840 - supervised_loss/batch: 0.8944\n",
            "2025-11-01 14:25:55,485 - supervised_finetuning - INFO - Epoch 840 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:25:55,486 - supervised_finetuning - INFO - Epoch 840 - learning_rate: 0.0001\n",
            "Classifier Epoch 41/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8282, Acc=59.38%]2025-11-01 14:25:55,530 - supervised_finetuning - INFO - Epoch 850 - supervised_loss/batch: 0.8282\n",
            "2025-11-01 14:25:55,531 - supervised_finetuning - INFO - Epoch 850 - supervised_accuracy/batch: 59.3750\n",
            "2025-11-01 14:25:55,531 - supervised_finetuning - INFO - Epoch 850 - learning_rate: 0.0001\n",
            "Classifier Epoch 41/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7652, Acc=57.14%]2025-11-01 14:25:55,581 - supervised_finetuning - INFO - Epoch 860 - supervised_loss/batch: 0.7652\n",
            "2025-11-01 14:25:55,581 - supervised_finetuning - INFO - Epoch 860 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:55,582 - supervised_finetuning - INFO - Epoch 860 - learning_rate: 0.0001\n",
            "Classifier Epoch 41/100: 100% 21/21 [00:00<00:00, 200.18it/s, Loss=0.7652, Acc=57.14%]\n",
            "2025-11-01 14:25:55,602 - supervised_finetuning - INFO - Epoch 41:\n",
            "2025-11-01 14:25:55,602 - supervised_finetuning - INFO -   Train Loss: 0.8361, Train Acc: 65.11%\n",
            "2025-11-01 14:25:55,602 - supervised_finetuning - INFO -   Train F1: 64.48%, Train Recall: 65.11%\n",
            "2025-11-01 14:25:55,602 - supervised_finetuning - INFO -   Train Precision: 64.65%\n",
            "2025-11-01 14:25:55,603 - supervised_finetuning - INFO - Epoch 40 - supervised_loss/epoch: 0.8361\n",
            "2025-11-01 14:25:55,603 - supervised_finetuning - INFO - Epoch 40 - supervised_accuracy/epoch: 65.1127\n",
            "2025-11-01 14:25:55,603 - supervised_finetuning - INFO - Epoch 40 - supervised_f1/epoch: 64.4770\n",
            "2025-11-01 14:25:55,603 - supervised_finetuning - INFO - Epoch 40 - supervised_recall/epoch: 65.1127\n",
            "2025-11-01 14:25:55,604 - supervised_finetuning - INFO - Epoch 40 - supervised_precision/epoch: 64.6541\n",
            "Classifier Epoch 42/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8983, Acc=64.06%]2025-11-01 14:25:55,612 - supervised_finetuning - INFO - Epoch 861 - supervised_loss/batch: 0.8983\n",
            "2025-11-01 14:25:55,612 - supervised_finetuning - INFO - Epoch 861 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:25:55,614 - supervised_finetuning - INFO - Epoch 861 - learning_rate: 0.0001\n",
            "Classifier Epoch 42/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9810, Acc=54.69%]2025-11-01 14:25:55,661 - supervised_finetuning - INFO - Epoch 871 - supervised_loss/batch: 0.9810\n",
            "2025-11-01 14:25:55,662 - supervised_finetuning - INFO - Epoch 871 - supervised_accuracy/batch: 54.6875\n",
            "2025-11-01 14:25:55,662 - supervised_finetuning - INFO - Epoch 871 - learning_rate: 0.0001\n",
            "Classifier Epoch 42/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8383, Acc=57.14%]2025-11-01 14:25:55,705 - supervised_finetuning - INFO - Epoch 881 - supervised_loss/batch: 0.8383\n",
            "2025-11-01 14:25:55,705 - supervised_finetuning - INFO - Epoch 881 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:55,705 - supervised_finetuning - INFO - Epoch 881 - learning_rate: 0.0001\n",
            "Classifier Epoch 42/100: 100% 21/21 [00:00<00:00, 206.36it/s, Loss=0.8383, Acc=57.14%]\n",
            "2025-11-01 14:25:55,722 - supervised_finetuning - INFO - Epoch 42:\n",
            "2025-11-01 14:25:55,722 - supervised_finetuning - INFO -   Train Loss: 0.8381, Train Acc: 65.11%\n",
            "2025-11-01 14:25:55,722 - supervised_finetuning - INFO -   Train F1: 64.55%, Train Recall: 65.11%\n",
            "2025-11-01 14:25:55,722 - supervised_finetuning - INFO -   Train Precision: 64.23%\n",
            "2025-11-01 14:25:55,722 - supervised_finetuning - INFO - Epoch 41 - supervised_loss/epoch: 0.8381\n",
            "2025-11-01 14:25:55,722 - supervised_finetuning - INFO - Epoch 41 - supervised_accuracy/epoch: 65.1127\n",
            "2025-11-01 14:25:55,722 - supervised_finetuning - INFO - Epoch 41 - supervised_f1/epoch: 64.5505\n",
            "2025-11-01 14:25:55,722 - supervised_finetuning - INFO - Epoch 41 - supervised_recall/epoch: 65.1127\n",
            "2025-11-01 14:25:55,723 - supervised_finetuning - INFO - Epoch 41 - supervised_precision/epoch: 64.2325\n",
            "Classifier Epoch 43/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9942, Acc=60.94%]2025-11-01 14:25:55,728 - supervised_finetuning - INFO - Epoch 882 - supervised_loss/batch: 0.9942\n",
            "2025-11-01 14:25:55,728 - supervised_finetuning - INFO - Epoch 882 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:25:55,728 - supervised_finetuning - INFO - Epoch 882 - learning_rate: 0.0001\n",
            "Classifier Epoch 43/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9800, Acc=62.50%]2025-11-01 14:25:55,772 - supervised_finetuning - INFO - Epoch 892 - supervised_loss/batch: 0.9800\n",
            "2025-11-01 14:25:55,772 - supervised_finetuning - INFO - Epoch 892 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:55,773 - supervised_finetuning - INFO - Epoch 892 - learning_rate: 0.0001\n",
            "Classifier Epoch 43/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0534, Acc=28.57%]2025-11-01 14:25:55,825 - supervised_finetuning - INFO - Epoch 902 - supervised_loss/batch: 1.0534\n",
            "2025-11-01 14:25:55,826 - supervised_finetuning - INFO - Epoch 902 - supervised_accuracy/batch: 28.5714\n",
            "2025-11-01 14:25:55,826 - supervised_finetuning - INFO - Epoch 902 - learning_rate: 0.0001\n",
            "Classifier Epoch 43/100: 100% 21/21 [00:00<00:00, 204.09it/s, Loss=1.0534, Acc=28.57%]\n",
            "2025-11-01 14:25:55,843 - supervised_finetuning - INFO - Epoch 43:\n",
            "2025-11-01 14:25:55,843 - supervised_finetuning - INFO -   Train Loss: 0.8457, Train Acc: 65.42%\n",
            "2025-11-01 14:25:55,843 - supervised_finetuning - INFO -   Train F1: 65.10%, Train Recall: 65.42%\n",
            "2025-11-01 14:25:55,843 - supervised_finetuning - INFO -   Train Precision: 64.98%\n",
            "2025-11-01 14:25:55,843 - supervised_finetuning - INFO - Epoch 42 - supervised_loss/epoch: 0.8457\n",
            "2025-11-01 14:25:55,844 - supervised_finetuning - INFO - Epoch 42 - supervised_accuracy/epoch: 65.4235\n",
            "2025-11-01 14:25:55,844 - supervised_finetuning - INFO - Epoch 42 - supervised_f1/epoch: 65.0966\n",
            "2025-11-01 14:25:55,844 - supervised_finetuning - INFO - Epoch 42 - supervised_recall/epoch: 65.4235\n",
            "2025-11-01 14:25:55,844 - supervised_finetuning - INFO - Epoch 42 - supervised_precision/epoch: 64.9774\n",
            "Classifier Epoch 44/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8456, Acc=62.50%]2025-11-01 14:25:55,850 - supervised_finetuning - INFO - Epoch 903 - supervised_loss/batch: 0.8456\n",
            "2025-11-01 14:25:55,851 - supervised_finetuning - INFO - Epoch 903 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:55,851 - supervised_finetuning - INFO - Epoch 903 - learning_rate: 0.0001\n",
            "Classifier Epoch 44/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8595, Acc=65.62%]2025-11-01 14:25:55,903 - supervised_finetuning - INFO - Epoch 913 - supervised_loss/batch: 0.8595\n",
            "2025-11-01 14:25:55,903 - supervised_finetuning - INFO - Epoch 913 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:25:55,903 - supervised_finetuning - INFO - Epoch 913 - learning_rate: 0.0001\n",
            "Classifier Epoch 44/100:  95% 20/21 [00:00<00:00, 192.20it/s, Loss=1.2862, Acc=14.29%]2025-11-01 14:25:55,953 - supervised_finetuning - INFO - Epoch 923 - supervised_loss/batch: 1.2862\n",
            "2025-11-01 14:25:55,954 - supervised_finetuning - INFO - Epoch 923 - supervised_accuracy/batch: 14.2857\n",
            "2025-11-01 14:25:55,954 - supervised_finetuning - INFO - Epoch 923 - learning_rate: 0.0001\n",
            "Classifier Epoch 44/100: 100% 21/21 [00:00<00:00, 191.74it/s, Loss=1.2862, Acc=14.29%]\n",
            "2025-11-01 14:25:55,972 - supervised_finetuning - INFO - Epoch 44:\n",
            "2025-11-01 14:25:55,972 - supervised_finetuning - INFO -   Train Loss: 0.8567, Train Acc: 65.73%\n",
            "2025-11-01 14:25:55,972 - supervised_finetuning - INFO -   Train F1: 65.29%, Train Recall: 65.73%\n",
            "2025-11-01 14:25:55,972 - supervised_finetuning - INFO -   Train Precision: 65.21%\n",
            "2025-11-01 14:25:55,972 - supervised_finetuning - INFO - Epoch 43 - supervised_loss/epoch: 0.8567\n",
            "2025-11-01 14:25:55,972 - supervised_finetuning - INFO - Epoch 43 - supervised_accuracy/epoch: 65.7343\n",
            "2025-11-01 14:25:55,973 - supervised_finetuning - INFO - Epoch 43 - supervised_f1/epoch: 65.2913\n",
            "2025-11-01 14:25:55,973 - supervised_finetuning - INFO - Epoch 43 - supervised_recall/epoch: 65.7343\n",
            "2025-11-01 14:25:55,973 - supervised_finetuning - INFO - Epoch 43 - supervised_precision/epoch: 65.2063\n",
            "Classifier Epoch 45/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6052, Acc=78.12%]2025-11-01 14:25:55,981 - supervised_finetuning - INFO - Epoch 924 - supervised_loss/batch: 0.6052\n",
            "2025-11-01 14:25:55,981 - supervised_finetuning - INFO - Epoch 924 - supervised_accuracy/batch: 78.1250\n",
            "2025-11-01 14:25:55,981 - supervised_finetuning - INFO - Epoch 924 - learning_rate: 0.0001\n",
            "Classifier Epoch 45/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8956, Acc=59.38%]2025-11-01 14:25:56,028 - supervised_finetuning - INFO - Epoch 934 - supervised_loss/batch: 0.8956\n",
            "2025-11-01 14:25:56,028 - supervised_finetuning - INFO - Epoch 934 - supervised_accuracy/batch: 59.3750\n",
            "2025-11-01 14:25:56,028 - supervised_finetuning - INFO - Epoch 934 - learning_rate: 0.0001\n",
            "Classifier Epoch 45/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0101, Acc=57.14%]2025-11-01 14:25:56,076 - supervised_finetuning - INFO - Epoch 944 - supervised_loss/batch: 1.0101\n",
            "2025-11-01 14:25:56,076 - supervised_finetuning - INFO - Epoch 944 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:25:56,076 - supervised_finetuning - INFO - Epoch 944 - learning_rate: 0.0001\n",
            "Classifier Epoch 45/100: 100% 21/21 [00:00<00:00, 203.32it/s, Loss=1.0101, Acc=57.14%]\n",
            "2025-11-01 14:25:56,094 - supervised_finetuning - INFO - Epoch 45:\n",
            "2025-11-01 14:25:56,094 - supervised_finetuning - INFO -   Train Loss: 0.8369, Train Acc: 66.28%\n",
            "2025-11-01 14:25:56,094 - supervised_finetuning - INFO -   Train F1: 65.91%, Train Recall: 66.28%\n",
            "2025-11-01 14:25:56,094 - supervised_finetuning - INFO -   Train Precision: 66.20%\n",
            "2025-11-01 14:25:56,094 - supervised_finetuning - INFO - Epoch 44 - supervised_loss/epoch: 0.8369\n",
            "2025-11-01 14:25:56,094 - supervised_finetuning - INFO - Epoch 44 - supervised_accuracy/epoch: 66.2782\n",
            "2025-11-01 14:25:56,095 - supervised_finetuning - INFO - Epoch 44 - supervised_f1/epoch: 65.9143\n",
            "2025-11-01 14:25:56,095 - supervised_finetuning - INFO - Epoch 44 - supervised_recall/epoch: 66.2782\n",
            "2025-11-01 14:25:56,095 - supervised_finetuning - INFO - Epoch 44 - supervised_precision/epoch: 66.1998\n",
            "Classifier Epoch 46/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5479, Acc=82.81%]2025-11-01 14:25:56,101 - supervised_finetuning - INFO - Epoch 945 - supervised_loss/batch: 0.5479\n",
            "2025-11-01 14:25:56,101 - supervised_finetuning - INFO - Epoch 945 - supervised_accuracy/batch: 82.8125\n",
            "2025-11-01 14:25:56,102 - supervised_finetuning - INFO - Epoch 945 - learning_rate: 0.0001\n",
            "Classifier Epoch 46/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8788, Acc=62.50%]2025-11-01 14:25:56,158 - supervised_finetuning - INFO - Epoch 955 - supervised_loss/batch: 0.8788\n",
            "2025-11-01 14:25:56,158 - supervised_finetuning - INFO - Epoch 955 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:56,159 - supervised_finetuning - INFO - Epoch 955 - learning_rate: 0.0001\n",
            "Classifier Epoch 46/100:  86% 18/21 [00:00<00:00, 179.15it/s, Loss=0.7151, Acc=71.43%]2025-11-01 14:25:56,210 - supervised_finetuning - INFO - Epoch 965 - supervised_loss/batch: 0.7151\n",
            "2025-11-01 14:25:56,210 - supervised_finetuning - INFO - Epoch 965 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:56,211 - supervised_finetuning - INFO - Epoch 965 - learning_rate: 0.0001\n",
            "Classifier Epoch 46/100: 100% 21/21 [00:00<00:00, 182.28it/s, Loss=0.7151, Acc=71.43%]\n",
            "2025-11-01 14:25:56,229 - supervised_finetuning - INFO - Epoch 46:\n",
            "2025-11-01 14:25:56,229 - supervised_finetuning - INFO -   Train Loss: 0.8153, Train Acc: 65.73%\n",
            "2025-11-01 14:25:56,229 - supervised_finetuning - INFO -   Train F1: 65.19%, Train Recall: 65.73%\n",
            "2025-11-01 14:25:56,229 - supervised_finetuning - INFO -   Train Precision: 64.89%\n",
            "2025-11-01 14:25:56,229 - supervised_finetuning - INFO - Epoch 45 - supervised_loss/epoch: 0.8153\n",
            "2025-11-01 14:25:56,230 - supervised_finetuning - INFO - Epoch 45 - supervised_accuracy/epoch: 65.7343\n",
            "2025-11-01 14:25:56,230 - supervised_finetuning - INFO - Epoch 45 - supervised_f1/epoch: 65.1894\n",
            "2025-11-01 14:25:56,230 - supervised_finetuning - INFO - Epoch 45 - supervised_recall/epoch: 65.7343\n",
            "2025-11-01 14:25:56,230 - supervised_finetuning - INFO - Epoch 45 - supervised_precision/epoch: 64.8928\n",
            "Classifier Epoch 47/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7196, Acc=71.88%]2025-11-01 14:25:56,236 - supervised_finetuning - INFO - Epoch 966 - supervised_loss/batch: 0.7196\n",
            "2025-11-01 14:25:56,236 - supervised_finetuning - INFO - Epoch 966 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:25:56,236 - supervised_finetuning - INFO - Epoch 966 - learning_rate: 0.0001\n",
            "Classifier Epoch 47/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7953, Acc=65.62%]2025-11-01 14:25:56,279 - supervised_finetuning - INFO - Epoch 976 - supervised_loss/batch: 0.7953\n",
            "2025-11-01 14:25:56,279 - supervised_finetuning - INFO - Epoch 976 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:25:56,279 - supervised_finetuning - INFO - Epoch 976 - learning_rate: 0.0001\n",
            "Classifier Epoch 47/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5831, Acc=100.00%]2025-11-01 14:25:56,333 - supervised_finetuning - INFO - Epoch 986 - supervised_loss/batch: 0.5831\n",
            "2025-11-01 14:25:56,333 - supervised_finetuning - INFO - Epoch 986 - supervised_accuracy/batch: 100.0000\n",
            "2025-11-01 14:25:56,333 - supervised_finetuning - INFO - Epoch 986 - learning_rate: 0.0001\n",
            "Classifier Epoch 47/100: 100% 21/21 [00:00<00:00, 203.66it/s, Loss=0.5831, Acc=100.00%]\n",
            "2025-11-01 14:25:56,358 - supervised_finetuning - INFO - Epoch 47:\n",
            "2025-11-01 14:25:56,358 - supervised_finetuning - INFO -   Train Loss: 0.8272, Train Acc: 66.28%\n",
            "2025-11-01 14:25:56,358 - supervised_finetuning - INFO -   Train F1: 65.79%, Train Recall: 66.28%\n",
            "2025-11-01 14:25:56,358 - supervised_finetuning - INFO -   Train Precision: 65.57%\n",
            "2025-11-01 14:25:56,358 - supervised_finetuning - INFO - Epoch 46 - supervised_loss/epoch: 0.8272\n",
            "2025-11-01 14:25:56,359 - supervised_finetuning - INFO - Epoch 46 - supervised_accuracy/epoch: 66.2782\n",
            "2025-11-01 14:25:56,359 - supervised_finetuning - INFO - Epoch 46 - supervised_f1/epoch: 65.7930\n",
            "2025-11-01 14:25:56,359 - supervised_finetuning - INFO - Epoch 46 - supervised_recall/epoch: 66.2782\n",
            "2025-11-01 14:25:56,359 - supervised_finetuning - INFO - Epoch 46 - supervised_precision/epoch: 65.5742\n",
            "Classifier Epoch 48/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7573, Acc=65.62%]2025-11-01 14:25:56,367 - supervised_finetuning - INFO - Epoch 987 - supervised_loss/batch: 0.7573\n",
            "2025-11-01 14:25:56,367 - supervised_finetuning - INFO - Epoch 987 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:25:56,367 - supervised_finetuning - INFO - Epoch 987 - learning_rate: 0.0001\n",
            "Classifier Epoch 48/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8685, Acc=68.75%]2025-11-01 14:25:56,425 - supervised_finetuning - INFO - Epoch 997 - supervised_loss/batch: 0.8685\n",
            "2025-11-01 14:25:56,426 - supervised_finetuning - INFO - Epoch 997 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:25:56,426 - supervised_finetuning - INFO - Epoch 997 - learning_rate: 0.0001\n",
            "Classifier Epoch 48/100:  90% 19/21 [00:00<00:00, 184.39it/s, Loss=0.8980, Acc=42.86%]2025-11-01 14:25:56,473 - supervised_finetuning - INFO - Epoch 1007 - supervised_loss/batch: 0.8980\n",
            "2025-11-01 14:25:56,474 - supervised_finetuning - INFO - Epoch 1007 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:25:56,475 - supervised_finetuning - INFO - Epoch 1007 - learning_rate: 0.0001\n",
            "Classifier Epoch 48/100: 100% 21/21 [00:00<00:00, 181.98it/s, Loss=0.8980, Acc=42.86%]\n",
            "2025-11-01 14:25:56,502 - supervised_finetuning - INFO - Epoch 48:\n",
            "2025-11-01 14:25:56,503 - supervised_finetuning - INFO -   Train Loss: 0.8198, Train Acc: 65.66%\n",
            "2025-11-01 14:25:56,503 - supervised_finetuning - INFO -   Train F1: 65.10%, Train Recall: 65.66%\n",
            "2025-11-01 14:25:56,503 - supervised_finetuning - INFO -   Train Precision: 65.00%\n",
            "2025-11-01 14:25:56,503 - supervised_finetuning - INFO - Epoch 47 - supervised_loss/epoch: 0.8198\n",
            "2025-11-01 14:25:56,503 - supervised_finetuning - INFO - Epoch 47 - supervised_accuracy/epoch: 65.6566\n",
            "2025-11-01 14:25:56,503 - supervised_finetuning - INFO - Epoch 47 - supervised_f1/epoch: 65.1045\n",
            "2025-11-01 14:25:56,503 - supervised_finetuning - INFO - Epoch 47 - supervised_recall/epoch: 65.6566\n",
            "2025-11-01 14:25:56,504 - supervised_finetuning - INFO - Epoch 47 - supervised_precision/epoch: 65.0040\n",
            "Classifier Epoch 49/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8596, Acc=67.19%]2025-11-01 14:25:56,512 - supervised_finetuning - INFO - Epoch 1008 - supervised_loss/batch: 0.8596\n",
            "2025-11-01 14:25:56,512 - supervised_finetuning - INFO - Epoch 1008 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:25:56,512 - supervised_finetuning - INFO - Epoch 1008 - learning_rate: 0.0001\n",
            "Classifier Epoch 49/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8137, Acc=62.50%]2025-11-01 14:25:56,570 - supervised_finetuning - INFO - Epoch 1018 - supervised_loss/batch: 0.8137\n",
            "2025-11-01 14:25:56,571 - supervised_finetuning - INFO - Epoch 1018 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:25:56,571 - supervised_finetuning - INFO - Epoch 1018 - learning_rate: 0.0001\n",
            "Classifier Epoch 49/100:  90% 19/21 [00:00<00:00, 186.59it/s, Loss=0.7382, Acc=71.43%]2025-11-01 14:25:56,615 - supervised_finetuning - INFO - Epoch 1028 - supervised_loss/batch: 0.7382\n",
            "2025-11-01 14:25:56,615 - supervised_finetuning - INFO - Epoch 1028 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:25:56,615 - supervised_finetuning - INFO - Epoch 1028 - learning_rate: 0.0001\n",
            "Classifier Epoch 49/100: 100% 21/21 [00:00<00:00, 188.56it/s, Loss=0.7382, Acc=71.43%]\n",
            "2025-11-01 14:25:56,633 - supervised_finetuning - INFO - Epoch 49:\n",
            "2025-11-01 14:25:56,633 - supervised_finetuning - INFO -   Train Loss: 0.8218, Train Acc: 65.03%\n",
            "2025-11-01 14:25:56,633 - supervised_finetuning - INFO -   Train F1: 64.56%, Train Recall: 65.03%\n",
            "2025-11-01 14:25:56,633 - supervised_finetuning - INFO -   Train Precision: 64.33%\n",
            "2025-11-01 14:25:56,633 - supervised_finetuning - INFO - Epoch 48 - supervised_loss/epoch: 0.8218\n",
            "2025-11-01 14:25:56,634 - supervised_finetuning - INFO - Epoch 48 - supervised_accuracy/epoch: 65.0350\n",
            "2025-11-01 14:25:56,634 - supervised_finetuning - INFO - Epoch 48 - supervised_f1/epoch: 64.5565\n",
            "2025-11-01 14:25:56,634 - supervised_finetuning - INFO - Epoch 48 - supervised_recall/epoch: 65.0350\n",
            "2025-11-01 14:25:56,634 - supervised_finetuning - INFO - Epoch 48 - supervised_precision/epoch: 64.3295\n",
            "Classifier Epoch 50/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8174, Acc=65.62%]2025-11-01 14:25:56,639 - supervised_finetuning - INFO - Epoch 1029 - supervised_loss/batch: 0.8174\n",
            "2025-11-01 14:25:56,639 - supervised_finetuning - INFO - Epoch 1029 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:25:56,640 - supervised_finetuning - INFO - Epoch 1029 - learning_rate: 0.0001\n",
            "Classifier Epoch 50/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8869, Acc=59.38%]2025-11-01 14:25:56,683 - supervised_finetuning - INFO - Epoch 1039 - supervised_loss/batch: 0.8869\n",
            "2025-11-01 14:25:56,683 - supervised_finetuning - INFO - Epoch 1039 - supervised_accuracy/batch: 59.3750\n",
            "2025-11-01 14:25:56,683 - supervised_finetuning - INFO - Epoch 1039 - learning_rate: 0.0001\n",
            "Classifier Epoch 50/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5665, Acc=85.71%]2025-11-01 14:25:56,728 - supervised_finetuning - INFO - Epoch 1049 - supervised_loss/batch: 0.5665\n",
            "2025-11-01 14:25:56,728 - supervised_finetuning - INFO - Epoch 1049 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:25:56,728 - supervised_finetuning - INFO - Epoch 1049 - learning_rate: 0.0001\n",
            "Classifier Epoch 50/100: 100% 21/21 [00:00<00:00, 224.20it/s, Loss=0.5665, Acc=85.71%]\n",
            "2025-11-01 14:25:56,744 - supervised_finetuning - INFO - Epoch 50:\n",
            "2025-11-01 14:25:56,745 - supervised_finetuning - INFO -   Train Loss: 0.8057, Train Acc: 66.90%\n",
            "2025-11-01 14:25:56,745 - supervised_finetuning - INFO -   Train F1: 66.41%, Train Recall: 66.90%\n",
            "2025-11-01 14:25:56,745 - supervised_finetuning - INFO -   Train Precision: 66.25%\n",
            "2025-11-01 14:25:56,745 - supervised_finetuning - INFO - Epoch 49 - supervised_loss/epoch: 0.8057\n",
            "2025-11-01 14:25:56,745 - supervised_finetuning - INFO - Epoch 49 - supervised_accuracy/epoch: 66.8998\n",
            "2025-11-01 14:25:56,745 - supervised_finetuning - INFO - Epoch 49 - supervised_f1/epoch: 66.4071\n",
            "2025-11-01 14:25:56,745 - supervised_finetuning - INFO - Epoch 49 - supervised_recall/epoch: 66.8998\n",
            "2025-11-01 14:25:56,745 - supervised_finetuning - INFO - Epoch 49 - supervised_precision/epoch: 66.2451\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:02,953 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 66.90%\n",
            "Classifier Epoch 51/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8503, Acc=70.31%]2025-11-01 14:26:02,960 - supervised_finetuning - INFO - Epoch 1050 - supervised_loss/batch: 0.8503\n",
            "2025-11-01 14:26:02,960 - supervised_finetuning - INFO - Epoch 1050 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:26:02,960 - supervised_finetuning - INFO - Epoch 1050 - learning_rate: 0.0001\n",
            "Classifier Epoch 51/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9897, Acc=56.25%]2025-11-01 14:26:03,039 - supervised_finetuning - INFO - Epoch 1060 - supervised_loss/batch: 0.9897\n",
            "2025-11-01 14:26:03,039 - supervised_finetuning - INFO - Epoch 1060 - supervised_accuracy/batch: 56.2500\n",
            "2025-11-01 14:26:03,039 - supervised_finetuning - INFO - Epoch 1060 - learning_rate: 0.0001\n",
            "Classifier Epoch 51/100:  71% 15/21 [00:00<00:00, 143.86it/s, Loss=0.3100, Acc=100.00%]2025-11-01 14:26:03,085 - supervised_finetuning - INFO - Epoch 1070 - supervised_loss/batch: 0.3100\n",
            "2025-11-01 14:26:03,085 - supervised_finetuning - INFO - Epoch 1070 - supervised_accuracy/batch: 100.0000\n",
            "2025-11-01 14:26:03,085 - supervised_finetuning - INFO - Epoch 1070 - learning_rate: 0.0001\n",
            "Classifier Epoch 51/100: 100% 21/21 [00:00<00:00, 159.63it/s, Loss=0.3100, Acc=100.00%]\n",
            "2025-11-01 14:26:03,103 - supervised_finetuning - INFO - Epoch 51:\n",
            "2025-11-01 14:26:03,103 - supervised_finetuning - INFO -   Train Loss: 0.7938, Train Acc: 66.36%\n",
            "2025-11-01 14:26:03,103 - supervised_finetuning - INFO -   Train F1: 65.69%, Train Recall: 66.36%\n",
            "2025-11-01 14:26:03,103 - supervised_finetuning - INFO -   Train Precision: 65.72%\n",
            "2025-11-01 14:26:03,104 - supervised_finetuning - INFO - Epoch 50 - supervised_loss/epoch: 0.7938\n",
            "2025-11-01 14:26:03,104 - supervised_finetuning - INFO - Epoch 50 - supervised_accuracy/epoch: 66.3559\n",
            "2025-11-01 14:26:03,104 - supervised_finetuning - INFO - Epoch 50 - supervised_f1/epoch: 65.6879\n",
            "2025-11-01 14:26:03,104 - supervised_finetuning - INFO - Epoch 50 - supervised_recall/epoch: 66.3559\n",
            "2025-11-01 14:26:03,104 - supervised_finetuning - INFO - Epoch 50 - supervised_precision/epoch: 65.7220\n",
            "Classifier Epoch 52/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8714, Acc=59.38%]2025-11-01 14:26:03,110 - supervised_finetuning - INFO - Epoch 1071 - supervised_loss/batch: 0.8714\n",
            "2025-11-01 14:26:03,110 - supervised_finetuning - INFO - Epoch 1071 - supervised_accuracy/batch: 59.3750\n",
            "2025-11-01 14:26:03,111 - supervised_finetuning - INFO - Epoch 1071 - learning_rate: 0.0001\n",
            "Classifier Epoch 52/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8749, Acc=56.25%]2025-11-01 14:26:03,158 - supervised_finetuning - INFO - Epoch 1081 - supervised_loss/batch: 0.8749\n",
            "2025-11-01 14:26:03,158 - supervised_finetuning - INFO - Epoch 1081 - supervised_accuracy/batch: 56.2500\n",
            "2025-11-01 14:26:03,158 - supervised_finetuning - INFO - Epoch 1081 - learning_rate: 0.0001\n",
            "Classifier Epoch 52/100:  90% 19/21 [00:00<00:00, 181.55it/s, Loss=1.0821, Acc=57.14%]2025-11-01 14:26:03,219 - supervised_finetuning - INFO - Epoch 1091 - supervised_loss/batch: 1.0821\n",
            "2025-11-01 14:26:03,219 - supervised_finetuning - INFO - Epoch 1091 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:26:03,220 - supervised_finetuning - INFO - Epoch 1091 - learning_rate: 0.0001\n",
            "Classifier Epoch 52/100: 100% 21/21 [00:00<00:00, 182.73it/s, Loss=1.0821, Acc=57.14%]\n",
            "2025-11-01 14:26:03,236 - supervised_finetuning - INFO - Epoch 52:\n",
            "2025-11-01 14:26:03,236 - supervised_finetuning - INFO -   Train Loss: 0.8282, Train Acc: 65.27%\n",
            "2025-11-01 14:26:03,237 - supervised_finetuning - INFO -   Train F1: 64.75%, Train Recall: 65.27%\n",
            "2025-11-01 14:26:03,237 - supervised_finetuning - INFO -   Train Precision: 64.67%\n",
            "2025-11-01 14:26:03,237 - supervised_finetuning - INFO - Epoch 51 - supervised_loss/epoch: 0.8282\n",
            "2025-11-01 14:26:03,237 - supervised_finetuning - INFO - Epoch 51 - supervised_accuracy/epoch: 65.2681\n",
            "2025-11-01 14:26:03,237 - supervised_finetuning - INFO - Epoch 51 - supervised_f1/epoch: 64.7495\n",
            "2025-11-01 14:26:03,237 - supervised_finetuning - INFO - Epoch 51 - supervised_recall/epoch: 65.2681\n",
            "2025-11-01 14:26:03,237 - supervised_finetuning - INFO - Epoch 51 - supervised_precision/epoch: 64.6673\n",
            "Classifier Epoch 53/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8643, Acc=73.44%]2025-11-01 14:26:03,243 - supervised_finetuning - INFO - Epoch 1092 - supervised_loss/batch: 0.8643\n",
            "2025-11-01 14:26:03,243 - supervised_finetuning - INFO - Epoch 1092 - supervised_accuracy/batch: 73.4375\n",
            "2025-11-01 14:26:03,243 - supervised_finetuning - INFO - Epoch 1092 - learning_rate: 0.0001\n",
            "Classifier Epoch 53/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8132, Acc=67.19%]2025-11-01 14:26:03,297 - supervised_finetuning - INFO - Epoch 1102 - supervised_loss/batch: 0.8132\n",
            "2025-11-01 14:26:03,297 - supervised_finetuning - INFO - Epoch 1102 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:26:03,297 - supervised_finetuning - INFO - Epoch 1102 - learning_rate: 0.0001\n",
            "Classifier Epoch 53/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5227, Acc=85.71%]2025-11-01 14:26:03,341 - supervised_finetuning - INFO - Epoch 1112 - supervised_loss/batch: 0.5227\n",
            "2025-11-01 14:26:03,342 - supervised_finetuning - INFO - Epoch 1112 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:03,342 - supervised_finetuning - INFO - Epoch 1112 - learning_rate: 0.0001\n",
            "Classifier Epoch 53/100: 100% 21/21 [00:00<00:00, 200.73it/s, Loss=0.5227, Acc=85.71%]\n",
            "2025-11-01 14:26:03,359 - supervised_finetuning - INFO - Epoch 53:\n",
            "2025-11-01 14:26:03,359 - supervised_finetuning - INFO -   Train Loss: 0.8017, Train Acc: 67.29%\n",
            "2025-11-01 14:26:03,359 - supervised_finetuning - INFO -   Train F1: 66.89%, Train Recall: 67.29%\n",
            "2025-11-01 14:26:03,359 - supervised_finetuning - INFO -   Train Precision: 66.65%\n",
            "2025-11-01 14:26:03,359 - supervised_finetuning - INFO - Epoch 52 - supervised_loss/epoch: 0.8017\n",
            "2025-11-01 14:26:03,360 - supervised_finetuning - INFO - Epoch 52 - supervised_accuracy/epoch: 67.2883\n",
            "2025-11-01 14:26:03,360 - supervised_finetuning - INFO - Epoch 52 - supervised_f1/epoch: 66.8926\n",
            "2025-11-01 14:26:03,360 - supervised_finetuning - INFO - Epoch 52 - supervised_recall/epoch: 67.2883\n",
            "2025-11-01 14:26:03,360 - supervised_finetuning - INFO - Epoch 52 - supervised_precision/epoch: 66.6528\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:04,477 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 67.29%\n",
            "Classifier Epoch 54/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.0404, Acc=59.38%]2025-11-01 14:26:04,502 - supervised_finetuning - INFO - Epoch 1113 - supervised_loss/batch: 1.0404\n",
            "2025-11-01 14:26:04,503 - supervised_finetuning - INFO - Epoch 1113 - supervised_accuracy/batch: 59.3750\n",
            "2025-11-01 14:26:04,503 - supervised_finetuning - INFO - Epoch 1113 - learning_rate: 0.0001\n",
            "Classifier Epoch 54/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7475, Acc=68.75%]2025-11-01 14:26:04,575 - supervised_finetuning - INFO - Epoch 1123 - supervised_loss/batch: 0.7475\n",
            "2025-11-01 14:26:04,575 - supervised_finetuning - INFO - Epoch 1123 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:04,575 - supervised_finetuning - INFO - Epoch 1123 - learning_rate: 0.0001\n",
            "Classifier Epoch 54/100:  57% 12/21 [00:00<00:00, 119.87it/s, Loss=0.8475, Acc=57.14%]2025-11-01 14:26:04,622 - supervised_finetuning - INFO - Epoch 1133 - supervised_loss/batch: 0.8475\n",
            "2025-11-01 14:26:04,623 - supervised_finetuning - INFO - Epoch 1133 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:26:04,623 - supervised_finetuning - INFO - Epoch 1133 - learning_rate: 0.0001\n",
            "Classifier Epoch 54/100: 100% 21/21 [00:00<00:00, 146.43it/s, Loss=0.8475, Acc=57.14%]\n",
            "2025-11-01 14:26:04,641 - supervised_finetuning - INFO - Epoch 54:\n",
            "2025-11-01 14:26:04,641 - supervised_finetuning - INFO -   Train Loss: 0.8070, Train Acc: 66.90%\n",
            "2025-11-01 14:26:04,641 - supervised_finetuning - INFO -   Train F1: 66.43%, Train Recall: 66.90%\n",
            "2025-11-01 14:26:04,641 - supervised_finetuning - INFO -   Train Precision: 66.68%\n",
            "2025-11-01 14:26:04,641 - supervised_finetuning - INFO - Epoch 53 - supervised_loss/epoch: 0.8070\n",
            "2025-11-01 14:26:04,641 - supervised_finetuning - INFO - Epoch 53 - supervised_accuracy/epoch: 66.8998\n",
            "2025-11-01 14:26:04,641 - supervised_finetuning - INFO - Epoch 53 - supervised_f1/epoch: 66.4328\n",
            "2025-11-01 14:26:04,641 - supervised_finetuning - INFO - Epoch 53 - supervised_recall/epoch: 66.8998\n",
            "2025-11-01 14:26:04,642 - supervised_finetuning - INFO - Epoch 53 - supervised_precision/epoch: 66.6762\n",
            "Classifier Epoch 55/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8524, Acc=57.81%]2025-11-01 14:26:04,652 - supervised_finetuning - INFO - Epoch 1134 - supervised_loss/batch: 0.8524\n",
            "2025-11-01 14:26:04,652 - supervised_finetuning - INFO - Epoch 1134 - supervised_accuracy/batch: 57.8125\n",
            "2025-11-01 14:26:04,652 - supervised_finetuning - INFO - Epoch 1134 - learning_rate: 0.0001\n",
            "Classifier Epoch 55/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7813, Acc=67.19%]2025-11-01 14:26:04,701 - supervised_finetuning - INFO - Epoch 1144 - supervised_loss/batch: 0.7813\n",
            "2025-11-01 14:26:04,702 - supervised_finetuning - INFO - Epoch 1144 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:26:04,702 - supervised_finetuning - INFO - Epoch 1144 - learning_rate: 0.0001\n",
            "Classifier Epoch 55/100:  81% 17/21 [00:00<00:00, 161.60it/s, Loss=0.9563, Acc=57.14%]2025-11-01 14:26:04,780 - supervised_finetuning - INFO - Epoch 1154 - supervised_loss/batch: 0.9563\n",
            "2025-11-01 14:26:04,780 - supervised_finetuning - INFO - Epoch 1154 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:26:04,780 - supervised_finetuning - INFO - Epoch 1154 - learning_rate: 0.0001\n",
            "Classifier Epoch 55/100: 100% 21/21 [00:00<00:00, 151.30it/s, Loss=0.9563, Acc=57.14%]\n",
            "2025-11-01 14:26:04,798 - supervised_finetuning - INFO - Epoch 55:\n",
            "2025-11-01 14:26:04,798 - supervised_finetuning - INFO -   Train Loss: 0.8147, Train Acc: 66.98%\n",
            "2025-11-01 14:26:04,798 - supervised_finetuning - INFO -   Train F1: 66.68%, Train Recall: 66.98%\n",
            "2025-11-01 14:26:04,798 - supervised_finetuning - INFO -   Train Precision: 66.47%\n",
            "2025-11-01 14:26:04,798 - supervised_finetuning - INFO - Epoch 54 - supervised_loss/epoch: 0.8147\n",
            "2025-11-01 14:26:04,799 - supervised_finetuning - INFO - Epoch 54 - supervised_accuracy/epoch: 66.9775\n",
            "2025-11-01 14:26:04,799 - supervised_finetuning - INFO - Epoch 54 - supervised_f1/epoch: 66.6775\n",
            "2025-11-01 14:26:04,799 - supervised_finetuning - INFO - Epoch 54 - supervised_recall/epoch: 66.9775\n",
            "2025-11-01 14:26:04,799 - supervised_finetuning - INFO - Epoch 54 - supervised_precision/epoch: 66.4694\n",
            "Classifier Epoch 56/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7791, Acc=65.62%]2025-11-01 14:26:04,805 - supervised_finetuning - INFO - Epoch 1155 - supervised_loss/batch: 0.7791\n",
            "2025-11-01 14:26:04,806 - supervised_finetuning - INFO - Epoch 1155 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:04,806 - supervised_finetuning - INFO - Epoch 1155 - learning_rate: 0.0001\n",
            "Classifier Epoch 56/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6847, Acc=68.75%]2025-11-01 14:26:04,858 - supervised_finetuning - INFO - Epoch 1165 - supervised_loss/batch: 0.6847\n",
            "2025-11-01 14:26:04,859 - supervised_finetuning - INFO - Epoch 1165 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:04,859 - supervised_finetuning - INFO - Epoch 1165 - learning_rate: 0.0001\n",
            "Classifier Epoch 56/100:  95% 20/21 [00:00<00:00, 191.44it/s, Loss=0.9944, Acc=57.14%]2025-11-01 14:26:04,909 - supervised_finetuning - INFO - Epoch 1175 - supervised_loss/batch: 0.9944\n",
            "2025-11-01 14:26:04,909 - supervised_finetuning - INFO - Epoch 1175 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:26:04,909 - supervised_finetuning - INFO - Epoch 1175 - learning_rate: 0.0001\n",
            "Classifier Epoch 56/100: 100% 21/21 [00:00<00:00, 190.87it/s, Loss=0.9944, Acc=57.14%]\n",
            "2025-11-01 14:26:04,928 - supervised_finetuning - INFO - Epoch 56:\n",
            "2025-11-01 14:26:04,928 - supervised_finetuning - INFO -   Train Loss: 0.8331, Train Acc: 65.35%\n",
            "2025-11-01 14:26:04,928 - supervised_finetuning - INFO -   Train F1: 64.79%, Train Recall: 65.35%\n",
            "2025-11-01 14:26:04,928 - supervised_finetuning - INFO -   Train Precision: 64.59%\n",
            "2025-11-01 14:26:04,928 - supervised_finetuning - INFO - Epoch 55 - supervised_loss/epoch: 0.8331\n",
            "2025-11-01 14:26:04,929 - supervised_finetuning - INFO - Epoch 55 - supervised_accuracy/epoch: 65.3458\n",
            "2025-11-01 14:26:04,929 - supervised_finetuning - INFO - Epoch 55 - supervised_f1/epoch: 64.7877\n",
            "2025-11-01 14:26:04,929 - supervised_finetuning - INFO - Epoch 55 - supervised_recall/epoch: 65.3458\n",
            "2025-11-01 14:26:04,929 - supervised_finetuning - INFO - Epoch 55 - supervised_precision/epoch: 64.5929\n",
            "Classifier Epoch 57/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7482, Acc=68.75%]2025-11-01 14:26:04,939 - supervised_finetuning - INFO - Epoch 1176 - supervised_loss/batch: 0.7482\n",
            "2025-11-01 14:26:04,939 - supervised_finetuning - INFO - Epoch 1176 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:04,940 - supervised_finetuning - INFO - Epoch 1176 - learning_rate: 0.0001\n",
            "Classifier Epoch 57/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7903, Acc=67.19%]2025-11-01 14:26:05,004 - supervised_finetuning - INFO - Epoch 1186 - supervised_loss/batch: 0.7903\n",
            "2025-11-01 14:26:05,004 - supervised_finetuning - INFO - Epoch 1186 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:26:05,004 - supervised_finetuning - INFO - Epoch 1186 - learning_rate: 0.0001\n",
            "Classifier Epoch 57/100:  81% 17/21 [00:00<00:00, 163.11it/s, Loss=0.7599, Acc=85.71%]2025-11-01 14:26:05,054 - supervised_finetuning - INFO - Epoch 1196 - supervised_loss/batch: 0.7599\n",
            "2025-11-01 14:26:05,054 - supervised_finetuning - INFO - Epoch 1196 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:05,054 - supervised_finetuning - INFO - Epoch 1196 - learning_rate: 0.0001\n",
            "Classifier Epoch 57/100: 100% 21/21 [00:00<00:00, 168.30it/s, Loss=0.7599, Acc=85.71%]\n",
            "2025-11-01 14:26:05,072 - supervised_finetuning - INFO - Epoch 57:\n",
            "2025-11-01 14:26:05,072 - supervised_finetuning - INFO -   Train Loss: 0.8124, Train Acc: 67.60%\n",
            "2025-11-01 14:26:05,072 - supervised_finetuning - INFO -   Train F1: 67.15%, Train Recall: 67.60%\n",
            "2025-11-01 14:26:05,072 - supervised_finetuning - INFO -   Train Precision: 67.09%\n",
            "2025-11-01 14:26:05,072 - supervised_finetuning - INFO - Epoch 56 - supervised_loss/epoch: 0.8124\n",
            "2025-11-01 14:26:05,072 - supervised_finetuning - INFO - Epoch 56 - supervised_accuracy/epoch: 67.5991\n",
            "2025-11-01 14:26:05,073 - supervised_finetuning - INFO - Epoch 56 - supervised_f1/epoch: 67.1536\n",
            "2025-11-01 14:26:05,073 - supervised_finetuning - INFO - Epoch 56 - supervised_recall/epoch: 67.5991\n",
            "2025-11-01 14:26:05,073 - supervised_finetuning - INFO - Epoch 56 - supervised_precision/epoch: 67.0903\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:06,143 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 67.60%\n",
            "Classifier Epoch 58/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8922, Acc=65.62%]2025-11-01 14:26:06,150 - supervised_finetuning - INFO - Epoch 1197 - supervised_loss/batch: 0.8922\n",
            "2025-11-01 14:26:06,150 - supervised_finetuning - INFO - Epoch 1197 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:06,150 - supervised_finetuning - INFO - Epoch 1197 - learning_rate: 0.0001\n",
            "Classifier Epoch 58/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7254, Acc=68.75%]2025-11-01 14:26:06,199 - supervised_finetuning - INFO - Epoch 1207 - supervised_loss/batch: 0.7254\n",
            "2025-11-01 14:26:06,200 - supervised_finetuning - INFO - Epoch 1207 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:06,200 - supervised_finetuning - INFO - Epoch 1207 - learning_rate: 0.0001\n",
            "Classifier Epoch 58/100:  71% 15/21 [00:00<00:00, 130.79it/s, Loss=0.8422, Acc=71.43%]2025-11-01 14:26:06,306 - supervised_finetuning - INFO - Epoch 1217 - supervised_loss/batch: 0.8422\n",
            "2025-11-01 14:26:06,307 - supervised_finetuning - INFO - Epoch 1217 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:06,307 - supervised_finetuning - INFO - Epoch 1217 - learning_rate: 0.0001\n",
            "Classifier Epoch 58/100: 100% 21/21 [00:00<00:00, 128.53it/s, Loss=0.8422, Acc=71.43%]\n",
            "2025-11-01 14:26:06,326 - supervised_finetuning - INFO - Epoch 58:\n",
            "2025-11-01 14:26:06,326 - supervised_finetuning - INFO -   Train Loss: 0.8133, Train Acc: 65.81%\n",
            "2025-11-01 14:26:06,326 - supervised_finetuning - INFO -   Train F1: 65.21%, Train Recall: 65.81%\n",
            "2025-11-01 14:26:06,326 - supervised_finetuning - INFO -   Train Precision: 65.01%\n",
            "2025-11-01 14:26:06,326 - supervised_finetuning - INFO - Epoch 57 - supervised_loss/epoch: 0.8133\n",
            "2025-11-01 14:26:06,327 - supervised_finetuning - INFO - Epoch 57 - supervised_accuracy/epoch: 65.8120\n",
            "2025-11-01 14:26:06,327 - supervised_finetuning - INFO - Epoch 57 - supervised_f1/epoch: 65.2109\n",
            "2025-11-01 14:26:06,327 - supervised_finetuning - INFO - Epoch 57 - supervised_recall/epoch: 65.8120\n",
            "2025-11-01 14:26:06,327 - supervised_finetuning - INFO - Epoch 57 - supervised_precision/epoch: 65.0089\n",
            "Classifier Epoch 59/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6615, Acc=71.88%]2025-11-01 14:26:06,333 - supervised_finetuning - INFO - Epoch 1218 - supervised_loss/batch: 0.6615\n",
            "2025-11-01 14:26:06,333 - supervised_finetuning - INFO - Epoch 1218 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:26:06,334 - supervised_finetuning - INFO - Epoch 1218 - learning_rate: 0.0001\n",
            "Classifier Epoch 59/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8278, Acc=64.06%]2025-11-01 14:26:06,383 - supervised_finetuning - INFO - Epoch 1228 - supervised_loss/batch: 0.8278\n",
            "2025-11-01 14:26:06,383 - supervised_finetuning - INFO - Epoch 1228 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:06,383 - supervised_finetuning - INFO - Epoch 1228 - learning_rate: 0.0001\n",
            "Classifier Epoch 59/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6841, Acc=71.43%]2025-11-01 14:26:06,429 - supervised_finetuning - INFO - Epoch 1238 - supervised_loss/batch: 0.6841\n",
            "2025-11-01 14:26:06,429 - supervised_finetuning - INFO - Epoch 1238 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:06,429 - supervised_finetuning - INFO - Epoch 1238 - learning_rate: 0.0001\n",
            "Classifier Epoch 59/100: 100% 21/21 [00:00<00:00, 206.42it/s, Loss=0.6841, Acc=71.43%]\n",
            "2025-11-01 14:26:06,445 - supervised_finetuning - INFO - Epoch 59:\n",
            "2025-11-01 14:26:06,446 - supervised_finetuning - INFO -   Train Loss: 0.7948, Train Acc: 66.98%\n",
            "2025-11-01 14:26:06,446 - supervised_finetuning - INFO -   Train F1: 66.32%, Train Recall: 66.98%\n",
            "2025-11-01 14:26:06,446 - supervised_finetuning - INFO -   Train Precision: 66.35%\n",
            "2025-11-01 14:26:06,446 - supervised_finetuning - INFO - Epoch 58 - supervised_loss/epoch: 0.7948\n",
            "2025-11-01 14:26:06,446 - supervised_finetuning - INFO - Epoch 58 - supervised_accuracy/epoch: 66.9775\n",
            "2025-11-01 14:26:06,446 - supervised_finetuning - INFO - Epoch 58 - supervised_f1/epoch: 66.3216\n",
            "2025-11-01 14:26:06,446 - supervised_finetuning - INFO - Epoch 58 - supervised_recall/epoch: 66.9775\n",
            "2025-11-01 14:26:06,446 - supervised_finetuning - INFO - Epoch 58 - supervised_precision/epoch: 66.3513\n",
            "Classifier Epoch 60/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9273, Acc=64.06%]2025-11-01 14:26:06,452 - supervised_finetuning - INFO - Epoch 1239 - supervised_loss/batch: 0.9273\n",
            "2025-11-01 14:26:06,452 - supervised_finetuning - INFO - Epoch 1239 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:06,452 - supervised_finetuning - INFO - Epoch 1239 - learning_rate: 0.0001\n",
            "Classifier Epoch 60/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7268, Acc=68.75%]2025-11-01 14:26:06,500 - supervised_finetuning - INFO - Epoch 1249 - supervised_loss/batch: 0.7268\n",
            "2025-11-01 14:26:06,500 - supervised_finetuning - INFO - Epoch 1249 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:06,501 - supervised_finetuning - INFO - Epoch 1249 - learning_rate: 0.0001\n",
            "Classifier Epoch 60/100:   0% 0/21 [00:00<?, ?it/s, Loss=1.3853, Acc=57.14%]2025-11-01 14:26:06,547 - supervised_finetuning - INFO - Epoch 1259 - supervised_loss/batch: 1.3853\n",
            "2025-11-01 14:26:06,547 - supervised_finetuning - INFO - Epoch 1259 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:26:06,547 - supervised_finetuning - INFO - Epoch 1259 - learning_rate: 0.0001\n",
            "Classifier Epoch 60/100: 100% 21/21 [00:00<00:00, 208.38it/s, Loss=1.3853, Acc=57.14%]\n",
            "2025-11-01 14:26:06,566 - supervised_finetuning - INFO - Epoch 60:\n",
            "2025-11-01 14:26:06,566 - supervised_finetuning - INFO -   Train Loss: 0.8210, Train Acc: 66.98%\n",
            "2025-11-01 14:26:06,566 - supervised_finetuning - INFO -   Train F1: 66.39%, Train Recall: 66.98%\n",
            "2025-11-01 14:26:06,566 - supervised_finetuning - INFO -   Train Precision: 66.81%\n",
            "2025-11-01 14:26:06,566 - supervised_finetuning - INFO - Epoch 59 - supervised_loss/epoch: 0.8210\n",
            "2025-11-01 14:26:06,566 - supervised_finetuning - INFO - Epoch 59 - supervised_accuracy/epoch: 66.9775\n",
            "2025-11-01 14:26:06,566 - supervised_finetuning - INFO - Epoch 59 - supervised_f1/epoch: 66.3857\n",
            "2025-11-01 14:26:06,567 - supervised_finetuning - INFO - Epoch 59 - supervised_recall/epoch: 66.9775\n",
            "2025-11-01 14:26:06,567 - supervised_finetuning - INFO - Epoch 59 - supervised_precision/epoch: 66.8071\n",
            "Classifier Epoch 61/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8491, Acc=64.06%]2025-11-01 14:26:06,572 - supervised_finetuning - INFO - Epoch 1260 - supervised_loss/batch: 0.8491\n",
            "2025-11-01 14:26:06,573 - supervised_finetuning - INFO - Epoch 1260 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:06,573 - supervised_finetuning - INFO - Epoch 1260 - learning_rate: 0.0001\n",
            "Classifier Epoch 61/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8146, Acc=64.06%]2025-11-01 14:26:06,622 - supervised_finetuning - INFO - Epoch 1270 - supervised_loss/batch: 0.8146\n",
            "2025-11-01 14:26:06,622 - supervised_finetuning - INFO - Epoch 1270 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:06,623 - supervised_finetuning - INFO - Epoch 1270 - learning_rate: 0.0001\n",
            "Classifier Epoch 61/100:  81% 17/21 [00:00<00:00, 165.09it/s, Loss=0.4467, Acc=85.71%]2025-11-01 14:26:06,699 - supervised_finetuning - INFO - Epoch 1280 - supervised_loss/batch: 0.4467\n",
            "2025-11-01 14:26:06,700 - supervised_finetuning - INFO - Epoch 1280 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:06,700 - supervised_finetuning - INFO - Epoch 1280 - learning_rate: 0.0001\n",
            "Classifier Epoch 61/100: 100% 21/21 [00:00<00:00, 157.47it/s, Loss=0.4467, Acc=85.71%]\n",
            "2025-11-01 14:26:06,719 - supervised_finetuning - INFO - Epoch 61:\n",
            "2025-11-01 14:26:06,719 - supervised_finetuning - INFO -   Train Loss: 0.7711, Train Acc: 66.90%\n",
            "2025-11-01 14:26:06,719 - supervised_finetuning - INFO -   Train F1: 66.38%, Train Recall: 66.90%\n",
            "2025-11-01 14:26:06,719 - supervised_finetuning - INFO -   Train Precision: 66.24%\n",
            "2025-11-01 14:26:06,719 - supervised_finetuning - INFO - Epoch 60 - supervised_loss/epoch: 0.7711\n",
            "2025-11-01 14:26:06,719 - supervised_finetuning - INFO - Epoch 60 - supervised_accuracy/epoch: 66.8998\n",
            "2025-11-01 14:26:06,720 - supervised_finetuning - INFO - Epoch 60 - supervised_f1/epoch: 66.3829\n",
            "2025-11-01 14:26:06,720 - supervised_finetuning - INFO - Epoch 60 - supervised_recall/epoch: 66.8998\n",
            "2025-11-01 14:26:06,720 - supervised_finetuning - INFO - Epoch 60 - supervised_precision/epoch: 66.2445\n",
            "Classifier Epoch 62/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8057, Acc=64.06%]2025-11-01 14:26:06,726 - supervised_finetuning - INFO - Epoch 1281 - supervised_loss/batch: 0.8057\n",
            "2025-11-01 14:26:06,726 - supervised_finetuning - INFO - Epoch 1281 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:06,726 - supervised_finetuning - INFO - Epoch 1281 - learning_rate: 0.0001\n",
            "Classifier Epoch 62/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9136, Acc=57.81%]2025-11-01 14:26:06,774 - supervised_finetuning - INFO - Epoch 1291 - supervised_loss/batch: 0.9136\n",
            "2025-11-01 14:26:06,774 - supervised_finetuning - INFO - Epoch 1291 - supervised_accuracy/batch: 57.8125\n",
            "2025-11-01 14:26:06,775 - supervised_finetuning - INFO - Epoch 1291 - learning_rate: 0.0001\n",
            "Classifier Epoch 62/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5469, Acc=71.43%]2025-11-01 14:26:06,821 - supervised_finetuning - INFO - Epoch 1301 - supervised_loss/batch: 0.5469\n",
            "2025-11-01 14:26:06,821 - supervised_finetuning - INFO - Epoch 1301 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:06,821 - supervised_finetuning - INFO - Epoch 1301 - learning_rate: 0.0001\n",
            "Classifier Epoch 62/100: 100% 21/21 [00:00<00:00, 207.53it/s, Loss=0.5469, Acc=71.43%]\n",
            "2025-11-01 14:26:06,838 - supervised_finetuning - INFO - Epoch 62:\n",
            "2025-11-01 14:26:06,838 - supervised_finetuning - INFO -   Train Loss: 0.7742, Train Acc: 67.99%\n",
            "2025-11-01 14:26:06,838 - supervised_finetuning - INFO -   Train F1: 67.61%, Train Recall: 67.99%\n",
            "2025-11-01 14:26:06,838 - supervised_finetuning - INFO -   Train Precision: 67.51%\n",
            "2025-11-01 14:26:06,839 - supervised_finetuning - INFO - Epoch 61 - supervised_loss/epoch: 0.7742\n",
            "2025-11-01 14:26:06,839 - supervised_finetuning - INFO - Epoch 61 - supervised_accuracy/epoch: 67.9876\n",
            "2025-11-01 14:26:06,839 - supervised_finetuning - INFO - Epoch 61 - supervised_f1/epoch: 67.6101\n",
            "2025-11-01 14:26:06,839 - supervised_finetuning - INFO - Epoch 61 - supervised_recall/epoch: 67.9876\n",
            "2025-11-01 14:26:06,839 - supervised_finetuning - INFO - Epoch 61 - supervised_precision/epoch: 67.5113\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:08,515 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 67.99%\n",
            "Classifier Epoch 63/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8487, Acc=64.06%]2025-11-01 14:26:08,529 - supervised_finetuning - INFO - Epoch 1302 - supervised_loss/batch: 0.8487\n",
            "2025-11-01 14:26:08,529 - supervised_finetuning - INFO - Epoch 1302 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:08,529 - supervised_finetuning - INFO - Epoch 1302 - learning_rate: 0.0001\n",
            "Classifier Epoch 63/100:  48% 10/21 [00:00<00:00, 93.42it/s, Loss=0.7494, Acc=62.50%]2025-11-01 14:26:08,631 - supervised_finetuning - INFO - Epoch 1312 - supervised_loss/batch: 0.7494\n",
            "2025-11-01 14:26:08,632 - supervised_finetuning - INFO - Epoch 1312 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:26:08,632 - supervised_finetuning - INFO - Epoch 1312 - learning_rate: 0.0001\n",
            "Classifier Epoch 63/100:  95% 20/21 [00:00<00:00, 96.10it/s, Loss=0.6714, Acc=85.71%]2025-11-01 14:26:08,735 - supervised_finetuning - INFO - Epoch 1322 - supervised_loss/batch: 0.6714\n",
            "2025-11-01 14:26:08,735 - supervised_finetuning - INFO - Epoch 1322 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:08,736 - supervised_finetuning - INFO - Epoch 1322 - learning_rate: 0.0001\n",
            "Classifier Epoch 63/100: 100% 21/21 [00:00<00:00, 95.17it/s, Loss=0.6714, Acc=85.71%]\n",
            "2025-11-01 14:26:08,775 - supervised_finetuning - INFO - Epoch 63:\n",
            "2025-11-01 14:26:08,776 - supervised_finetuning - INFO -   Train Loss: 0.7763, Train Acc: 67.91%\n",
            "2025-11-01 14:26:08,776 - supervised_finetuning - INFO -   Train F1: 67.58%, Train Recall: 67.91%\n",
            "2025-11-01 14:26:08,777 - supervised_finetuning - INFO -   Train Precision: 67.45%\n",
            "2025-11-01 14:26:08,778 - supervised_finetuning - INFO - Epoch 62 - supervised_loss/epoch: 0.7763\n",
            "2025-11-01 14:26:08,779 - supervised_finetuning - INFO - Epoch 62 - supervised_accuracy/epoch: 67.9099\n",
            "2025-11-01 14:26:08,779 - supervised_finetuning - INFO - Epoch 62 - supervised_f1/epoch: 67.5804\n",
            "2025-11-01 14:26:08,780 - supervised_finetuning - INFO - Epoch 62 - supervised_recall/epoch: 67.9099\n",
            "2025-11-01 14:26:08,780 - supervised_finetuning - INFO - Epoch 62 - supervised_precision/epoch: 67.4511\n",
            "Classifier Epoch 64/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6625, Acc=70.31%]2025-11-01 14:26:08,803 - supervised_finetuning - INFO - Epoch 1323 - supervised_loss/batch: 0.6625\n",
            "2025-11-01 14:26:08,803 - supervised_finetuning - INFO - Epoch 1323 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:26:08,804 - supervised_finetuning - INFO - Epoch 1323 - learning_rate: 0.0001\n",
            "Classifier Epoch 64/100:  38% 8/21 [00:00<00:00, 74.53it/s, Loss=0.9087, Acc=53.12%]2025-11-01 14:26:08,923 - supervised_finetuning - INFO - Epoch 1333 - supervised_loss/batch: 0.9087\n",
            "2025-11-01 14:26:08,926 - supervised_finetuning - INFO - Epoch 1333 - supervised_accuracy/batch: 53.1250\n",
            "2025-11-01 14:26:08,926 - supervised_finetuning - INFO - Epoch 1333 - learning_rate: 0.0001\n",
            "Classifier Epoch 64/100:  81% 17/21 [00:00<00:00, 81.49it/s, Loss=1.2412, Acc=57.14%]2025-11-01 14:26:09,049 - supervised_finetuning - INFO - Epoch 1343 - supervised_loss/batch: 1.2412\n",
            "2025-11-01 14:26:09,049 - supervised_finetuning - INFO - Epoch 1343 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:26:09,050 - supervised_finetuning - INFO - Epoch 1343 - learning_rate: 0.0001\n",
            "Classifier Epoch 64/100: 100% 21/21 [00:00<00:00, 78.49it/s, Loss=1.2412, Acc=57.14%]\n",
            "2025-11-01 14:26:09,083 - supervised_finetuning - INFO - Epoch 64:\n",
            "2025-11-01 14:26:09,084 - supervised_finetuning - INFO -   Train Loss: 0.8017, Train Acc: 66.82%\n",
            "2025-11-01 14:26:09,084 - supervised_finetuning - INFO -   Train F1: 66.28%, Train Recall: 66.82%\n",
            "2025-11-01 14:26:09,084 - supervised_finetuning - INFO -   Train Precision: 66.25%\n",
            "2025-11-01 14:26:09,084 - supervised_finetuning - INFO - Epoch 63 - supervised_loss/epoch: 0.8017\n",
            "2025-11-01 14:26:09,085 - supervised_finetuning - INFO - Epoch 63 - supervised_accuracy/epoch: 66.8221\n",
            "2025-11-01 14:26:09,085 - supervised_finetuning - INFO - Epoch 63 - supervised_f1/epoch: 66.2759\n",
            "2025-11-01 14:26:09,085 - supervised_finetuning - INFO - Epoch 63 - supervised_recall/epoch: 66.8221\n",
            "2025-11-01 14:26:09,086 - supervised_finetuning - INFO - Epoch 63 - supervised_precision/epoch: 66.2490\n",
            "Classifier Epoch 65/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7660, Acc=64.06%]2025-11-01 14:26:09,096 - supervised_finetuning - INFO - Epoch 1344 - supervised_loss/batch: 0.7660\n",
            "2025-11-01 14:26:09,096 - supervised_finetuning - INFO - Epoch 1344 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:09,096 - supervised_finetuning - INFO - Epoch 1344 - learning_rate: 0.0001\n",
            "Classifier Epoch 65/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9301, Acc=65.62%]2025-11-01 14:26:09,195 - supervised_finetuning - INFO - Epoch 1354 - supervised_loss/batch: 0.9301\n",
            "2025-11-01 14:26:09,195 - supervised_finetuning - INFO - Epoch 1354 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:09,197 - supervised_finetuning - INFO - Epoch 1354 - learning_rate: 0.0001\n",
            "Classifier Epoch 65/100:  52% 11/21 [00:00<00:00, 99.57it/s, Loss=0.8467, Acc=28.57%]2025-11-01 14:26:09,306 - supervised_finetuning - INFO - Epoch 1364 - supervised_loss/batch: 0.8467\n",
            "2025-11-01 14:26:09,308 - supervised_finetuning - INFO - Epoch 1364 - supervised_accuracy/batch: 28.5714\n",
            "2025-11-01 14:26:09,308 - supervised_finetuning - INFO - Epoch 1364 - learning_rate: 0.0001\n",
            "Classifier Epoch 65/100: 100% 21/21 [00:00<00:00, 94.65it/s, Loss=0.8467, Acc=28.57%]\n",
            "2025-11-01 14:26:09,347 - supervised_finetuning - INFO - Epoch 65:\n",
            "2025-11-01 14:26:09,347 - supervised_finetuning - INFO -   Train Loss: 0.8006, Train Acc: 67.75%\n",
            "2025-11-01 14:26:09,347 - supervised_finetuning - INFO -   Train F1: 67.18%, Train Recall: 67.75%\n",
            "2025-11-01 14:26:09,348 - supervised_finetuning - INFO -   Train Precision: 67.16%\n",
            "2025-11-01 14:26:09,348 - supervised_finetuning - INFO - Epoch 64 - supervised_loss/epoch: 0.8006\n",
            "2025-11-01 14:26:09,348 - supervised_finetuning - INFO - Epoch 64 - supervised_accuracy/epoch: 67.7545\n",
            "2025-11-01 14:26:09,348 - supervised_finetuning - INFO - Epoch 64 - supervised_f1/epoch: 67.1764\n",
            "2025-11-01 14:26:09,348 - supervised_finetuning - INFO - Epoch 64 - supervised_recall/epoch: 67.7545\n",
            "2025-11-01 14:26:09,348 - supervised_finetuning - INFO - Epoch 64 - supervised_precision/epoch: 67.1642\n",
            "Classifier Epoch 66/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7754, Acc=70.31%]2025-11-01 14:26:09,358 - supervised_finetuning - INFO - Epoch 1365 - supervised_loss/batch: 0.7754\n",
            "2025-11-01 14:26:09,358 - supervised_finetuning - INFO - Epoch 1365 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:26:09,358 - supervised_finetuning - INFO - Epoch 1365 - learning_rate: 0.0001\n",
            "Classifier Epoch 66/100:  48% 10/21 [00:00<00:00, 98.44it/s, Loss=0.7062, Acc=70.31%]2025-11-01 14:26:09,461 - supervised_finetuning - INFO - Epoch 1375 - supervised_loss/batch: 0.7062\n",
            "2025-11-01 14:26:09,463 - supervised_finetuning - INFO - Epoch 1375 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:26:09,463 - supervised_finetuning - INFO - Epoch 1375 - learning_rate: 0.0001\n",
            "Classifier Epoch 66/100:  95% 20/21 [00:00<00:00, 92.96it/s, Loss=0.6326, Acc=85.71%]2025-11-01 14:26:09,575 - supervised_finetuning - INFO - Epoch 1385 - supervised_loss/batch: 0.6326\n",
            "2025-11-01 14:26:09,576 - supervised_finetuning - INFO - Epoch 1385 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:09,578 - supervised_finetuning - INFO - Epoch 1385 - learning_rate: 0.0001\n",
            "Classifier Epoch 66/100: 100% 21/21 [00:00<00:00, 91.38it/s, Loss=0.6326, Acc=85.71%]\n",
            "2025-11-01 14:26:09,616 - supervised_finetuning - INFO - Epoch 66:\n",
            "2025-11-01 14:26:09,616 - supervised_finetuning - INFO -   Train Loss: 0.7711, Train Acc: 68.69%\n",
            "2025-11-01 14:26:09,616 - supervised_finetuning - INFO -   Train F1: 68.41%, Train Recall: 68.69%\n",
            "2025-11-01 14:26:09,616 - supervised_finetuning - INFO -   Train Precision: 68.39%\n",
            "2025-11-01 14:26:09,616 - supervised_finetuning - INFO - Epoch 65 - supervised_loss/epoch: 0.7711\n",
            "2025-11-01 14:26:09,616 - supervised_finetuning - INFO - Epoch 65 - supervised_accuracy/epoch: 68.6869\n",
            "2025-11-01 14:26:09,616 - supervised_finetuning - INFO - Epoch 65 - supervised_f1/epoch: 68.4146\n",
            "2025-11-01 14:26:09,616 - supervised_finetuning - INFO - Epoch 65 - supervised_recall/epoch: 68.6869\n",
            "2025-11-01 14:26:09,617 - supervised_finetuning - INFO - Epoch 65 - supervised_precision/epoch: 68.3898\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:11,236 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 68.69%\n",
            "Classifier Epoch 67/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7258, Acc=65.62%]2025-11-01 14:26:11,256 - supervised_finetuning - INFO - Epoch 1386 - supervised_loss/batch: 0.7258\n",
            "2025-11-01 14:26:11,256 - supervised_finetuning - INFO - Epoch 1386 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:11,257 - supervised_finetuning - INFO - Epoch 1386 - learning_rate: 0.0001\n",
            "Classifier Epoch 67/100:  38% 8/21 [00:00<00:00, 77.59it/s, Loss=0.9269, Acc=56.25%]2025-11-01 14:26:11,374 - supervised_finetuning - INFO - Epoch 1396 - supervised_loss/batch: 0.9269\n",
            "2025-11-01 14:26:11,374 - supervised_finetuning - INFO - Epoch 1396 - supervised_accuracy/batch: 56.2500\n",
            "2025-11-01 14:26:11,374 - supervised_finetuning - INFO - Epoch 1396 - learning_rate: 0.0001\n",
            "Classifier Epoch 67/100:  90% 19/21 [00:00<00:00, 92.72it/s, Loss=1.0540, Acc=42.86%]2025-11-01 14:26:11,467 - supervised_finetuning - INFO - Epoch 1406 - supervised_loss/batch: 1.0540\n",
            "2025-11-01 14:26:11,468 - supervised_finetuning - INFO - Epoch 1406 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:26:11,468 - supervised_finetuning - INFO - Epoch 1406 - learning_rate: 0.0001\n",
            "Classifier Epoch 67/100: 100% 21/21 [00:00<00:00, 91.51it/s, Loss=1.0540, Acc=42.86%]\n",
            "2025-11-01 14:26:11,509 - supervised_finetuning - INFO - Epoch 67:\n",
            "2025-11-01 14:26:11,509 - supervised_finetuning - INFO -   Train Loss: 0.7996, Train Acc: 67.06%\n",
            "2025-11-01 14:26:11,509 - supervised_finetuning - INFO -   Train F1: 66.48%, Train Recall: 67.06%\n",
            "2025-11-01 14:26:11,510 - supervised_finetuning - INFO -   Train Precision: 66.48%\n",
            "2025-11-01 14:26:11,510 - supervised_finetuning - INFO - Epoch 66 - supervised_loss/epoch: 0.7996\n",
            "2025-11-01 14:26:11,511 - supervised_finetuning - INFO - Epoch 66 - supervised_accuracy/epoch: 67.0552\n",
            "2025-11-01 14:26:11,513 - supervised_finetuning - INFO - Epoch 66 - supervised_f1/epoch: 66.4815\n",
            "2025-11-01 14:26:11,513 - supervised_finetuning - INFO - Epoch 66 - supervised_recall/epoch: 67.0552\n",
            "2025-11-01 14:26:11,513 - supervised_finetuning - INFO - Epoch 66 - supervised_precision/epoch: 66.4847\n",
            "Classifier Epoch 68/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6621, Acc=78.12%]2025-11-01 14:26:11,527 - supervised_finetuning - INFO - Epoch 1407 - supervised_loss/batch: 0.6621\n",
            "2025-11-01 14:26:11,527 - supervised_finetuning - INFO - Epoch 1407 - supervised_accuracy/batch: 78.1250\n",
            "2025-11-01 14:26:11,527 - supervised_finetuning - INFO - Epoch 1407 - learning_rate: 0.0001\n",
            "Classifier Epoch 68/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7229, Acc=65.62%]2025-11-01 14:26:11,587 - supervised_finetuning - INFO - Epoch 1417 - supervised_loss/batch: 0.7229\n",
            "2025-11-01 14:26:11,587 - supervised_finetuning - INFO - Epoch 1417 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:11,587 - supervised_finetuning - INFO - Epoch 1417 - learning_rate: 0.0001\n",
            "Classifier Epoch 68/100:  76% 16/21 [00:00<00:00, 159.51it/s, Loss=0.8204, Acc=57.14%]2025-11-01 14:26:11,635 - supervised_finetuning - INFO - Epoch 1427 - supervised_loss/batch: 0.8204\n",
            "2025-11-01 14:26:11,635 - supervised_finetuning - INFO - Epoch 1427 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:26:11,636 - supervised_finetuning - INFO - Epoch 1427 - learning_rate: 0.0001\n",
            "Classifier Epoch 68/100: 100% 21/21 [00:00<00:00, 172.10it/s, Loss=0.8204, Acc=57.14%]\n",
            "2025-11-01 14:26:11,681 - supervised_finetuning - INFO - Epoch 68:\n",
            "2025-11-01 14:26:11,682 - supervised_finetuning - INFO -   Train Loss: 0.7950, Train Acc: 67.13%\n",
            "2025-11-01 14:26:11,682 - supervised_finetuning - INFO -   Train F1: 66.92%, Train Recall: 67.13%\n",
            "2025-11-01 14:26:11,682 - supervised_finetuning - INFO -   Train Precision: 66.96%\n",
            "2025-11-01 14:26:11,684 - supervised_finetuning - INFO - Epoch 67 - supervised_loss/epoch: 0.7950\n",
            "2025-11-01 14:26:11,684 - supervised_finetuning - INFO - Epoch 67 - supervised_accuracy/epoch: 67.1329\n",
            "2025-11-01 14:26:11,685 - supervised_finetuning - INFO - Epoch 67 - supervised_f1/epoch: 66.9197\n",
            "2025-11-01 14:26:11,685 - supervised_finetuning - INFO - Epoch 67 - supervised_recall/epoch: 67.1329\n",
            "2025-11-01 14:26:11,687 - supervised_finetuning - INFO - Epoch 67 - supervised_precision/epoch: 66.9560\n",
            "Classifier Epoch 69/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8666, Acc=60.94%]2025-11-01 14:26:11,707 - supervised_finetuning - INFO - Epoch 1428 - supervised_loss/batch: 0.8666\n",
            "2025-11-01 14:26:11,708 - supervised_finetuning - INFO - Epoch 1428 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:26:11,708 - supervised_finetuning - INFO - Epoch 1428 - learning_rate: 0.0001\n",
            "Classifier Epoch 69/100:  33% 7/21 [00:00<00:00, 68.43it/s, Loss=0.6865, Acc=79.69%]2025-11-01 14:26:11,820 - supervised_finetuning - INFO - Epoch 1438 - supervised_loss/batch: 0.6865\n",
            "2025-11-01 14:26:11,821 - supervised_finetuning - INFO - Epoch 1438 - supervised_accuracy/batch: 79.6875\n",
            "2025-11-01 14:26:11,821 - supervised_finetuning - INFO - Epoch 1438 - learning_rate: 0.0001\n",
            "Classifier Epoch 69/100:  33% 7/21 [00:00<00:00, 68.43it/s, Loss=0.3776, Acc=100.00%]2025-11-01 14:26:11,882 - supervised_finetuning - INFO - Epoch 1448 - supervised_loss/batch: 0.3776\n",
            "2025-11-01 14:26:11,882 - supervised_finetuning - INFO - Epoch 1448 - supervised_accuracy/batch: 100.0000\n",
            "2025-11-01 14:26:11,882 - supervised_finetuning - INFO - Epoch 1448 - learning_rate: 0.0001\n",
            "Classifier Epoch 69/100: 100% 21/21 [00:00<00:00, 107.94it/s, Loss=0.3776, Acc=100.00%]\n",
            "2025-11-01 14:26:11,899 - supervised_finetuning - INFO - Epoch 69:\n",
            "2025-11-01 14:26:11,899 - supervised_finetuning - INFO -   Train Loss: 0.7622, Train Acc: 67.99%\n",
            "2025-11-01 14:26:11,899 - supervised_finetuning - INFO -   Train F1: 67.49%, Train Recall: 67.99%\n",
            "2025-11-01 14:26:11,899 - supervised_finetuning - INFO -   Train Precision: 67.36%\n",
            "2025-11-01 14:26:11,900 - supervised_finetuning - INFO - Epoch 68 - supervised_loss/epoch: 0.7622\n",
            "2025-11-01 14:26:11,900 - supervised_finetuning - INFO - Epoch 68 - supervised_accuracy/epoch: 67.9876\n",
            "2025-11-01 14:26:11,900 - supervised_finetuning - INFO - Epoch 68 - supervised_f1/epoch: 67.4894\n",
            "2025-11-01 14:26:11,901 - supervised_finetuning - INFO - Epoch 68 - supervised_recall/epoch: 67.9876\n",
            "2025-11-01 14:26:11,901 - supervised_finetuning - INFO - Epoch 68 - supervised_precision/epoch: 67.3575\n",
            "Classifier Epoch 70/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9493, Acc=60.94%]2025-11-01 14:26:11,906 - supervised_finetuning - INFO - Epoch 1449 - supervised_loss/batch: 0.9493\n",
            "2025-11-01 14:26:11,907 - supervised_finetuning - INFO - Epoch 1449 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:26:11,907 - supervised_finetuning - INFO - Epoch 1449 - learning_rate: 0.0001\n",
            "Classifier Epoch 70/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6967, Acc=68.75%]2025-11-01 14:26:11,967 - supervised_finetuning - INFO - Epoch 1459 - supervised_loss/batch: 0.6967\n",
            "2025-11-01 14:26:11,967 - supervised_finetuning - INFO - Epoch 1459 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:11,968 - supervised_finetuning - INFO - Epoch 1459 - learning_rate: 0.0001\n",
            "Classifier Epoch 70/100:  71% 15/21 [00:00<00:00, 136.84it/s, Loss=0.8810, Acc=42.86%]2025-11-01 14:26:12,069 - supervised_finetuning - INFO - Epoch 1469 - supervised_loss/batch: 0.8810\n",
            "2025-11-01 14:26:12,069 - supervised_finetuning - INFO - Epoch 1469 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:26:12,069 - supervised_finetuning - INFO - Epoch 1469 - learning_rate: 0.0001\n",
            "Classifier Epoch 70/100: 100% 21/21 [00:00<00:00, 124.83it/s, Loss=0.8810, Acc=42.86%]\n",
            "2025-11-01 14:26:12,113 - supervised_finetuning - INFO - Epoch 70:\n",
            "2025-11-01 14:26:12,114 - supervised_finetuning - INFO -   Train Loss: 0.7785, Train Acc: 67.52%\n",
            "2025-11-01 14:26:12,114 - supervised_finetuning - INFO -   Train F1: 67.16%, Train Recall: 67.52%\n",
            "2025-11-01 14:26:12,114 - supervised_finetuning - INFO -   Train Precision: 67.22%\n",
            "2025-11-01 14:26:12,115 - supervised_finetuning - INFO - Epoch 69 - supervised_loss/epoch: 0.7785\n",
            "2025-11-01 14:26:12,115 - supervised_finetuning - INFO - Epoch 69 - supervised_accuracy/epoch: 67.5214\n",
            "2025-11-01 14:26:12,116 - supervised_finetuning - INFO - Epoch 69 - supervised_f1/epoch: 67.1609\n",
            "2025-11-01 14:26:12,116 - supervised_finetuning - INFO - Epoch 69 - supervised_recall/epoch: 67.5214\n",
            "2025-11-01 14:26:12,116 - supervised_finetuning - INFO - Epoch 69 - supervised_precision/epoch: 67.2210\n",
            "Classifier Epoch 71/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9904, Acc=57.81%]2025-11-01 14:26:12,131 - supervised_finetuning - INFO - Epoch 1470 - supervised_loss/batch: 0.9904\n",
            "2025-11-01 14:26:12,133 - supervised_finetuning - INFO - Epoch 1470 - supervised_accuracy/batch: 57.8125\n",
            "2025-11-01 14:26:12,133 - supervised_finetuning - INFO - Epoch 1470 - learning_rate: 0.0001\n",
            "Classifier Epoch 71/100:  48% 10/21 [00:00<00:00, 99.61it/s, Loss=0.7894, Acc=67.19%]2025-11-01 14:26:12,226 - supervised_finetuning - INFO - Epoch 1480 - supervised_loss/batch: 0.7894\n",
            "2025-11-01 14:26:12,227 - supervised_finetuning - INFO - Epoch 1480 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:26:12,229 - supervised_finetuning - INFO - Epoch 1480 - learning_rate: 0.0001\n",
            "Classifier Epoch 71/100:  95% 20/21 [00:00<00:00, 93.68it/s, Loss=0.4581, Acc=85.71%]2025-11-01 14:26:12,338 - supervised_finetuning - INFO - Epoch 1490 - supervised_loss/batch: 0.4581\n",
            "2025-11-01 14:26:12,340 - supervised_finetuning - INFO - Epoch 1490 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:12,340 - supervised_finetuning - INFO - Epoch 1490 - learning_rate: 0.0001\n",
            "Classifier Epoch 71/100: 100% 21/21 [00:00<00:00, 94.34it/s, Loss=0.4581, Acc=85.71%]\n",
            "2025-11-01 14:26:12,366 - supervised_finetuning - INFO - Epoch 71:\n",
            "2025-11-01 14:26:12,366 - supervised_finetuning - INFO -   Train Loss: 0.7671, Train Acc: 67.29%\n",
            "2025-11-01 14:26:12,366 - supervised_finetuning - INFO -   Train F1: 66.84%, Train Recall: 67.29%\n",
            "2025-11-01 14:26:12,366 - supervised_finetuning - INFO -   Train Precision: 66.92%\n",
            "2025-11-01 14:26:12,366 - supervised_finetuning - INFO - Epoch 70 - supervised_loss/epoch: 0.7671\n",
            "2025-11-01 14:26:12,367 - supervised_finetuning - INFO - Epoch 70 - supervised_accuracy/epoch: 67.2883\n",
            "2025-11-01 14:26:12,367 - supervised_finetuning - INFO - Epoch 70 - supervised_f1/epoch: 66.8374\n",
            "2025-11-01 14:26:12,367 - supervised_finetuning - INFO - Epoch 70 - supervised_recall/epoch: 67.2883\n",
            "2025-11-01 14:26:12,368 - supervised_finetuning - INFO - Epoch 70 - supervised_precision/epoch: 66.9180\n",
            "Classifier Epoch 72/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7713, Acc=68.75%]2025-11-01 14:26:12,378 - supervised_finetuning - INFO - Epoch 1491 - supervised_loss/batch: 0.7713\n",
            "2025-11-01 14:26:12,379 - supervised_finetuning - INFO - Epoch 1491 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:12,379 - supervised_finetuning - INFO - Epoch 1491 - learning_rate: 0.0001\n",
            "Classifier Epoch 72/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7550, Acc=73.44%]2025-11-01 14:26:12,451 - supervised_finetuning - INFO - Epoch 1501 - supervised_loss/batch: 0.7550\n",
            "2025-11-01 14:26:12,452 - supervised_finetuning - INFO - Epoch 1501 - supervised_accuracy/batch: 73.4375\n",
            "2025-11-01 14:26:12,452 - supervised_finetuning - INFO - Epoch 1501 - learning_rate: 0.0001\n",
            "Classifier Epoch 72/100:  67% 14/21 [00:00<00:00, 133.10it/s, Loss=0.5256, Acc=71.43%]2025-11-01 14:26:12,540 - supervised_finetuning - INFO - Epoch 1511 - supervised_loss/batch: 0.5256\n",
            "2025-11-01 14:26:12,540 - supervised_finetuning - INFO - Epoch 1511 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:12,540 - supervised_finetuning - INFO - Epoch 1511 - learning_rate: 0.0001\n",
            "Classifier Epoch 72/100: 100% 21/21 [00:00<00:00, 122.51it/s, Loss=0.5256, Acc=71.43%]\n",
            "2025-11-01 14:26:12,593 - supervised_finetuning - INFO - Epoch 72:\n",
            "2025-11-01 14:26:12,597 - supervised_finetuning - INFO -   Train Loss: 0.7646, Train Acc: 67.29%\n",
            "2025-11-01 14:26:12,597 - supervised_finetuning - INFO -   Train F1: 66.92%, Train Recall: 67.29%\n",
            "2025-11-01 14:26:12,597 - supervised_finetuning - INFO -   Train Precision: 67.03%\n",
            "2025-11-01 14:26:12,598 - supervised_finetuning - INFO - Epoch 71 - supervised_loss/epoch: 0.7646\n",
            "2025-11-01 14:26:12,599 - supervised_finetuning - INFO - Epoch 71 - supervised_accuracy/epoch: 67.2883\n",
            "2025-11-01 14:26:12,600 - supervised_finetuning - INFO - Epoch 71 - supervised_f1/epoch: 66.9151\n",
            "2025-11-01 14:26:12,600 - supervised_finetuning - INFO - Epoch 71 - supervised_recall/epoch: 67.2883\n",
            "2025-11-01 14:26:12,602 - supervised_finetuning - INFO - Epoch 71 - supervised_precision/epoch: 67.0284\n",
            "Classifier Epoch 73/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6589, Acc=70.31%]2025-11-01 14:26:12,617 - supervised_finetuning - INFO - Epoch 1512 - supervised_loss/batch: 0.6589\n",
            "2025-11-01 14:26:12,617 - supervised_finetuning - INFO - Epoch 1512 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:26:12,617 - supervised_finetuning - INFO - Epoch 1512 - learning_rate: 0.0001\n",
            "Classifier Epoch 73/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5951, Acc=75.00%]2025-11-01 14:26:12,697 - supervised_finetuning - INFO - Epoch 1522 - supervised_loss/batch: 0.5951\n",
            "2025-11-01 14:26:12,698 - supervised_finetuning - INFO - Epoch 1522 - supervised_accuracy/batch: 75.0000\n",
            "2025-11-01 14:26:12,698 - supervised_finetuning - INFO - Epoch 1522 - learning_rate: 0.0001\n",
            "Classifier Epoch 73/100:  57% 12/21 [00:00<00:00, 102.17it/s, Loss=0.8015, Acc=71.43%]2025-11-01 14:26:12,783 - supervised_finetuning - INFO - Epoch 1532 - supervised_loss/batch: 0.8015\n",
            "2025-11-01 14:26:12,783 - supervised_finetuning - INFO - Epoch 1532 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:12,784 - supervised_finetuning - INFO - Epoch 1532 - learning_rate: 0.0001\n",
            "Classifier Epoch 73/100: 100% 21/21 [00:00<00:00, 115.98it/s, Loss=0.8015, Acc=71.43%]\n",
            "2025-11-01 14:26:12,803 - supervised_finetuning - INFO - Epoch 73:\n",
            "2025-11-01 14:26:12,803 - supervised_finetuning - INFO -   Train Loss: 0.7743, Train Acc: 67.60%\n",
            "2025-11-01 14:26:12,803 - supervised_finetuning - INFO -   Train F1: 67.22%, Train Recall: 67.60%\n",
            "2025-11-01 14:26:12,804 - supervised_finetuning - INFO -   Train Precision: 67.36%\n",
            "2025-11-01 14:26:12,804 - supervised_finetuning - INFO - Epoch 72 - supervised_loss/epoch: 0.7743\n",
            "2025-11-01 14:26:12,804 - supervised_finetuning - INFO - Epoch 72 - supervised_accuracy/epoch: 67.5991\n",
            "2025-11-01 14:26:12,804 - supervised_finetuning - INFO - Epoch 72 - supervised_f1/epoch: 67.2170\n",
            "2025-11-01 14:26:12,804 - supervised_finetuning - INFO - Epoch 72 - supervised_recall/epoch: 67.5991\n",
            "2025-11-01 14:26:12,805 - supervised_finetuning - INFO - Epoch 72 - supervised_precision/epoch: 67.3635\n",
            "Classifier Epoch 74/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5906, Acc=82.81%]2025-11-01 14:26:12,812 - supervised_finetuning - INFO - Epoch 1533 - supervised_loss/batch: 0.5906\n",
            "2025-11-01 14:26:12,812 - supervised_finetuning - INFO - Epoch 1533 - supervised_accuracy/batch: 82.8125\n",
            "2025-11-01 14:26:12,812 - supervised_finetuning - INFO - Epoch 1533 - learning_rate: 0.0001\n",
            "Classifier Epoch 74/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7293, Acc=65.62%]2025-11-01 14:26:12,896 - supervised_finetuning - INFO - Epoch 1543 - supervised_loss/batch: 0.7293\n",
            "2025-11-01 14:26:12,897 - supervised_finetuning - INFO - Epoch 1543 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:12,897 - supervised_finetuning - INFO - Epoch 1543 - learning_rate: 0.0001\n",
            "Classifier Epoch 74/100:  62% 13/21 [00:00<00:00, 124.11it/s, Loss=0.9260, Acc=42.86%]2025-11-01 14:26:12,967 - supervised_finetuning - INFO - Epoch 1553 - supervised_loss/batch: 0.9260\n",
            "2025-11-01 14:26:12,967 - supervised_finetuning - INFO - Epoch 1553 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:26:12,968 - supervised_finetuning - INFO - Epoch 1553 - learning_rate: 0.0001\n",
            "Classifier Epoch 74/100: 100% 21/21 [00:00<00:00, 128.98it/s, Loss=0.9260, Acc=42.86%]\n",
            "2025-11-01 14:26:12,989 - supervised_finetuning - INFO - Epoch 74:\n",
            "2025-11-01 14:26:12,990 - supervised_finetuning - INFO -   Train Loss: 0.7822, Train Acc: 67.68%\n",
            "2025-11-01 14:26:12,990 - supervised_finetuning - INFO -   Train F1: 67.20%, Train Recall: 67.68%\n",
            "2025-11-01 14:26:12,990 - supervised_finetuning - INFO -   Train Precision: 66.98%\n",
            "2025-11-01 14:26:12,990 - supervised_finetuning - INFO - Epoch 73 - supervised_loss/epoch: 0.7822\n",
            "2025-11-01 14:26:12,990 - supervised_finetuning - INFO - Epoch 73 - supervised_accuracy/epoch: 67.6768\n",
            "2025-11-01 14:26:12,990 - supervised_finetuning - INFO - Epoch 73 - supervised_f1/epoch: 67.2013\n",
            "2025-11-01 14:26:12,991 - supervised_finetuning - INFO - Epoch 73 - supervised_recall/epoch: 67.6768\n",
            "2025-11-01 14:26:12,991 - supervised_finetuning - INFO - Epoch 73 - supervised_precision/epoch: 66.9833\n",
            "Classifier Epoch 75/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8243, Acc=57.81%]2025-11-01 14:26:12,998 - supervised_finetuning - INFO - Epoch 1554 - supervised_loss/batch: 0.8243\n",
            "2025-11-01 14:26:12,998 - supervised_finetuning - INFO - Epoch 1554 - supervised_accuracy/batch: 57.8125\n",
            "2025-11-01 14:26:12,999 - supervised_finetuning - INFO - Epoch 1554 - learning_rate: 0.0001\n",
            "Classifier Epoch 75/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7490, Acc=68.75%]2025-11-01 14:26:13,066 - supervised_finetuning - INFO - Epoch 1564 - supervised_loss/batch: 0.7490\n",
            "2025-11-01 14:26:13,066 - supervised_finetuning - INFO - Epoch 1564 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:13,066 - supervised_finetuning - INFO - Epoch 1564 - learning_rate: 0.0001\n",
            "Classifier Epoch 75/100:  71% 15/21 [00:00<00:00, 141.00it/s, Loss=0.4461, Acc=85.71%]2025-11-01 14:26:13,150 - supervised_finetuning - INFO - Epoch 1574 - supervised_loss/batch: 0.4461\n",
            "2025-11-01 14:26:13,152 - supervised_finetuning - INFO - Epoch 1574 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:13,154 - supervised_finetuning - INFO - Epoch 1574 - learning_rate: 0.0001\n",
            "Classifier Epoch 75/100: 100% 21/21 [00:00<00:00, 128.74it/s, Loss=0.4461, Acc=85.71%]\n",
            "2025-11-01 14:26:13,197 - supervised_finetuning - INFO - Epoch 75:\n",
            "2025-11-01 14:26:13,197 - supervised_finetuning - INFO -   Train Loss: 0.7596, Train Acc: 67.60%\n",
            "2025-11-01 14:26:13,197 - supervised_finetuning - INFO -   Train F1: 67.23%, Train Recall: 67.60%\n",
            "2025-11-01 14:26:13,197 - supervised_finetuning - INFO -   Train Precision: 67.24%\n",
            "2025-11-01 14:26:13,197 - supervised_finetuning - INFO - Epoch 74 - supervised_loss/epoch: 0.7596\n",
            "2025-11-01 14:26:13,198 - supervised_finetuning - INFO - Epoch 74 - supervised_accuracy/epoch: 67.5991\n",
            "2025-11-01 14:26:13,198 - supervised_finetuning - INFO - Epoch 74 - supervised_f1/epoch: 67.2326\n",
            "2025-11-01 14:26:13,198 - supervised_finetuning - INFO - Epoch 74 - supervised_recall/epoch: 67.5991\n",
            "2025-11-01 14:26:13,200 - supervised_finetuning - INFO - Epoch 74 - supervised_precision/epoch: 67.2404\n",
            "Classifier Epoch 76/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7216, Acc=70.31%]2025-11-01 14:26:13,218 - supervised_finetuning - INFO - Epoch 1575 - supervised_loss/batch: 0.7216\n",
            "2025-11-01 14:26:13,218 - supervised_finetuning - INFO - Epoch 1575 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:26:13,218 - supervised_finetuning - INFO - Epoch 1575 - learning_rate: 0.0001\n",
            "Classifier Epoch 76/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6704, Acc=73.44%]2025-11-01 14:26:13,274 - supervised_finetuning - INFO - Epoch 1585 - supervised_loss/batch: 0.6704\n",
            "2025-11-01 14:26:13,274 - supervised_finetuning - INFO - Epoch 1585 - supervised_accuracy/batch: 73.4375\n",
            "2025-11-01 14:26:13,274 - supervised_finetuning - INFO - Epoch 1585 - learning_rate: 0.0001\n",
            "Classifier Epoch 76/100:  81% 17/21 [00:00<00:00, 168.96it/s, Loss=0.7590, Acc=71.43%]2025-11-01 14:26:13,318 - supervised_finetuning - INFO - Epoch 1595 - supervised_loss/batch: 0.7590\n",
            "2025-11-01 14:26:13,318 - supervised_finetuning - INFO - Epoch 1595 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:13,318 - supervised_finetuning - INFO - Epoch 1595 - learning_rate: 0.0001\n",
            "Classifier Epoch 76/100: 100% 21/21 [00:00<00:00, 177.88it/s, Loss=0.7590, Acc=71.43%]\n",
            "2025-11-01 14:26:13,364 - supervised_finetuning - INFO - Epoch 76:\n",
            "2025-11-01 14:26:13,366 - supervised_finetuning - INFO -   Train Loss: 0.7584, Train Acc: 68.76%\n",
            "2025-11-01 14:26:13,366 - supervised_finetuning - INFO -   Train F1: 68.23%, Train Recall: 68.76%\n",
            "2025-11-01 14:26:13,366 - supervised_finetuning - INFO -   Train Precision: 68.21%\n",
            "2025-11-01 14:26:13,366 - supervised_finetuning - INFO - Epoch 75 - supervised_loss/epoch: 0.7584\n",
            "2025-11-01 14:26:13,367 - supervised_finetuning - INFO - Epoch 75 - supervised_accuracy/epoch: 68.7646\n",
            "2025-11-01 14:26:13,367 - supervised_finetuning - INFO - Epoch 75 - supervised_f1/epoch: 68.2301\n",
            "2025-11-01 14:26:13,367 - supervised_finetuning - INFO - Epoch 75 - supervised_recall/epoch: 68.7646\n",
            "2025-11-01 14:26:13,367 - supervised_finetuning - INFO - Epoch 75 - supervised_precision/epoch: 68.2102\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:14,726 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 68.76%\n",
            "Classifier Epoch 77/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7002, Acc=70.31%]2025-11-01 14:26:14,740 - supervised_finetuning - INFO - Epoch 1596 - supervised_loss/batch: 0.7002\n",
            "2025-11-01 14:26:14,740 - supervised_finetuning - INFO - Epoch 1596 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:26:14,740 - supervised_finetuning - INFO - Epoch 1596 - learning_rate: 0.0001\n",
            "Classifier Epoch 77/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8804, Acc=65.62%]2025-11-01 14:26:14,786 - supervised_finetuning - INFO - Epoch 1606 - supervised_loss/batch: 0.8804\n",
            "2025-11-01 14:26:14,786 - supervised_finetuning - INFO - Epoch 1606 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:14,786 - supervised_finetuning - INFO - Epoch 1606 - learning_rate: 0.0001\n",
            "Classifier Epoch 77/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5300, Acc=85.71%]2025-11-01 14:26:14,835 - supervised_finetuning - INFO - Epoch 1616 - supervised_loss/batch: 0.5300\n",
            "2025-11-01 14:26:14,836 - supervised_finetuning - INFO - Epoch 1616 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:14,836 - supervised_finetuning - INFO - Epoch 1616 - learning_rate: 0.0001\n",
            "Classifier Epoch 77/100: 100% 21/21 [00:00<00:00, 201.76it/s, Loss=0.5300, Acc=85.71%]\n",
            "2025-11-01 14:26:14,851 - supervised_finetuning - INFO - Epoch 77:\n",
            "2025-11-01 14:26:14,851 - supervised_finetuning - INFO -   Train Loss: 0.7462, Train Acc: 68.69%\n",
            "2025-11-01 14:26:14,851 - supervised_finetuning - INFO -   Train F1: 68.25%, Train Recall: 68.69%\n",
            "2025-11-01 14:26:14,851 - supervised_finetuning - INFO -   Train Precision: 68.55%\n",
            "2025-11-01 14:26:14,851 - supervised_finetuning - INFO - Epoch 76 - supervised_loss/epoch: 0.7462\n",
            "2025-11-01 14:26:14,852 - supervised_finetuning - INFO - Epoch 76 - supervised_accuracy/epoch: 68.6869\n",
            "2025-11-01 14:26:14,852 - supervised_finetuning - INFO - Epoch 76 - supervised_f1/epoch: 68.2549\n",
            "2025-11-01 14:26:14,852 - supervised_finetuning - INFO - Epoch 76 - supervised_recall/epoch: 68.6869\n",
            "2025-11-01 14:26:14,852 - supervised_finetuning - INFO - Epoch 76 - supervised_precision/epoch: 68.5529\n",
            "Classifier Epoch 78/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8167, Acc=60.94%]2025-11-01 14:26:14,858 - supervised_finetuning - INFO - Epoch 1617 - supervised_loss/batch: 0.8167\n",
            "2025-11-01 14:26:14,858 - supervised_finetuning - INFO - Epoch 1617 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:26:14,859 - supervised_finetuning - INFO - Epoch 1617 - learning_rate: 0.0001\n",
            "Classifier Epoch 78/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7974, Acc=67.19%]2025-11-01 14:26:14,907 - supervised_finetuning - INFO - Epoch 1627 - supervised_loss/batch: 0.7974\n",
            "2025-11-01 14:26:14,907 - supervised_finetuning - INFO - Epoch 1627 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:26:14,907 - supervised_finetuning - INFO - Epoch 1627 - learning_rate: 0.0001\n",
            "Classifier Epoch 78/100:  95% 20/21 [00:00<00:00, 188.26it/s, Loss=0.2931, Acc=100.00%]2025-11-01 14:26:14,966 - supervised_finetuning - INFO - Epoch 1637 - supervised_loss/batch: 0.2931\n",
            "2025-11-01 14:26:14,966 - supervised_finetuning - INFO - Epoch 1637 - supervised_accuracy/batch: 100.0000\n",
            "2025-11-01 14:26:14,966 - supervised_finetuning - INFO - Epoch 1637 - learning_rate: 0.0001\n",
            "Classifier Epoch 78/100: 100% 21/21 [00:00<00:00, 184.57it/s, Loss=0.2931, Acc=100.00%]\n",
            "2025-11-01 14:26:14,987 - supervised_finetuning - INFO - Epoch 78:\n",
            "2025-11-01 14:26:14,988 - supervised_finetuning - INFO -   Train Loss: 0.7338, Train Acc: 68.84%\n",
            "2025-11-01 14:26:14,988 - supervised_finetuning - INFO -   Train F1: 68.40%, Train Recall: 68.84%\n",
            "2025-11-01 14:26:14,988 - supervised_finetuning - INFO -   Train Precision: 68.21%\n",
            "2025-11-01 14:26:14,988 - supervised_finetuning - INFO - Epoch 77 - supervised_loss/epoch: 0.7338\n",
            "2025-11-01 14:26:14,988 - supervised_finetuning - INFO - Epoch 77 - supervised_accuracy/epoch: 68.8423\n",
            "2025-11-01 14:26:14,988 - supervised_finetuning - INFO - Epoch 77 - supervised_f1/epoch: 68.3975\n",
            "2025-11-01 14:26:14,989 - supervised_finetuning - INFO - Epoch 77 - supervised_recall/epoch: 68.8423\n",
            "2025-11-01 14:26:14,989 - supervised_finetuning - INFO - Epoch 77 - supervised_precision/epoch: 68.2105\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:20,920 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 68.84%\n",
            "Classifier Epoch 79/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6843, Acc=75.00%]2025-11-01 14:26:20,933 - supervised_finetuning - INFO - Epoch 1638 - supervised_loss/batch: 0.6843\n",
            "2025-11-01 14:26:20,933 - supervised_finetuning - INFO - Epoch 1638 - supervised_accuracy/batch: 75.0000\n",
            "2025-11-01 14:26:20,933 - supervised_finetuning - INFO - Epoch 1638 - learning_rate: 0.0001\n",
            "Classifier Epoch 79/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8600, Acc=60.94%]2025-11-01 14:26:20,977 - supervised_finetuning - INFO - Epoch 1648 - supervised_loss/batch: 0.8600\n",
            "2025-11-01 14:26:20,977 - supervised_finetuning - INFO - Epoch 1648 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:26:20,977 - supervised_finetuning - INFO - Epoch 1648 - learning_rate: 0.0001\n",
            "Classifier Epoch 79/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6238, Acc=71.43%]2025-11-01 14:26:21,013 - supervised_finetuning - INFO - Epoch 1658 - supervised_loss/batch: 0.6238\n",
            "2025-11-01 14:26:21,013 - supervised_finetuning - INFO - Epoch 1658 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:21,013 - supervised_finetuning - INFO - Epoch 1658 - learning_rate: 0.0001\n",
            "Classifier Epoch 79/100: 100% 21/21 [00:00<00:00, 229.58it/s, Loss=0.6238, Acc=71.43%]\n",
            "2025-11-01 14:26:21,026 - supervised_finetuning - INFO - Epoch 79:\n",
            "2025-11-01 14:26:21,026 - supervised_finetuning - INFO -   Train Loss: 0.7563, Train Acc: 69.08%\n",
            "2025-11-01 14:26:21,026 - supervised_finetuning - INFO -   Train F1: 68.63%, Train Recall: 69.08%\n",
            "2025-11-01 14:26:21,027 - supervised_finetuning - INFO -   Train Precision: 68.52%\n",
            "2025-11-01 14:26:21,027 - supervised_finetuning - INFO - Epoch 78 - supervised_loss/epoch: 0.7563\n",
            "2025-11-01 14:26:21,027 - supervised_finetuning - INFO - Epoch 78 - supervised_accuracy/epoch: 69.0754\n",
            "2025-11-01 14:26:21,028 - supervised_finetuning - INFO - Epoch 78 - supervised_f1/epoch: 68.6330\n",
            "2025-11-01 14:26:21,028 - supervised_finetuning - INFO - Epoch 78 - supervised_recall/epoch: 69.0754\n",
            "2025-11-01 14:26:21,028 - supervised_finetuning - INFO - Epoch 78 - supervised_precision/epoch: 68.5194\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:22,301 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 69.08%\n",
            "Classifier Epoch 80/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7730, Acc=71.88%]2025-11-01 14:26:22,310 - supervised_finetuning - INFO - Epoch 1659 - supervised_loss/batch: 0.7730\n",
            "2025-11-01 14:26:22,310 - supervised_finetuning - INFO - Epoch 1659 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:26:22,311 - supervised_finetuning - INFO - Epoch 1659 - learning_rate: 0.0001\n",
            "Classifier Epoch 80/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8448, Acc=60.94%]2025-11-01 14:26:22,358 - supervised_finetuning - INFO - Epoch 1669 - supervised_loss/batch: 0.8448\n",
            "2025-11-01 14:26:22,358 - supervised_finetuning - INFO - Epoch 1669 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:26:22,358 - supervised_finetuning - INFO - Epoch 1669 - learning_rate: 0.0001\n",
            "Classifier Epoch 80/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7958, Acc=71.43%]2025-11-01 14:26:22,405 - supervised_finetuning - INFO - Epoch 1679 - supervised_loss/batch: 0.7958\n",
            "2025-11-01 14:26:22,405 - supervised_finetuning - INFO - Epoch 1679 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:22,405 - supervised_finetuning - INFO - Epoch 1679 - learning_rate: 0.0001\n",
            "Classifier Epoch 80/100: 100% 21/21 [00:00<00:00, 201.69it/s, Loss=0.7958, Acc=71.43%]\n",
            "2025-11-01 14:26:22,421 - supervised_finetuning - INFO - Epoch 80:\n",
            "2025-11-01 14:26:22,421 - supervised_finetuning - INFO -   Train Loss: 0.7534, Train Acc: 68.69%\n",
            "2025-11-01 14:26:22,421 - supervised_finetuning - INFO -   Train F1: 68.51%, Train Recall: 68.69%\n",
            "2025-11-01 14:26:22,421 - supervised_finetuning - INFO -   Train Precision: 68.76%\n",
            "2025-11-01 14:26:22,422 - supervised_finetuning - INFO - Epoch 79 - supervised_loss/epoch: 0.7534\n",
            "2025-11-01 14:26:22,422 - supervised_finetuning - INFO - Epoch 79 - supervised_accuracy/epoch: 68.6869\n",
            "2025-11-01 14:26:22,422 - supervised_finetuning - INFO - Epoch 79 - supervised_f1/epoch: 68.5127\n",
            "2025-11-01 14:26:22,422 - supervised_finetuning - INFO - Epoch 79 - supervised_recall/epoch: 68.6869\n",
            "2025-11-01 14:26:22,422 - supervised_finetuning - INFO - Epoch 79 - supervised_precision/epoch: 68.7570\n",
            "Classifier Epoch 81/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5486, Acc=75.00%]2025-11-01 14:26:22,429 - supervised_finetuning - INFO - Epoch 1680 - supervised_loss/batch: 0.5486\n",
            "2025-11-01 14:26:22,429 - supervised_finetuning - INFO - Epoch 1680 - supervised_accuracy/batch: 75.0000\n",
            "2025-11-01 14:26:22,429 - supervised_finetuning - INFO - Epoch 1680 - learning_rate: 0.0001\n",
            "Classifier Epoch 81/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7352, Acc=68.75%]2025-11-01 14:26:22,522 - supervised_finetuning - INFO - Epoch 1690 - supervised_loss/batch: 0.7352\n",
            "2025-11-01 14:26:22,523 - supervised_finetuning - INFO - Epoch 1690 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:22,523 - supervised_finetuning - INFO - Epoch 1690 - learning_rate: 0.0001\n",
            "Classifier Epoch 81/100:  52% 11/21 [00:00<00:00, 109.76it/s, Loss=0.3489, Acc=85.71%]2025-11-01 14:26:22,565 - supervised_finetuning - INFO - Epoch 1700 - supervised_loss/batch: 0.3489\n",
            "2025-11-01 14:26:22,566 - supervised_finetuning - INFO - Epoch 1700 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:22,566 - supervised_finetuning - INFO - Epoch 1700 - learning_rate: 0.0001\n",
            "Classifier Epoch 81/100: 100% 21/21 [00:00<00:00, 146.15it/s, Loss=0.3489, Acc=85.71%]\n",
            "2025-11-01 14:26:22,583 - supervised_finetuning - INFO - Epoch 81:\n",
            "2025-11-01 14:26:22,583 - supervised_finetuning - INFO -   Train Loss: 0.7750, Train Acc: 65.66%\n",
            "2025-11-01 14:26:22,583 - supervised_finetuning - INFO -   Train F1: 65.22%, Train Recall: 65.66%\n",
            "2025-11-01 14:26:22,583 - supervised_finetuning - INFO -   Train Precision: 65.42%\n",
            "2025-11-01 14:26:22,583 - supervised_finetuning - INFO - Epoch 80 - supervised_loss/epoch: 0.7750\n",
            "2025-11-01 14:26:22,584 - supervised_finetuning - INFO - Epoch 80 - supervised_accuracy/epoch: 65.6566\n",
            "2025-11-01 14:26:22,584 - supervised_finetuning - INFO - Epoch 80 - supervised_f1/epoch: 65.2223\n",
            "2025-11-01 14:26:22,584 - supervised_finetuning - INFO - Epoch 80 - supervised_recall/epoch: 65.6566\n",
            "2025-11-01 14:26:22,584 - supervised_finetuning - INFO - Epoch 80 - supervised_precision/epoch: 65.4188\n",
            "Classifier Epoch 82/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8684, Acc=62.50%]2025-11-01 14:26:22,590 - supervised_finetuning - INFO - Epoch 1701 - supervised_loss/batch: 0.8684\n",
            "2025-11-01 14:26:22,590 - supervised_finetuning - INFO - Epoch 1701 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:26:22,590 - supervised_finetuning - INFO - Epoch 1701 - learning_rate: 0.0001\n",
            "Classifier Epoch 82/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7474, Acc=67.19%]2025-11-01 14:26:22,642 - supervised_finetuning - INFO - Epoch 1711 - supervised_loss/batch: 0.7474\n",
            "2025-11-01 14:26:22,642 - supervised_finetuning - INFO - Epoch 1711 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:26:22,642 - supervised_finetuning - INFO - Epoch 1711 - learning_rate: 0.0001\n",
            "Classifier Epoch 82/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7928, Acc=42.86%]2025-11-01 14:26:22,687 - supervised_finetuning - INFO - Epoch 1721 - supervised_loss/batch: 0.7928\n",
            "2025-11-01 14:26:22,687 - supervised_finetuning - INFO - Epoch 1721 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:26:22,687 - supervised_finetuning - INFO - Epoch 1721 - learning_rate: 0.0001\n",
            "Classifier Epoch 82/100: 100% 21/21 [00:00<00:00, 203.26it/s, Loss=0.7928, Acc=42.86%]\n",
            "2025-11-01 14:26:22,703 - supervised_finetuning - INFO - Epoch 82:\n",
            "2025-11-01 14:26:22,703 - supervised_finetuning - INFO -   Train Loss: 0.7840, Train Acc: 67.68%\n",
            "2025-11-01 14:26:22,703 - supervised_finetuning - INFO -   Train F1: 67.21%, Train Recall: 67.68%\n",
            "2025-11-01 14:26:22,703 - supervised_finetuning - INFO -   Train Precision: 67.79%\n",
            "2025-11-01 14:26:22,703 - supervised_finetuning - INFO - Epoch 81 - supervised_loss/epoch: 0.7840\n",
            "2025-11-01 14:26:22,704 - supervised_finetuning - INFO - Epoch 81 - supervised_accuracy/epoch: 67.6768\n",
            "2025-11-01 14:26:22,704 - supervised_finetuning - INFO - Epoch 81 - supervised_f1/epoch: 67.2116\n",
            "2025-11-01 14:26:22,704 - supervised_finetuning - INFO - Epoch 81 - supervised_recall/epoch: 67.6768\n",
            "2025-11-01 14:26:22,704 - supervised_finetuning - INFO - Epoch 81 - supervised_precision/epoch: 67.7878\n",
            "Classifier Epoch 83/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7453, Acc=67.19%]2025-11-01 14:26:22,710 - supervised_finetuning - INFO - Epoch 1722 - supervised_loss/batch: 0.7453\n",
            "2025-11-01 14:26:22,710 - supervised_finetuning - INFO - Epoch 1722 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:26:22,710 - supervised_finetuning - INFO - Epoch 1722 - learning_rate: 0.0001\n",
            "Classifier Epoch 83/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8590, Acc=67.19%]2025-11-01 14:26:22,762 - supervised_finetuning - INFO - Epoch 1732 - supervised_loss/batch: 0.8590\n",
            "2025-11-01 14:26:22,762 - supervised_finetuning - INFO - Epoch 1732 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:26:22,763 - supervised_finetuning - INFO - Epoch 1732 - learning_rate: 0.0001\n",
            "Classifier Epoch 83/100:  90% 19/21 [00:00<00:00, 184.90it/s, Loss=0.7783, Acc=71.43%]2025-11-01 14:26:22,819 - supervised_finetuning - INFO - Epoch 1742 - supervised_loss/batch: 0.7783\n",
            "2025-11-01 14:26:22,819 - supervised_finetuning - INFO - Epoch 1742 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:22,820 - supervised_finetuning - INFO - Epoch 1742 - learning_rate: 0.0001\n",
            "Classifier Epoch 83/100: 100% 21/21 [00:00<00:00, 182.11it/s, Loss=0.7783, Acc=71.43%]\n",
            "2025-11-01 14:26:22,837 - supervised_finetuning - INFO - Epoch 83:\n",
            "2025-11-01 14:26:22,838 - supervised_finetuning - INFO -   Train Loss: 0.7636, Train Acc: 67.99%\n",
            "2025-11-01 14:26:22,838 - supervised_finetuning - INFO -   Train F1: 67.54%, Train Recall: 67.99%\n",
            "2025-11-01 14:26:22,838 - supervised_finetuning - INFO -   Train Precision: 67.40%\n",
            "2025-11-01 14:26:22,838 - supervised_finetuning - INFO - Epoch 82 - supervised_loss/epoch: 0.7636\n",
            "2025-11-01 14:26:22,838 - supervised_finetuning - INFO - Epoch 82 - supervised_accuracy/epoch: 67.9876\n",
            "2025-11-01 14:26:22,839 - supervised_finetuning - INFO - Epoch 82 - supervised_f1/epoch: 67.5387\n",
            "2025-11-01 14:26:22,839 - supervised_finetuning - INFO - Epoch 82 - supervised_recall/epoch: 67.9876\n",
            "2025-11-01 14:26:22,840 - supervised_finetuning - INFO - Epoch 82 - supervised_precision/epoch: 67.3969\n",
            "Classifier Epoch 84/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9135, Acc=62.50%]2025-11-01 14:26:22,846 - supervised_finetuning - INFO - Epoch 1743 - supervised_loss/batch: 0.9135\n",
            "2025-11-01 14:26:22,846 - supervised_finetuning - INFO - Epoch 1743 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:26:22,846 - supervised_finetuning - INFO - Epoch 1743 - learning_rate: 0.0001\n",
            "Classifier Epoch 84/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8842, Acc=65.62%]2025-11-01 14:26:22,895 - supervised_finetuning - INFO - Epoch 1753 - supervised_loss/batch: 0.8842\n",
            "2025-11-01 14:26:22,896 - supervised_finetuning - INFO - Epoch 1753 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:22,896 - supervised_finetuning - INFO - Epoch 1753 - learning_rate: 0.0001\n",
            "Classifier Epoch 84/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6259, Acc=71.43%]2025-11-01 14:26:22,944 - supervised_finetuning - INFO - Epoch 1763 - supervised_loss/batch: 0.6259\n",
            "2025-11-01 14:26:22,944 - supervised_finetuning - INFO - Epoch 1763 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:22,944 - supervised_finetuning - INFO - Epoch 1763 - learning_rate: 0.0001\n",
            "Classifier Epoch 84/100: 100% 21/21 [00:00<00:00, 201.02it/s, Loss=0.6259, Acc=71.43%]\n",
            "2025-11-01 14:26:22,962 - supervised_finetuning - INFO - Epoch 84:\n",
            "2025-11-01 14:26:22,962 - supervised_finetuning - INFO -   Train Loss: 0.7494, Train Acc: 68.61%\n",
            "2025-11-01 14:26:22,962 - supervised_finetuning - INFO -   Train F1: 68.19%, Train Recall: 68.61%\n",
            "2025-11-01 14:26:22,962 - supervised_finetuning - INFO -   Train Precision: 68.18%\n",
            "2025-11-01 14:26:22,962 - supervised_finetuning - INFO - Epoch 83 - supervised_loss/epoch: 0.7494\n",
            "2025-11-01 14:26:22,962 - supervised_finetuning - INFO - Epoch 83 - supervised_accuracy/epoch: 68.6092\n",
            "2025-11-01 14:26:22,963 - supervised_finetuning - INFO - Epoch 83 - supervised_f1/epoch: 68.1949\n",
            "2025-11-01 14:26:22,963 - supervised_finetuning - INFO - Epoch 83 - supervised_recall/epoch: 68.6092\n",
            "2025-11-01 14:26:22,963 - supervised_finetuning - INFO - Epoch 83 - supervised_precision/epoch: 68.1848\n",
            "Classifier Epoch 85/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9035, Acc=60.94%]2025-11-01 14:26:22,969 - supervised_finetuning - INFO - Epoch 1764 - supervised_loss/batch: 0.9035\n",
            "2025-11-01 14:26:22,969 - supervised_finetuning - INFO - Epoch 1764 - supervised_accuracy/batch: 60.9375\n",
            "2025-11-01 14:26:22,969 - supervised_finetuning - INFO - Epoch 1764 - learning_rate: 0.0001\n",
            "Classifier Epoch 85/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7254, Acc=70.31%]2025-11-01 14:26:23,016 - supervised_finetuning - INFO - Epoch 1774 - supervised_loss/batch: 0.7254\n",
            "2025-11-01 14:26:23,016 - supervised_finetuning - INFO - Epoch 1774 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:26:23,016 - supervised_finetuning - INFO - Epoch 1774 - learning_rate: 0.0001\n",
            "Classifier Epoch 85/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6659, Acc=71.43%]2025-11-01 14:26:23,065 - supervised_finetuning - INFO - Epoch 1784 - supervised_loss/batch: 0.6659\n",
            "2025-11-01 14:26:23,066 - supervised_finetuning - INFO - Epoch 1784 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:23,067 - supervised_finetuning - INFO - Epoch 1784 - learning_rate: 0.0001\n",
            "Classifier Epoch 85/100: 100% 21/21 [00:00<00:00, 202.14it/s, Loss=0.6659, Acc=71.43%]\n",
            "2025-11-01 14:26:23,091 - supervised_finetuning - INFO - Epoch 85:\n",
            "2025-11-01 14:26:23,091 - supervised_finetuning - INFO -   Train Loss: 0.7387, Train Acc: 69.15%\n",
            "2025-11-01 14:26:23,091 - supervised_finetuning - INFO -   Train F1: 68.88%, Train Recall: 69.15%\n",
            "2025-11-01 14:26:23,091 - supervised_finetuning - INFO -   Train Precision: 68.70%\n",
            "2025-11-01 14:26:23,091 - supervised_finetuning - INFO - Epoch 84 - supervised_loss/epoch: 0.7387\n",
            "2025-11-01 14:26:23,092 - supervised_finetuning - INFO - Epoch 84 - supervised_accuracy/epoch: 69.1531\n",
            "2025-11-01 14:26:23,092 - supervised_finetuning - INFO - Epoch 84 - supervised_f1/epoch: 68.8770\n",
            "2025-11-01 14:26:23,092 - supervised_finetuning - INFO - Epoch 84 - supervised_recall/epoch: 69.1531\n",
            "2025-11-01 14:26:23,092 - supervised_finetuning - INFO - Epoch 84 - supervised_precision/epoch: 68.6966\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:27,440 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 69.15%\n",
            "Classifier Epoch 86/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7625, Acc=70.31%]2025-11-01 14:26:27,445 - supervised_finetuning - INFO - Epoch 1785 - supervised_loss/batch: 0.7625\n",
            "2025-11-01 14:26:27,445 - supervised_finetuning - INFO - Epoch 1785 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:26:27,446 - supervised_finetuning - INFO - Epoch 1785 - learning_rate: 0.0001\n",
            "Classifier Epoch 86/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8426, Acc=57.81%]2025-11-01 14:26:27,481 - supervised_finetuning - INFO - Epoch 1795 - supervised_loss/batch: 0.8426\n",
            "2025-11-01 14:26:27,481 - supervised_finetuning - INFO - Epoch 1795 - supervised_accuracy/batch: 57.8125\n",
            "2025-11-01 14:26:27,482 - supervised_finetuning - INFO - Epoch 1795 - learning_rate: 0.0001\n",
            "Classifier Epoch 86/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.3384, Acc=85.71%]2025-11-01 14:26:27,516 - supervised_finetuning - INFO - Epoch 1805 - supervised_loss/batch: 0.3384\n",
            "2025-11-01 14:26:27,517 - supervised_finetuning - INFO - Epoch 1805 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:27,517 - supervised_finetuning - INFO - Epoch 1805 - learning_rate: 0.0001\n",
            "Classifier Epoch 86/100: 100% 21/21 [00:00<00:00, 273.42it/s, Loss=0.3384, Acc=85.71%]\n",
            "2025-11-01 14:26:27,527 - supervised_finetuning - INFO - Epoch 86:\n",
            "2025-11-01 14:26:27,527 - supervised_finetuning - INFO -   Train Loss: 0.7464, Train Acc: 68.07%\n",
            "2025-11-01 14:26:27,527 - supervised_finetuning - INFO -   Train F1: 67.67%, Train Recall: 68.07%\n",
            "2025-11-01 14:26:27,527 - supervised_finetuning - INFO -   Train Precision: 67.75%\n",
            "2025-11-01 14:26:27,528 - supervised_finetuning - INFO - Epoch 85 - supervised_loss/epoch: 0.7464\n",
            "2025-11-01 14:26:27,528 - supervised_finetuning - INFO - Epoch 85 - supervised_accuracy/epoch: 68.0653\n",
            "2025-11-01 14:26:27,528 - supervised_finetuning - INFO - Epoch 85 - supervised_f1/epoch: 67.6729\n",
            "2025-11-01 14:26:27,528 - supervised_finetuning - INFO - Epoch 85 - supervised_recall/epoch: 68.0653\n",
            "2025-11-01 14:26:27,529 - supervised_finetuning - INFO - Epoch 85 - supervised_precision/epoch: 67.7534\n",
            "Classifier Epoch 87/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6893, Acc=73.44%]2025-11-01 14:26:27,533 - supervised_finetuning - INFO - Epoch 1806 - supervised_loss/batch: 0.6893\n",
            "2025-11-01 14:26:27,533 - supervised_finetuning - INFO - Epoch 1806 - supervised_accuracy/batch: 73.4375\n",
            "2025-11-01 14:26:27,533 - supervised_finetuning - INFO - Epoch 1806 - learning_rate: 0.0001\n",
            "Classifier Epoch 87/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6776, Acc=68.75%]2025-11-01 14:26:27,569 - supervised_finetuning - INFO - Epoch 1816 - supervised_loss/batch: 0.6776\n",
            "2025-11-01 14:26:27,569 - supervised_finetuning - INFO - Epoch 1816 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:27,570 - supervised_finetuning - INFO - Epoch 1816 - learning_rate: 0.0001\n",
            "Classifier Epoch 87/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.5104, Acc=85.71%]2025-11-01 14:26:27,605 - supervised_finetuning - INFO - Epoch 1826 - supervised_loss/batch: 0.5104\n",
            "2025-11-01 14:26:27,605 - supervised_finetuning - INFO - Epoch 1826 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:27,605 - supervised_finetuning - INFO - Epoch 1826 - learning_rate: 0.0001\n",
            "Classifier Epoch 87/100: 100% 21/21 [00:00<00:00, 273.82it/s, Loss=0.5104, Acc=85.71%]\n",
            "2025-11-01 14:26:27,615 - supervised_finetuning - INFO - Epoch 87:\n",
            "2025-11-01 14:26:27,616 - supervised_finetuning - INFO -   Train Loss: 0.7323, Train Acc: 68.61%\n",
            "2025-11-01 14:26:27,616 - supervised_finetuning - INFO -   Train F1: 68.23%, Train Recall: 68.61%\n",
            "2025-11-01 14:26:27,616 - supervised_finetuning - INFO -   Train Precision: 68.18%\n",
            "2025-11-01 14:26:27,616 - supervised_finetuning - INFO - Epoch 86 - supervised_loss/epoch: 0.7323\n",
            "2025-11-01 14:26:27,616 - supervised_finetuning - INFO - Epoch 86 - supervised_accuracy/epoch: 68.6092\n",
            "2025-11-01 14:26:27,616 - supervised_finetuning - INFO - Epoch 86 - supervised_f1/epoch: 68.2324\n",
            "2025-11-01 14:26:27,616 - supervised_finetuning - INFO - Epoch 86 - supervised_recall/epoch: 68.6092\n",
            "2025-11-01 14:26:27,617 - supervised_finetuning - INFO - Epoch 86 - supervised_precision/epoch: 68.1774\n",
            "Classifier Epoch 88/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7464, Acc=65.62%]2025-11-01 14:26:27,621 - supervised_finetuning - INFO - Epoch 1827 - supervised_loss/batch: 0.7464\n",
            "2025-11-01 14:26:27,621 - supervised_finetuning - INFO - Epoch 1827 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:27,622 - supervised_finetuning - INFO - Epoch 1827 - learning_rate: 0.0001\n",
            "Classifier Epoch 88/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6144, Acc=76.56%]2025-11-01 14:26:27,658 - supervised_finetuning - INFO - Epoch 1837 - supervised_loss/batch: 0.6144\n",
            "2025-11-01 14:26:27,658 - supervised_finetuning - INFO - Epoch 1837 - supervised_accuracy/batch: 76.5625\n",
            "2025-11-01 14:26:27,659 - supervised_finetuning - INFO - Epoch 1837 - learning_rate: 0.0001\n",
            "Classifier Epoch 88/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9435, Acc=57.14%]2025-11-01 14:26:27,716 - supervised_finetuning - INFO - Epoch 1847 - supervised_loss/batch: 0.9435\n",
            "2025-11-01 14:26:27,716 - supervised_finetuning - INFO - Epoch 1847 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:26:27,716 - supervised_finetuning - INFO - Epoch 1847 - learning_rate: 0.0001\n",
            "Classifier Epoch 88/100: 100% 21/21 [00:00<00:00, 211.10it/s, Loss=0.9435, Acc=57.14%]\n",
            "2025-11-01 14:26:27,739 - supervised_finetuning - INFO - Epoch 88:\n",
            "2025-11-01 14:26:27,739 - supervised_finetuning - INFO -   Train Loss: 0.7490, Train Acc: 69.00%\n",
            "2025-11-01 14:26:27,739 - supervised_finetuning - INFO -   Train F1: 68.63%, Train Recall: 69.00%\n",
            "2025-11-01 14:26:27,739 - supervised_finetuning - INFO -   Train Precision: 69.20%\n",
            "2025-11-01 14:26:27,739 - supervised_finetuning - INFO - Epoch 87 - supervised_loss/epoch: 0.7490\n",
            "2025-11-01 14:26:27,740 - supervised_finetuning - INFO - Epoch 87 - supervised_accuracy/epoch: 68.9977\n",
            "2025-11-01 14:26:27,740 - supervised_finetuning - INFO - Epoch 87 - supervised_f1/epoch: 68.6335\n",
            "2025-11-01 14:26:27,740 - supervised_finetuning - INFO - Epoch 87 - supervised_recall/epoch: 68.9977\n",
            "2025-11-01 14:26:27,742 - supervised_finetuning - INFO - Epoch 87 - supervised_precision/epoch: 69.2019\n",
            "Classifier Epoch 89/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7344, Acc=64.06%]2025-11-01 14:26:27,751 - supervised_finetuning - INFO - Epoch 1848 - supervised_loss/batch: 0.7344\n",
            "2025-11-01 14:26:27,751 - supervised_finetuning - INFO - Epoch 1848 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:27,751 - supervised_finetuning - INFO - Epoch 1848 - learning_rate: 0.0001\n",
            "Classifier Epoch 89/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7566, Acc=65.62%]2025-11-01 14:26:27,790 - supervised_finetuning - INFO - Epoch 1858 - supervised_loss/batch: 0.7566\n",
            "2025-11-01 14:26:27,790 - supervised_finetuning - INFO - Epoch 1858 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:27,790 - supervised_finetuning - INFO - Epoch 1858 - learning_rate: 0.0001\n",
            "Classifier Epoch 89/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6982, Acc=57.14%]2025-11-01 14:26:27,827 - supervised_finetuning - INFO - Epoch 1868 - supervised_loss/batch: 0.6982\n",
            "2025-11-01 14:26:27,828 - supervised_finetuning - INFO - Epoch 1868 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:26:27,828 - supervised_finetuning - INFO - Epoch 1868 - learning_rate: 0.0001\n",
            "Classifier Epoch 89/100: 100% 21/21 [00:00<00:00, 251.07it/s, Loss=0.6982, Acc=57.14%]\n",
            "2025-11-01 14:26:27,838 - supervised_finetuning - INFO - Epoch 89:\n",
            "2025-11-01 14:26:27,838 - supervised_finetuning - INFO -   Train Loss: 0.7304, Train Acc: 69.00%\n",
            "2025-11-01 14:26:27,838 - supervised_finetuning - INFO -   Train F1: 68.47%, Train Recall: 69.00%\n",
            "2025-11-01 14:26:27,839 - supervised_finetuning - INFO -   Train Precision: 68.25%\n",
            "2025-11-01 14:26:27,839 - supervised_finetuning - INFO - Epoch 88 - supervised_loss/epoch: 0.7304\n",
            "2025-11-01 14:26:27,839 - supervised_finetuning - INFO - Epoch 88 - supervised_accuracy/epoch: 68.9977\n",
            "2025-11-01 14:26:27,839 - supervised_finetuning - INFO - Epoch 88 - supervised_f1/epoch: 68.4660\n",
            "2025-11-01 14:26:27,839 - supervised_finetuning - INFO - Epoch 88 - supervised_recall/epoch: 68.9977\n",
            "2025-11-01 14:26:27,840 - supervised_finetuning - INFO - Epoch 88 - supervised_precision/epoch: 68.2550\n",
            "Classifier Epoch 90/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7448, Acc=71.88%]2025-11-01 14:26:27,844 - supervised_finetuning - INFO - Epoch 1869 - supervised_loss/batch: 0.7448\n",
            "2025-11-01 14:26:27,844 - supervised_finetuning - INFO - Epoch 1869 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:26:27,845 - supervised_finetuning - INFO - Epoch 1869 - learning_rate: 0.0001\n",
            "Classifier Epoch 90/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7200, Acc=64.06%]2025-11-01 14:26:27,881 - supervised_finetuning - INFO - Epoch 1879 - supervised_loss/batch: 0.7200\n",
            "2025-11-01 14:26:27,882 - supervised_finetuning - INFO - Epoch 1879 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:27,882 - supervised_finetuning - INFO - Epoch 1879 - learning_rate: 0.0001\n",
            "Classifier Epoch 90/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6530, Acc=85.71%]2025-11-01 14:26:27,919 - supervised_finetuning - INFO - Epoch 1889 - supervised_loss/batch: 0.6530\n",
            "2025-11-01 14:26:27,919 - supervised_finetuning - INFO - Epoch 1889 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:27,919 - supervised_finetuning - INFO - Epoch 1889 - learning_rate: 0.0001\n",
            "Classifier Epoch 90/100: 100% 21/21 [00:00<00:00, 263.77it/s, Loss=0.6530, Acc=85.71%]\n",
            "2025-11-01 14:26:27,930 - supervised_finetuning - INFO - Epoch 90:\n",
            "2025-11-01 14:26:27,930 - supervised_finetuning - INFO -   Train Loss: 0.7370, Train Acc: 68.53%\n",
            "2025-11-01 14:26:27,930 - supervised_finetuning - INFO -   Train F1: 68.15%, Train Recall: 68.53%\n",
            "2025-11-01 14:26:27,930 - supervised_finetuning - INFO -   Train Precision: 68.53%\n",
            "2025-11-01 14:26:27,930 - supervised_finetuning - INFO - Epoch 89 - supervised_loss/epoch: 0.7370\n",
            "2025-11-01 14:26:27,931 - supervised_finetuning - INFO - Epoch 89 - supervised_accuracy/epoch: 68.5315\n",
            "2025-11-01 14:26:27,931 - supervised_finetuning - INFO - Epoch 89 - supervised_f1/epoch: 68.1472\n",
            "2025-11-01 14:26:27,931 - supervised_finetuning - INFO - Epoch 89 - supervised_recall/epoch: 68.5315\n",
            "2025-11-01 14:26:27,931 - supervised_finetuning - INFO - Epoch 89 - supervised_precision/epoch: 68.5309\n",
            "Classifier Epoch 91/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7509, Acc=67.19%]2025-11-01 14:26:27,936 - supervised_finetuning - INFO - Epoch 1890 - supervised_loss/batch: 0.7509\n",
            "2025-11-01 14:26:27,936 - supervised_finetuning - INFO - Epoch 1890 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:26:27,937 - supervised_finetuning - INFO - Epoch 1890 - learning_rate: 0.0001\n",
            "Classifier Epoch 91/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6679, Acc=68.75%]2025-11-01 14:26:27,976 - supervised_finetuning - INFO - Epoch 1900 - supervised_loss/batch: 0.6679\n",
            "2025-11-01 14:26:27,976 - supervised_finetuning - INFO - Epoch 1900 - supervised_accuracy/batch: 68.7500\n",
            "2025-11-01 14:26:27,976 - supervised_finetuning - INFO - Epoch 1900 - learning_rate: 0.0001\n",
            "Classifier Epoch 91/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.3142, Acc=100.00%]2025-11-01 14:26:28,023 - supervised_finetuning - INFO - Epoch 1910 - supervised_loss/batch: 0.3142\n",
            "2025-11-01 14:26:28,023 - supervised_finetuning - INFO - Epoch 1910 - supervised_accuracy/batch: 100.0000\n",
            "2025-11-01 14:26:28,024 - supervised_finetuning - INFO - Epoch 1910 - learning_rate: 0.0001\n",
            "Classifier Epoch 91/100: 100% 21/21 [00:00<00:00, 227.70it/s, Loss=0.3142, Acc=100.00%]\n",
            "2025-11-01 14:26:28,036 - supervised_finetuning - INFO - Epoch 91:\n",
            "2025-11-01 14:26:28,036 - supervised_finetuning - INFO -   Train Loss: 0.7147, Train Acc: 70.16%\n",
            "2025-11-01 14:26:28,036 - supervised_finetuning - INFO -   Train F1: 69.89%, Train Recall: 70.16%\n",
            "2025-11-01 14:26:28,036 - supervised_finetuning - INFO -   Train Precision: 69.91%\n",
            "2025-11-01 14:26:28,037 - supervised_finetuning - INFO - Epoch 90 - supervised_loss/epoch: 0.7147\n",
            "2025-11-01 14:26:28,037 - supervised_finetuning - INFO - Epoch 90 - supervised_accuracy/epoch: 70.1632\n",
            "2025-11-01 14:26:28,037 - supervised_finetuning - INFO - Epoch 90 - supervised_f1/epoch: 69.8861\n",
            "2025-11-01 14:26:28,037 - supervised_finetuning - INFO - Epoch 90 - supervised_recall/epoch: 70.1632\n",
            "2025-11-01 14:26:28,038 - supervised_finetuning - INFO - Epoch 90 - supervised_precision/epoch: 69.9083\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:29,183 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 70.16%\n",
            "Classifier Epoch 92/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.9049, Acc=56.25%]2025-11-01 14:26:29,199 - supervised_finetuning - INFO - Epoch 1911 - supervised_loss/batch: 0.9049\n",
            "2025-11-01 14:26:29,200 - supervised_finetuning - INFO - Epoch 1911 - supervised_accuracy/batch: 56.2500\n",
            "2025-11-01 14:26:29,200 - supervised_finetuning - INFO - Epoch 1911 - learning_rate: 0.0001\n",
            "Classifier Epoch 92/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8233, Acc=65.62%]2025-11-01 14:26:29,285 - supervised_finetuning - INFO - Epoch 1921 - supervised_loss/batch: 0.8233\n",
            "2025-11-01 14:26:29,286 - supervised_finetuning - INFO - Epoch 1921 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:29,286 - supervised_finetuning - INFO - Epoch 1921 - learning_rate: 0.0001\n",
            "Classifier Epoch 92/100:  52% 11/21 [00:00<00:00, 108.11it/s, Loss=0.9791, Acc=42.86%]2025-11-01 14:26:29,346 - supervised_finetuning - INFO - Epoch 1931 - supervised_loss/batch: 0.9791\n",
            "2025-11-01 14:26:29,346 - supervised_finetuning - INFO - Epoch 1931 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:26:29,346 - supervised_finetuning - INFO - Epoch 1931 - learning_rate: 0.0001\n",
            "Classifier Epoch 92/100: 100% 21/21 [00:00<00:00, 129.67it/s, Loss=0.9791, Acc=42.86%]\n",
            "2025-11-01 14:26:29,368 - supervised_finetuning - INFO - Epoch 92:\n",
            "2025-11-01 14:26:29,369 - supervised_finetuning - INFO -   Train Loss: 0.7404, Train Acc: 69.77%\n",
            "2025-11-01 14:26:29,369 - supervised_finetuning - INFO -   Train F1: 69.39%, Train Recall: 69.77%\n",
            "2025-11-01 14:26:29,369 - supervised_finetuning - INFO -   Train Precision: 69.29%\n",
            "2025-11-01 14:26:29,369 - supervised_finetuning - INFO - Epoch 91 - supervised_loss/epoch: 0.7404\n",
            "2025-11-01 14:26:29,369 - supervised_finetuning - INFO - Epoch 91 - supervised_accuracy/epoch: 69.7747\n",
            "2025-11-01 14:26:29,370 - supervised_finetuning - INFO - Epoch 91 - supervised_f1/epoch: 69.3862\n",
            "2025-11-01 14:26:29,370 - supervised_finetuning - INFO - Epoch 91 - supervised_recall/epoch: 69.7747\n",
            "2025-11-01 14:26:29,370 - supervised_finetuning - INFO - Epoch 91 - supervised_precision/epoch: 69.2910\n",
            "Classifier Epoch 93/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7724, Acc=64.06%]2025-11-01 14:26:29,377 - supervised_finetuning - INFO - Epoch 1932 - supervised_loss/batch: 0.7724\n",
            "2025-11-01 14:26:29,378 - supervised_finetuning - INFO - Epoch 1932 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:29,379 - supervised_finetuning - INFO - Epoch 1932 - learning_rate: 0.0001\n",
            "Classifier Epoch 93/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8991, Acc=64.06%]2025-11-01 14:26:29,458 - supervised_finetuning - INFO - Epoch 1942 - supervised_loss/batch: 0.8991\n",
            "2025-11-01 14:26:29,459 - supervised_finetuning - INFO - Epoch 1942 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:29,459 - supervised_finetuning - INFO - Epoch 1942 - learning_rate: 0.0001\n",
            "Classifier Epoch 93/100:  62% 13/21 [00:00<00:00, 128.45it/s, Loss=1.1931, Acc=42.86%]2025-11-01 14:26:29,526 - supervised_finetuning - INFO - Epoch 1952 - supervised_loss/batch: 1.1931\n",
            "2025-11-01 14:26:29,526 - supervised_finetuning - INFO - Epoch 1952 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:26:29,526 - supervised_finetuning - INFO - Epoch 1952 - learning_rate: 0.0001\n",
            "Classifier Epoch 93/100: 100% 21/21 [00:00<00:00, 134.76it/s, Loss=1.1931, Acc=42.86%]\n",
            "2025-11-01 14:26:29,545 - supervised_finetuning - INFO - Epoch 93:\n",
            "2025-11-01 14:26:29,546 - supervised_finetuning - INFO -   Train Loss: 0.7521, Train Acc: 69.23%\n",
            "2025-11-01 14:26:29,546 - supervised_finetuning - INFO -   Train F1: 69.05%, Train Recall: 69.23%\n",
            "2025-11-01 14:26:29,546 - supervised_finetuning - INFO -   Train Precision: 69.16%\n",
            "2025-11-01 14:26:29,546 - supervised_finetuning - INFO - Epoch 92 - supervised_loss/epoch: 0.7521\n",
            "2025-11-01 14:26:29,546 - supervised_finetuning - INFO - Epoch 92 - supervised_accuracy/epoch: 69.2308\n",
            "2025-11-01 14:26:29,546 - supervised_finetuning - INFO - Epoch 92 - supervised_f1/epoch: 69.0520\n",
            "2025-11-01 14:26:29,547 - supervised_finetuning - INFO - Epoch 92 - supervised_recall/epoch: 69.2308\n",
            "2025-11-01 14:26:29,547 - supervised_finetuning - INFO - Epoch 92 - supervised_precision/epoch: 69.1623\n",
            "Classifier Epoch 94/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6766, Acc=71.88%]2025-11-01 14:26:29,556 - supervised_finetuning - INFO - Epoch 1953 - supervised_loss/batch: 0.6766\n",
            "2025-11-01 14:26:29,556 - supervised_finetuning - INFO - Epoch 1953 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:26:29,556 - supervised_finetuning - INFO - Epoch 1953 - learning_rate: 0.0001\n",
            "Classifier Epoch 94/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7707, Acc=73.44%]2025-11-01 14:26:29,630 - supervised_finetuning - INFO - Epoch 1963 - supervised_loss/batch: 0.7707\n",
            "2025-11-01 14:26:29,631 - supervised_finetuning - INFO - Epoch 1963 - supervised_accuracy/batch: 73.4375\n",
            "2025-11-01 14:26:29,631 - supervised_finetuning - INFO - Epoch 1963 - learning_rate: 0.0001\n",
            "Classifier Epoch 94/100:  71% 15/21 [00:00<00:00, 142.84it/s, Loss=1.1181, Acc=42.86%]2025-11-01 14:26:29,701 - supervised_finetuning - INFO - Epoch 1973 - supervised_loss/batch: 1.1181\n",
            "2025-11-01 14:26:29,701 - supervised_finetuning - INFO - Epoch 1973 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:26:29,701 - supervised_finetuning - INFO - Epoch 1973 - learning_rate: 0.0001\n",
            "Classifier Epoch 94/100: 100% 21/21 [00:00<00:00, 136.19it/s, Loss=1.1181, Acc=42.86%]\n",
            "2025-11-01 14:26:29,724 - supervised_finetuning - INFO - Epoch 94:\n",
            "2025-11-01 14:26:29,724 - supervised_finetuning - INFO -   Train Loss: 0.7448, Train Acc: 70.16%\n",
            "2025-11-01 14:26:29,725 - supervised_finetuning - INFO -   Train F1: 69.62%, Train Recall: 70.16%\n",
            "2025-11-01 14:26:29,725 - supervised_finetuning - INFO -   Train Precision: 70.17%\n",
            "2025-11-01 14:26:29,725 - supervised_finetuning - INFO - Epoch 93 - supervised_loss/epoch: 0.7448\n",
            "2025-11-01 14:26:29,725 - supervised_finetuning - INFO - Epoch 93 - supervised_accuracy/epoch: 70.1632\n",
            "2025-11-01 14:26:29,725 - supervised_finetuning - INFO - Epoch 93 - supervised_f1/epoch: 69.6201\n",
            "2025-11-01 14:26:29,725 - supervised_finetuning - INFO - Epoch 93 - supervised_recall/epoch: 70.1632\n",
            "2025-11-01 14:26:29,726 - supervised_finetuning - INFO - Epoch 93 - supervised_precision/epoch: 70.1695\n",
            "Classifier Epoch 95/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7035, Acc=67.19%]2025-11-01 14:26:29,734 - supervised_finetuning - INFO - Epoch 1974 - supervised_loss/batch: 0.7035\n",
            "2025-11-01 14:26:29,734 - supervised_finetuning - INFO - Epoch 1974 - supervised_accuracy/batch: 67.1875\n",
            "2025-11-01 14:26:29,735 - supervised_finetuning - INFO - Epoch 1974 - learning_rate: 0.0001\n",
            "Classifier Epoch 95/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6390, Acc=71.88%]2025-11-01 14:26:29,800 - supervised_finetuning - INFO - Epoch 1984 - supervised_loss/batch: 0.6390\n",
            "2025-11-01 14:26:29,800 - supervised_finetuning - INFO - Epoch 1984 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:26:29,800 - supervised_finetuning - INFO - Epoch 1984 - learning_rate: 0.0001\n",
            "Classifier Epoch 95/100:  67% 14/21 [00:00<00:00, 139.29it/s, Loss=0.7847, Acc=71.43%]2025-11-01 14:26:29,878 - supervised_finetuning - INFO - Epoch 1994 - supervised_loss/batch: 0.7847\n",
            "2025-11-01 14:26:29,878 - supervised_finetuning - INFO - Epoch 1994 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:29,878 - supervised_finetuning - INFO - Epoch 1994 - learning_rate: 0.0001\n",
            "Classifier Epoch 95/100: 100% 21/21 [00:00<00:00, 137.70it/s, Loss=0.7847, Acc=71.43%]\n",
            "2025-11-01 14:26:29,897 - supervised_finetuning - INFO - Epoch 95:\n",
            "2025-11-01 14:26:29,897 - supervised_finetuning - INFO -   Train Loss: 0.7246, Train Acc: 70.32%\n",
            "2025-11-01 14:26:29,897 - supervised_finetuning - INFO -   Train F1: 69.99%, Train Recall: 70.32%\n",
            "2025-11-01 14:26:29,897 - supervised_finetuning - INFO -   Train Precision: 70.01%\n",
            "2025-11-01 14:26:29,898 - supervised_finetuning - INFO - Epoch 94 - supervised_loss/epoch: 0.7246\n",
            "2025-11-01 14:26:29,898 - supervised_finetuning - INFO - Epoch 94 - supervised_accuracy/epoch: 70.3186\n",
            "2025-11-01 14:26:29,898 - supervised_finetuning - INFO - Epoch 94 - supervised_f1/epoch: 69.9912\n",
            "2025-11-01 14:26:29,899 - supervised_finetuning - INFO - Epoch 94 - supervised_recall/epoch: 70.3186\n",
            "2025-11-01 14:26:29,899 - supervised_finetuning - INFO - Epoch 94 - supervised_precision/epoch: 70.0125\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-11-01 14:26:31,230 - supervised_finetuning - INFO -   ✓ New best model saved! Train Acc: 70.32%\n",
            "Classifier Epoch 96/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7761, Acc=64.06%]2025-11-01 14:26:31,239 - supervised_finetuning - INFO - Epoch 1995 - supervised_loss/batch: 0.7761\n",
            "2025-11-01 14:26:31,239 - supervised_finetuning - INFO - Epoch 1995 - supervised_accuracy/batch: 64.0625\n",
            "2025-11-01 14:26:31,239 - supervised_finetuning - INFO - Epoch 1995 - learning_rate: 0.0001\n",
            "Classifier Epoch 96/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8241, Acc=62.50%]2025-11-01 14:26:31,314 - supervised_finetuning - INFO - Epoch 2005 - supervised_loss/batch: 0.8241\n",
            "2025-11-01 14:26:31,314 - supervised_finetuning - INFO - Epoch 2005 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:26:31,314 - supervised_finetuning - INFO - Epoch 2005 - learning_rate: 0.0001\n",
            "Classifier Epoch 96/100:  67% 14/21 [00:00<00:00, 135.44it/s, Loss=1.1845, Acc=57.14%]2025-11-01 14:26:31,379 - supervised_finetuning - INFO - Epoch 2015 - supervised_loss/batch: 1.1845\n",
            "2025-11-01 14:26:31,379 - supervised_finetuning - INFO - Epoch 2015 - supervised_accuracy/batch: 57.1429\n",
            "2025-11-01 14:26:31,379 - supervised_finetuning - INFO - Epoch 2015 - learning_rate: 0.0001\n",
            "Classifier Epoch 96/100: 100% 21/21 [00:00<00:00, 140.99it/s, Loss=1.1845, Acc=57.14%]\n",
            "2025-11-01 14:26:31,398 - supervised_finetuning - INFO - Epoch 96:\n",
            "2025-11-01 14:26:31,398 - supervised_finetuning - INFO -   Train Loss: 0.7641, Train Acc: 68.76%\n",
            "2025-11-01 14:26:31,398 - supervised_finetuning - INFO -   Train F1: 68.44%, Train Recall: 68.76%\n",
            "2025-11-01 14:26:31,398 - supervised_finetuning - INFO -   Train Precision: 68.58%\n",
            "2025-11-01 14:26:31,398 - supervised_finetuning - INFO - Epoch 95 - supervised_loss/epoch: 0.7641\n",
            "2025-11-01 14:26:31,398 - supervised_finetuning - INFO - Epoch 95 - supervised_accuracy/epoch: 68.7646\n",
            "2025-11-01 14:26:31,398 - supervised_finetuning - INFO - Epoch 95 - supervised_f1/epoch: 68.4433\n",
            "2025-11-01 14:26:31,399 - supervised_finetuning - INFO - Epoch 95 - supervised_recall/epoch: 68.7646\n",
            "2025-11-01 14:26:31,399 - supervised_finetuning - INFO - Epoch 95 - supervised_precision/epoch: 68.5804\n",
            "Classifier Epoch 97/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6689, Acc=73.44%]2025-11-01 14:26:31,405 - supervised_finetuning - INFO - Epoch 2016 - supervised_loss/batch: 0.6689\n",
            "2025-11-01 14:26:31,406 - supervised_finetuning - INFO - Epoch 2016 - supervised_accuracy/batch: 73.4375\n",
            "2025-11-01 14:26:31,406 - supervised_finetuning - INFO - Epoch 2016 - learning_rate: 0.0001\n",
            "Classifier Epoch 97/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7545, Acc=65.62%]2025-11-01 14:26:31,474 - supervised_finetuning - INFO - Epoch 2026 - supervised_loss/batch: 0.7545\n",
            "2025-11-01 14:26:31,475 - supervised_finetuning - INFO - Epoch 2026 - supervised_accuracy/batch: 65.6250\n",
            "2025-11-01 14:26:31,475 - supervised_finetuning - INFO - Epoch 2026 - learning_rate: 0.0001\n",
            "Classifier Epoch 97/100:  71% 15/21 [00:00<00:00, 146.98it/s, Loss=0.9352, Acc=85.71%]2025-11-01 14:26:31,552 - supervised_finetuning - INFO - Epoch 2036 - supervised_loss/batch: 0.9352\n",
            "2025-11-01 14:26:31,553 - supervised_finetuning - INFO - Epoch 2036 - supervised_accuracy/batch: 85.7143\n",
            "2025-11-01 14:26:31,553 - supervised_finetuning - INFO - Epoch 2036 - learning_rate: 0.0001\n",
            "Classifier Epoch 97/100: 100% 21/21 [00:00<00:00, 136.18it/s, Loss=0.9352, Acc=85.71%]\n",
            "2025-11-01 14:26:31,579 - supervised_finetuning - INFO - Epoch 97:\n",
            "2025-11-01 14:26:31,579 - supervised_finetuning - INFO -   Train Loss: 0.7366, Train Acc: 69.08%\n",
            "2025-11-01 14:26:31,579 - supervised_finetuning - INFO -   Train F1: 68.65%, Train Recall: 69.08%\n",
            "2025-11-01 14:26:31,580 - supervised_finetuning - INFO -   Train Precision: 68.76%\n",
            "2025-11-01 14:26:31,580 - supervised_finetuning - INFO - Epoch 96 - supervised_loss/epoch: 0.7366\n",
            "2025-11-01 14:26:31,580 - supervised_finetuning - INFO - Epoch 96 - supervised_accuracy/epoch: 69.0754\n",
            "2025-11-01 14:26:31,581 - supervised_finetuning - INFO - Epoch 96 - supervised_f1/epoch: 68.6532\n",
            "2025-11-01 14:26:31,581 - supervised_finetuning - INFO - Epoch 96 - supervised_recall/epoch: 69.0754\n",
            "2025-11-01 14:26:31,581 - supervised_finetuning - INFO - Epoch 96 - supervised_precision/epoch: 68.7585\n",
            "Classifier Epoch 98/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7147, Acc=71.88%]2025-11-01 14:26:31,589 - supervised_finetuning - INFO - Epoch 2037 - supervised_loss/batch: 0.7147\n",
            "2025-11-01 14:26:31,590 - supervised_finetuning - INFO - Epoch 2037 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:26:31,591 - supervised_finetuning - INFO - Epoch 2037 - learning_rate: 0.0001\n",
            "Classifier Epoch 98/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6489, Acc=70.31%]2025-11-01 14:26:31,665 - supervised_finetuning - INFO - Epoch 2047 - supervised_loss/batch: 0.6489\n",
            "2025-11-01 14:26:31,665 - supervised_finetuning - INFO - Epoch 2047 - supervised_accuracy/batch: 70.3125\n",
            "2025-11-01 14:26:31,665 - supervised_finetuning - INFO - Epoch 2047 - learning_rate: 0.0001\n",
            "Classifier Epoch 98/100:  67% 14/21 [00:00<00:00, 134.38it/s, Loss=1.0260, Acc=42.86%]2025-11-01 14:26:31,737 - supervised_finetuning - INFO - Epoch 2057 - supervised_loss/batch: 1.0260\n",
            "2025-11-01 14:26:31,737 - supervised_finetuning - INFO - Epoch 2057 - supervised_accuracy/batch: 42.8571\n",
            "2025-11-01 14:26:31,738 - supervised_finetuning - INFO - Epoch 2057 - learning_rate: 0.0001\n",
            "Classifier Epoch 98/100: 100% 21/21 [00:00<00:00, 134.45it/s, Loss=1.0260, Acc=42.86%]\n",
            "2025-11-01 14:26:31,757 - supervised_finetuning - INFO - Epoch 98:\n",
            "2025-11-01 14:26:31,757 - supervised_finetuning - INFO -   Train Loss: 0.7396, Train Acc: 68.76%\n",
            "2025-11-01 14:26:31,757 - supervised_finetuning - INFO -   Train F1: 68.34%, Train Recall: 68.76%\n",
            "2025-11-01 14:26:31,757 - supervised_finetuning - INFO -   Train Precision: 68.10%\n",
            "2025-11-01 14:26:31,757 - supervised_finetuning - INFO - Epoch 97 - supervised_loss/epoch: 0.7396\n",
            "2025-11-01 14:26:31,758 - supervised_finetuning - INFO - Epoch 97 - supervised_accuracy/epoch: 68.7646\n",
            "2025-11-01 14:26:31,758 - supervised_finetuning - INFO - Epoch 97 - supervised_f1/epoch: 68.3391\n",
            "2025-11-01 14:26:31,758 - supervised_finetuning - INFO - Epoch 97 - supervised_recall/epoch: 68.7646\n",
            "2025-11-01 14:26:31,758 - supervised_finetuning - INFO - Epoch 97 - supervised_precision/epoch: 68.0960\n",
            "Classifier Epoch 99/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.6177, Acc=75.00%]2025-11-01 14:26:31,765 - supervised_finetuning - INFO - Epoch 2058 - supervised_loss/batch: 0.6177\n",
            "2025-11-01 14:26:31,766 - supervised_finetuning - INFO - Epoch 2058 - supervised_accuracy/batch: 75.0000\n",
            "2025-11-01 14:26:31,767 - supervised_finetuning - INFO - Epoch 2058 - learning_rate: 0.0001\n",
            "Classifier Epoch 99/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8219, Acc=62.50%]2025-11-01 14:26:31,843 - supervised_finetuning - INFO - Epoch 2068 - supervised_loss/batch: 0.8219\n",
            "2025-11-01 14:26:31,843 - supervised_finetuning - INFO - Epoch 2068 - supervised_accuracy/batch: 62.5000\n",
            "2025-11-01 14:26:31,844 - supervised_finetuning - INFO - Epoch 2068 - learning_rate: 0.0001\n",
            "Classifier Epoch 99/100:  62% 13/21 [00:00<00:00, 127.52it/s, Loss=0.6698, Acc=71.43%]2025-11-01 14:26:31,942 - supervised_finetuning - INFO - Epoch 2078 - supervised_loss/batch: 0.6698\n",
            "2025-11-01 14:26:31,943 - supervised_finetuning - INFO - Epoch 2078 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:31,944 - supervised_finetuning - INFO - Epoch 2078 - learning_rate: 0.0001\n",
            "Classifier Epoch 99/100: 100% 21/21 [00:00<00:00, 113.04it/s, Loss=0.6698, Acc=71.43%]\n",
            "2025-11-01 14:26:31,975 - supervised_finetuning - INFO - Epoch 99:\n",
            "2025-11-01 14:26:31,975 - supervised_finetuning - INFO -   Train Loss: 0.7287, Train Acc: 68.84%\n",
            "2025-11-01 14:26:31,975 - supervised_finetuning - INFO -   Train F1: 68.68%, Train Recall: 68.84%\n",
            "2025-11-01 14:26:31,975 - supervised_finetuning - INFO -   Train Precision: 69.35%\n",
            "2025-11-01 14:26:31,976 - supervised_finetuning - INFO - Epoch 98 - supervised_loss/epoch: 0.7287\n",
            "2025-11-01 14:26:31,976 - supervised_finetuning - INFO - Epoch 98 - supervised_accuracy/epoch: 68.8423\n",
            "2025-11-01 14:26:31,976 - supervised_finetuning - INFO - Epoch 98 - supervised_f1/epoch: 68.6779\n",
            "2025-11-01 14:26:31,977 - supervised_finetuning - INFO - Epoch 98 - supervised_recall/epoch: 68.8423\n",
            "2025-11-01 14:26:31,977 - supervised_finetuning - INFO - Epoch 98 - supervised_precision/epoch: 69.3465\n",
            "Classifier Epoch 100/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.7101, Acc=75.00%]2025-11-01 14:26:31,986 - supervised_finetuning - INFO - Epoch 2079 - supervised_loss/batch: 0.7101\n",
            "2025-11-01 14:26:31,987 - supervised_finetuning - INFO - Epoch 2079 - supervised_accuracy/batch: 75.0000\n",
            "2025-11-01 14:26:31,987 - supervised_finetuning - INFO - Epoch 2079 - learning_rate: 0.0001\n",
            "Classifier Epoch 100/100:   0% 0/21 [00:00<?, ?it/s, Loss=0.8026, Acc=71.88%]2025-11-01 14:26:32,056 - supervised_finetuning - INFO - Epoch 2089 - supervised_loss/batch: 0.8026\n",
            "2025-11-01 14:26:32,056 - supervised_finetuning - INFO - Epoch 2089 - supervised_accuracy/batch: 71.8750\n",
            "2025-11-01 14:26:32,057 - supervised_finetuning - INFO - Epoch 2089 - learning_rate: 0.0001\n",
            "Classifier Epoch 100/100:  67% 14/21 [00:00<00:00, 139.69it/s, Loss=0.7087, Acc=71.43%]2025-11-01 14:26:32,128 - supervised_finetuning - INFO - Epoch 2099 - supervised_loss/batch: 0.7087\n",
            "2025-11-01 14:26:32,129 - supervised_finetuning - INFO - Epoch 2099 - supervised_accuracy/batch: 71.4286\n",
            "2025-11-01 14:26:32,129 - supervised_finetuning - INFO - Epoch 2099 - learning_rate: 0.0001\n",
            "Classifier Epoch 100/100: 100% 21/21 [00:00<00:00, 138.47it/s, Loss=0.7087, Acc=71.43%]\n",
            "2025-11-01 14:26:32,148 - supervised_finetuning - INFO - Epoch 100:\n",
            "2025-11-01 14:26:32,148 - supervised_finetuning - INFO -   Train Loss: 0.7213, Train Acc: 69.15%\n",
            "2025-11-01 14:26:32,148 - supervised_finetuning - INFO -   Train F1: 68.62%, Train Recall: 69.15%\n",
            "2025-11-01 14:26:32,148 - supervised_finetuning - INFO -   Train Precision: 68.69%\n",
            "2025-11-01 14:26:32,148 - supervised_finetuning - INFO - Epoch 99 - supervised_loss/epoch: 0.7213\n",
            "2025-11-01 14:26:32,148 - supervised_finetuning - INFO - Epoch 99 - supervised_accuracy/epoch: 69.1531\n",
            "2025-11-01 14:26:32,149 - supervised_finetuning - INFO - Epoch 99 - supervised_f1/epoch: 68.6188\n",
            "2025-11-01 14:26:32,149 - supervised_finetuning - INFO - Epoch 99 - supervised_recall/epoch: 69.1531\n",
            "2025-11-01 14:26:32,149 - supervised_finetuning - INFO - Epoch 99 - supervised_precision/epoch: 68.6930\n",
            "2025-11-01 14:26:33,979 - supervised_finetuning - INFO - Supervised fine-tuning completed! Best Train Acc: 70.32%\n",
            "2025-11-01 14:26:33,979 - supervised_finetuning - INFO - ✓ Encoder: FROZEN (contrastive features)\n",
            "2025-11-01 14:26:33,979 - supervised_finetuning - INFO - ✓ Classifier: TRAINED (on label features)\n",
            "2025-11-01 14:26:33,979 - supervised_finetuning - INFO - === TWO-STAGE Supervised Fine-tuning Finished ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation (5-class dataset)"
      ],
      "metadata": {
        "id": "eLS_xJ6dRy8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing / model evaluation:\n",
        "!python \"/content/SSCLNet-Implementation/eval.py\""
      ],
      "metadata": {
        "id": "c9GFNgJXRHML",
        "outputId": "f029a5c4-322a-4b75-d7b8-adc9d90afaef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-01 14:26:51.827700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762007211.877114   53582 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762007211.894682   53582 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762007211.929838   53582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762007211.930000   53582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762007211.930355   53582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762007211.930521   53582 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-01 14:26:51.941366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✓ All directories created\n",
            "✓ All directories created\n",
            "2025-11-01 14:27:00,574 - model_evaluation - INFO - === Model Evaluation on Test Set ===\n",
            "================================================================================\n",
            "MODEL SUMMARY\n",
            "================================================================================\n",
            "Model: SSCLNet\n",
            "Total trainable parameters: 25,669,541\n",
            "Model size: 98.12 MB\n",
            "\n",
            "Layer breakdown:\n",
            "  encoder.conv1.weight: 3,136\n",
            "  encoder.bn1.weight: 64\n",
            "  encoder.bn1.bias: 64\n",
            "  encoder.layer1.0.conv1.weight: 4,096\n",
            "  encoder.layer1.0.bn1.weight: 64\n",
            "  encoder.layer1.0.bn1.bias: 64\n",
            "  encoder.layer1.0.conv2.weight: 36,864\n",
            "  encoder.layer1.0.bn2.weight: 64\n",
            "  encoder.layer1.0.bn2.bias: 64\n",
            "  encoder.layer1.0.conv3.weight: 16,384\n",
            "  encoder.layer1.0.bn3.weight: 256\n",
            "  encoder.layer1.0.bn3.bias: 256\n",
            "  encoder.layer1.0.downsample.0.weight: 16,384\n",
            "  encoder.layer1.0.downsample.1.weight: 256\n",
            "  encoder.layer1.0.downsample.1.bias: 256\n",
            "  encoder.layer1.1.conv1.weight: 16,384\n",
            "  encoder.layer1.1.bn1.weight: 64\n",
            "  encoder.layer1.1.bn1.bias: 64\n",
            "  encoder.layer1.1.conv2.weight: 36,864\n",
            "  encoder.layer1.1.bn2.weight: 64\n",
            "  encoder.layer1.1.bn2.bias: 64\n",
            "  encoder.layer1.1.conv3.weight: 16,384\n",
            "  encoder.layer1.1.bn3.weight: 256\n",
            "  encoder.layer1.1.bn3.bias: 256\n",
            "  encoder.layer1.2.conv1.weight: 16,384\n",
            "  encoder.layer1.2.bn1.weight: 64\n",
            "  encoder.layer1.2.bn1.bias: 64\n",
            "  encoder.layer1.2.conv2.weight: 36,864\n",
            "  encoder.layer1.2.bn2.weight: 64\n",
            "  encoder.layer1.2.bn2.bias: 64\n",
            "  encoder.layer1.2.conv3.weight: 16,384\n",
            "  encoder.layer1.2.bn3.weight: 256\n",
            "  encoder.layer1.2.bn3.bias: 256\n",
            "  encoder.layer2.0.conv1.weight: 32,768\n",
            "  encoder.layer2.0.bn1.weight: 128\n",
            "  encoder.layer2.0.bn1.bias: 128\n",
            "  encoder.layer2.0.conv2.weight: 147,456\n",
            "  encoder.layer2.0.bn2.weight: 128\n",
            "  encoder.layer2.0.bn2.bias: 128\n",
            "  encoder.layer2.0.conv3.weight: 65,536\n",
            "  encoder.layer2.0.bn3.weight: 512\n",
            "  encoder.layer2.0.bn3.bias: 512\n",
            "  encoder.layer2.0.downsample.0.weight: 131,072\n",
            "  encoder.layer2.0.downsample.1.weight: 512\n",
            "  encoder.layer2.0.downsample.1.bias: 512\n",
            "  encoder.layer2.1.conv1.weight: 65,536\n",
            "  encoder.layer2.1.bn1.weight: 128\n",
            "  encoder.layer2.1.bn1.bias: 128\n",
            "  encoder.layer2.1.conv2.weight: 147,456\n",
            "  encoder.layer2.1.bn2.weight: 128\n",
            "  encoder.layer2.1.bn2.bias: 128\n",
            "  encoder.layer2.1.conv3.weight: 65,536\n",
            "  encoder.layer2.1.bn3.weight: 512\n",
            "  encoder.layer2.1.bn3.bias: 512\n",
            "  encoder.layer2.2.conv1.weight: 65,536\n",
            "  encoder.layer2.2.bn1.weight: 128\n",
            "  encoder.layer2.2.bn1.bias: 128\n",
            "  encoder.layer2.2.conv2.weight: 147,456\n",
            "  encoder.layer2.2.bn2.weight: 128\n",
            "  encoder.layer2.2.bn2.bias: 128\n",
            "  encoder.layer2.2.conv3.weight: 65,536\n",
            "  encoder.layer2.2.bn3.weight: 512\n",
            "  encoder.layer2.2.bn3.bias: 512\n",
            "  encoder.layer2.3.conv1.weight: 65,536\n",
            "  encoder.layer2.3.bn1.weight: 128\n",
            "  encoder.layer2.3.bn1.bias: 128\n",
            "  encoder.layer2.3.conv2.weight: 147,456\n",
            "  encoder.layer2.3.bn2.weight: 128\n",
            "  encoder.layer2.3.bn2.bias: 128\n",
            "  encoder.layer2.3.conv3.weight: 65,536\n",
            "  encoder.layer2.3.bn3.weight: 512\n",
            "  encoder.layer2.3.bn3.bias: 512\n",
            "  encoder.layer3.0.conv1.weight: 131,072\n",
            "  encoder.layer3.0.bn1.weight: 256\n",
            "  encoder.layer3.0.bn1.bias: 256\n",
            "  encoder.layer3.0.conv2.weight: 589,824\n",
            "  encoder.layer3.0.bn2.weight: 256\n",
            "  encoder.layer3.0.bn2.bias: 256\n",
            "  encoder.layer3.0.conv3.weight: 262,144\n",
            "  encoder.layer3.0.bn3.weight: 1,024\n",
            "  encoder.layer3.0.bn3.bias: 1,024\n",
            "  encoder.layer3.0.downsample.0.weight: 524,288\n",
            "  encoder.layer3.0.downsample.1.weight: 1,024\n",
            "  encoder.layer3.0.downsample.1.bias: 1,024\n",
            "  encoder.layer3.1.conv1.weight: 262,144\n",
            "  encoder.layer3.1.bn1.weight: 256\n",
            "  encoder.layer3.1.bn1.bias: 256\n",
            "  encoder.layer3.1.conv2.weight: 589,824\n",
            "  encoder.layer3.1.bn2.weight: 256\n",
            "  encoder.layer3.1.bn2.bias: 256\n",
            "  encoder.layer3.1.conv3.weight: 262,144\n",
            "  encoder.layer3.1.bn3.weight: 1,024\n",
            "  encoder.layer3.1.bn3.bias: 1,024\n",
            "  encoder.layer3.2.conv1.weight: 262,144\n",
            "  encoder.layer3.2.bn1.weight: 256\n",
            "  encoder.layer3.2.bn1.bias: 256\n",
            "  encoder.layer3.2.conv2.weight: 589,824\n",
            "  encoder.layer3.2.bn2.weight: 256\n",
            "  encoder.layer3.2.bn2.bias: 256\n",
            "  encoder.layer3.2.conv3.weight: 262,144\n",
            "  encoder.layer3.2.bn3.weight: 1,024\n",
            "  encoder.layer3.2.bn3.bias: 1,024\n",
            "  encoder.layer3.3.conv1.weight: 262,144\n",
            "  encoder.layer3.3.bn1.weight: 256\n",
            "  encoder.layer3.3.bn1.bias: 256\n",
            "  encoder.layer3.3.conv2.weight: 589,824\n",
            "  encoder.layer3.3.bn2.weight: 256\n",
            "  encoder.layer3.3.bn2.bias: 256\n",
            "  encoder.layer3.3.conv3.weight: 262,144\n",
            "  encoder.layer3.3.bn3.weight: 1,024\n",
            "  encoder.layer3.3.bn3.bias: 1,024\n",
            "  encoder.layer3.4.conv1.weight: 262,144\n",
            "  encoder.layer3.4.bn1.weight: 256\n",
            "  encoder.layer3.4.bn1.bias: 256\n",
            "  encoder.layer3.4.conv2.weight: 589,824\n",
            "  encoder.layer3.4.bn2.weight: 256\n",
            "  encoder.layer3.4.bn2.bias: 256\n",
            "  encoder.layer3.4.conv3.weight: 262,144\n",
            "  encoder.layer3.4.bn3.weight: 1,024\n",
            "  encoder.layer3.4.bn3.bias: 1,024\n",
            "  encoder.layer3.5.conv1.weight: 262,144\n",
            "  encoder.layer3.5.bn1.weight: 256\n",
            "  encoder.layer3.5.bn1.bias: 256\n",
            "  encoder.layer3.5.conv2.weight: 589,824\n",
            "  encoder.layer3.5.bn2.weight: 256\n",
            "  encoder.layer3.5.bn2.bias: 256\n",
            "  encoder.layer3.5.conv3.weight: 262,144\n",
            "  encoder.layer3.5.bn3.weight: 1,024\n",
            "  encoder.layer3.5.bn3.bias: 1,024\n",
            "  encoder.layer4.0.conv1.weight: 524,288\n",
            "  encoder.layer4.0.bn1.weight: 512\n",
            "  encoder.layer4.0.bn1.bias: 512\n",
            "  encoder.layer4.0.conv2.weight: 2,359,296\n",
            "  encoder.layer4.0.bn2.weight: 512\n",
            "  encoder.layer4.0.bn2.bias: 512\n",
            "  encoder.layer4.0.conv3.weight: 1,048,576\n",
            "  encoder.layer4.0.bn3.weight: 2,048\n",
            "  encoder.layer4.0.bn3.bias: 2,048\n",
            "  encoder.layer4.0.downsample.0.weight: 2,097,152\n",
            "  encoder.layer4.0.downsample.1.weight: 2,048\n",
            "  encoder.layer4.0.downsample.1.bias: 2,048\n",
            "  encoder.layer4.1.conv1.weight: 1,048,576\n",
            "  encoder.layer4.1.bn1.weight: 512\n",
            "  encoder.layer4.1.bn1.bias: 512\n",
            "  encoder.layer4.1.conv2.weight: 2,359,296\n",
            "  encoder.layer4.1.bn2.weight: 512\n",
            "  encoder.layer4.1.bn2.bias: 512\n",
            "  encoder.layer4.1.conv3.weight: 1,048,576\n",
            "  encoder.layer4.1.bn3.weight: 2,048\n",
            "  encoder.layer4.1.bn3.bias: 2,048\n",
            "  encoder.layer4.2.conv1.weight: 1,048,576\n",
            "  encoder.layer4.2.bn1.weight: 512\n",
            "  encoder.layer4.2.bn1.bias: 512\n",
            "  encoder.layer4.2.conv2.weight: 2,359,296\n",
            "  encoder.layer4.2.bn2.weight: 512\n",
            "  encoder.layer4.2.bn2.bias: 512\n",
            "  encoder.layer4.2.conv3.weight: 1,048,576\n",
            "  encoder.layer4.2.bn3.weight: 2,048\n",
            "  encoder.layer4.2.bn3.bias: 2,048\n",
            "  projection_head.mlp.0.weight: 1,048,576\n",
            "  projection_head.mlp.0.bias: 512\n",
            "  projection_head.mlp.2.weight: 262,144\n",
            "  projection_head.mlp.2.bias: 512\n",
            "  projection_head.mlp.4.weight: 131,072\n",
            "  projection_head.mlp.4.bias: 256\n",
            "  projection_head.mlp.6.weight: 65,536\n",
            "  projection_head.mlp.6.bias: 256\n",
            "  projection_head.mlp.9.weight: 8,192\n",
            "  projection_head.mlp.9.bias: 32\n",
            "  classifier.classifier.0.weight: 524,288\n",
            "  classifier.classifier.0.bias: 256\n",
            "  classifier.classifier.2.weight: 65,536\n",
            "  classifier.classifier.2.bias: 256\n",
            "  classifier.classifier.4.weight: 32,768\n",
            "  classifier.classifier.4.bias: 128\n",
            "  classifier.classifier.6.weight: 16,384\n",
            "  classifier.classifier.6.bias: 128\n",
            "  classifier.classifier.8.weight: 8,192\n",
            "  classifier.classifier.8.bias: 64\n",
            "  classifier.classifier.10.weight: 2,048\n",
            "  classifier.classifier.10.bias: 32\n",
            "  classifier.classifier.12.weight: 512\n",
            "  classifier.classifier.12.bias: 16\n",
            "  classifier.classifier.14.weight: 80\n",
            "  classifier.classifier.14.bias: 5\n",
            "================================================================================\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Created test DataLoader:\n",
            "  - Batch size: 64\n",
            "  - Shuffle: False\n",
            "  - Workers: 4\n",
            "  - Classes: ['Glioblastoma', 'glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
            "  - Number of samples: 655\n",
            "==================================================\n",
            "DATASET ANALYSIS\n",
            "==================================================\n",
            "Total samples: 655\n",
            "Class distribution:\n",
            "  Glioblastoma: 90 (13.7%)\n",
            "  glioma_tumor: 149 (22.7%)\n",
            "  meningioma_tumor: 163 (24.9%)\n",
            "  no_tumor: 95 (14.5%)\n",
            "  pituitary_tumor: 158 (24.1%)\n",
            "2025-11-01 14:27:03,471 - model_evaluation - INFO - Evaluating on 655 test samples\n",
            "2025-11-01 14:27:03,472 - model_evaluation - INFO - Running evaluation on test set...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "2025-11-01 14:27:06,971 - model_evaluation - INFO - Processed 10/11 batches\n",
            "2025-11-01 14:27:07,205 - model_evaluation - INFO - \n",
            "==================================================\n",
            "2025-11-01 14:27:07,205 - model_evaluation - INFO - FINAL TEST RESULTS\n",
            "2025-11-01 14:27:07,205 - model_evaluation - INFO - ==================================================\n",
            "2025-11-01 14:27:07,205 - model_evaluation - INFO - Test Loss: 0.7804\n",
            "2025-11-01 14:27:07,206 - model_evaluation - INFO - Test Accuracy: 65.65%\n",
            "2025-11-01 14:27:07,206 - model_evaluation - INFO - Test Precision: 65.03%\n",
            "2025-11-01 14:27:07,206 - model_evaluation - INFO - Test Recall: 65.65%\n",
            "2025-11-01 14:27:07,206 - model_evaluation - INFO - Test F1-Score: 65.05%\n",
            "2025-11-01 14:27:07,206 - model_evaluation - INFO - ==================================================\n",
            "2025-11-01 14:27:07,206 - model_evaluation - INFO - \n",
            "DETAILED CLASSIFICATION REPORT:\n",
            "2025-11-01 14:27:07,206 - model_evaluation - INFO - ==================================================\n",
            "2025-11-01 14:27:07,217 - model_evaluation - INFO - \n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "    Glioblastoma       0.95      0.97      0.96        90\n",
            "    glioma_tumor       0.55      0.42      0.48       149\n",
            "meningioma_tumor       0.52      0.53      0.53       163\n",
            "        no_tumor       0.59      0.68      0.63        95\n",
            " pituitary_tumor       0.74      0.81      0.77       158\n",
            "\n",
            "        accuracy                           0.66       655\n",
            "       macro avg       0.67      0.68      0.67       655\n",
            "    weighted avg       0.65      0.66      0.65       655\n",
            "\n",
            "2025-11-01 14:27:07,217 - model_evaluation - INFO - \n",
            "CONFUSION MATRIX:\n",
            "2025-11-01 14:27:07,217 - model_evaluation - INFO - ==================================================\n",
            "2025-11-01 14:27:07,219 - model_evaluation - INFO - \n",
            "[[ 87   0   0   3   0]\n",
            " [  4  63  52  20  10]\n",
            " [  0  31  87  16  29]\n",
            " [  1   9  14  65   6]\n",
            " [  0  11  13   6 128]]\n",
            "2025-11-01 14:27:08,664 - model_evaluation - INFO - Confusion matrix saved as 'plots/confusion_matrix.png'\n",
            "2025-11-01 14:27:08,664 - model_evaluation - INFO - \n",
            "==================================================\n",
            "2025-11-01 14:27:08,664 - model_evaluation - INFO - ROC CURVE ANALYSIS\n",
            "2025-11-01 14:27:08,664 - model_evaluation - INFO - ==================================================\n",
            "2025-11-01 14:27:10,206 - model_evaluation - INFO - ROC curve saved as 'plots/roc_curve.png'\n",
            "2025-11-01 14:27:10,207 - model_evaluation - INFO - \n",
            "AUC Scores:\n",
            "2025-11-01 14:27:10,207 - model_evaluation - INFO -   Glioblastoma: 0.9990\n",
            "2025-11-01 14:27:10,207 - model_evaluation - INFO -   glioma_tumor: 0.8038\n",
            "2025-11-01 14:27:10,207 - model_evaluation - INFO -   meningioma_tumor: 0.8036\n",
            "2025-11-01 14:27:10,208 - model_evaluation - INFO -   no_tumor: 0.9373\n",
            "2025-11-01 14:27:10,208 - model_evaluation - INFO -   pituitary_tumor: 0.9372\n",
            "2025-11-01 14:27:10,208 - model_evaluation - INFO -   Micro-average: 0.9156\n",
            "2025-11-01 14:27:10,208 - model_evaluation - INFO - \n",
            "PER-CLASS ACCURACY:\n",
            "2025-11-01 14:27:10,208 - model_evaluation - INFO - ==================================================\n",
            "2025-11-01 14:27:10,209 - model_evaluation - INFO - Glioblastoma: 96.67%\n",
            "2025-11-01 14:27:10,209 - model_evaluation - INFO - glioma_tumor: 42.28%\n",
            "2025-11-01 14:27:10,209 - model_evaluation - INFO - meningioma_tumor: 53.37%\n",
            "2025-11-01 14:27:10,210 - model_evaluation - INFO - no_tumor: 68.42%\n",
            "2025-11-01 14:27:10,210 - model_evaluation - INFO - pituitary_tumor: 81.01%\n",
            "2025-11-01 14:27:10,210 - model_evaluation - INFO - === Model Evaluation Completed ===\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "✓ Results saved to: results/final_evaluation_results.json\n",
            "\n",
            "✓ Evaluation completed! Check:\n",
            "  - logs/model_evaluation.log for detailed logs\n",
            "  - results/final_evaluation_results.json for comprehensive results\n",
            "  - plots/ for confusion matrix and ROC curves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deleting the previously created folder \"SSCLNet-implementation-outputs\" and recreating it and saving everything in one go\n",
        "!rm -rf '/content/SSCLNet-implementation-outputs'"
      ],
      "metadata": {
        "id": "01U705WVwBHQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving it all for backup\n",
        "import os\n",
        "\n",
        "# Make a new folder in Colab’s /content directory\n",
        "new_folder = \"/content/SSCLNet-implementation-outputs/5-class\"\n",
        "os.makedirs(new_folder, exist_ok=True)\n",
        "\n",
        "print(f\"Created: {new_folder}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8A-xGk59EGK",
        "outputId": "27aebec9-4c46-486f-d850-da7b0fd7e244"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created: /content/SSCLNet-implementation-outputs/5-class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/checkpoints \"/content/SSCLNet-implementation-outputs/5-class\"\n",
        "!cp -r /content/configs \"/content/SSCLNet-implementation-outputs/5-class\"\n",
        "!cp -r /content/logs \"/content/SSCLNet-implementation-outputs/5-class\"\n",
        "!cp -r /content/models \"/content/SSCLNet-implementation-outputs/5-class\"\n",
        "!cp -r /content/plots \"/content/SSCLNet-implementation-outputs/5-class\"\n",
        "!cp -r /content/results \"/content/SSCLNet-implementation-outputs/5-class\"\n",
        "!cp -r /content/runs \"/content/SSCLNet-implementation-outputs/5-class\"\n",
        "!cp -r /content/training_output \"/content/SSCLNet-implementation-outputs/5-class\"\n",
        "\n",
        "!zip -r SSCLNet_implementation_outputs_5Class.zip \"/content/SSCLNet-implementation-outputs/5-class\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbTOpWny9av5",
        "outputId": "7cb9ae6d-4c27-450c-ad55-4d1ce7f5a7cc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/SSCLNet-implementation-outputs/5-class/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/results/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/results/final_evaluation_results.json (deflated 59%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/results/test_results.txt (deflated 43%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/contrastive_pretraining/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/contrastive_pretraining/events.out.tfevents.1761995789.8022d5696356.3017.0 (deflated 71%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/supervised_finetuning/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/supervised_finetuning/events.out.tfevents.1762006146.8022d5696356.48883.0 (deflated 77%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/supervised_finetuning/events.out.tfevents.1762007065.8022d5696356.52869.0 (deflated 9%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/supervised_finetuning/events.out.tfevents.1762007104.8022d5696356.53053.0 (deflated 75%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/supervised_finetuning/events.out.tfevents.1762004972.8022d5696356.43789.0 (deflated 75%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/model_evaluation/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/model_evaluation/events.out.tfevents.1762006174.8022d5696356.49052.0 (deflated 9%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/model_evaluation/events.out.tfevents.1762007220.8022d5696356.53582.0 (deflated 9%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/model_evaluation/events.out.tfevents.1762006325.8022d5696356.49718.0 (deflated 9%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/runs/model_evaluation/events.out.tfevents.1762005846.8022d5696356.47555.0 (deflated 9%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/logs/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/logs/contrastive_pretraining.log (deflated 89%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/logs/model_evaluation.log (deflated 87%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/logs/supervised_finetuning.log (deflated 92%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/training_output/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/training_output/supervised_training/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/training_output/supervised_training/models/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/training_output/supervised_training/models/brain_mri_5class/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/training_output/supervised_training/models/brain_mri_5class/supervised_final.pth (deflated 7%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/training_output/supervised_training/models/supervised_final.pth (deflated 7%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/training_output/contrastive_training/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/training_output/contrastive_training/models/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/training_output/contrastive_training/models/contrastive_pretrained.pth (deflated 7%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_9.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_19.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_57.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_66.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_78.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_50.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_37.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_12.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_7.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_8.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_26.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_16.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_10.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/contrastive_epoch_100.pth (deflated 9%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_11.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_85.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_91.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_76.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_79.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_95.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_28.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_36.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_23.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_6.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_15.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_4.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_3.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_5.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_13.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/contrastive_epoch_80.pth (deflated 9%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/contrastive_epoch_60.pth (deflated 9%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/contrastive_epoch_40.pth (deflated 9%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_1.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_62.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_24.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/contrastive_epoch_20.pth (deflated 9%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_20.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/checkpoints/supervised_best_epoch_53.pth (deflated 8%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/plots/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/plots/contrastive_loss.png (deflated 23%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/plots/supervised_training_history.png (deflated 26%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/plots/roc_curve.png (deflated 19%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/plots/class_distribution.png (deflated 25%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/plots/confusion_matrix.png (deflated 24%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/configs/ (stored 0%)\n",
            "  adding: content/SSCLNet-implementation-outputs/5-class/models/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training Phase 2 - Supervised Learning (2-class dataset)\n",
        "\n",
        "**Note:** Update the path to `TRAIN_DATA_PATH`, `TEST_DATA_PATH` and `NUM_CLASSES` in `config.py` to the 2-class dataset."
      ],
      "metadata": {
        "id": "9e3z70WhtXQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/SSCLNet-Implementation/train_supervised.py\""
      ],
      "metadata": {
        "id": "X3FjkH-kty4z",
        "outputId": "f4d8c9ad-caf3-4237-f341-6e1b49ec55ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-01 14:49:28.402390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762008568.433642   59414 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762008568.440039   59414 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762008568.457087   59414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762008568.457146   59414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762008568.457152   59414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762008568.457155   59414 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-01 14:49:28.462071: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✓ All directories created\n",
            "✓ All directories created\n",
            "✓ Random seed set to: 42\n",
            "2025-11-01 14:49:34,688 - supervised_finetuning - INFO - === Starting TWO-STAGE Supervised Fine-tuning ===\n",
            "Keys in file: odict_keys(['encoder.conv1.weight', 'encoder.bn1.weight', 'encoder.bn1.bias', 'encoder.bn1.running_mean', 'encoder.bn1.running_var', 'encoder.bn1.num_batches_tracked', 'encoder.layer1.0.conv1.weight', 'encoder.layer1.0.bn1.weight', 'encoder.layer1.0.bn1.bias', 'encoder.layer1.0.bn1.running_mean', 'encoder.layer1.0.bn1.running_var', 'encoder.layer1.0.bn1.num_batches_tracked', 'encoder.layer1.0.conv2.weight', 'encoder.layer1.0.bn2.weight', 'encoder.layer1.0.bn2.bias', 'encoder.layer1.0.bn2.running_mean', 'encoder.layer1.0.bn2.running_var', 'encoder.layer1.0.bn2.num_batches_tracked', 'encoder.layer1.0.conv3.weight', 'encoder.layer1.0.bn3.weight', 'encoder.layer1.0.bn3.bias', 'encoder.layer1.0.bn3.running_mean', 'encoder.layer1.0.bn3.running_var', 'encoder.layer1.0.bn3.num_batches_tracked', 'encoder.layer1.0.downsample.0.weight', 'encoder.layer1.0.downsample.1.weight', 'encoder.layer1.0.downsample.1.bias', 'encoder.layer1.0.downsample.1.running_mean', 'encoder.layer1.0.downsample.1.running_var', 'encoder.layer1.0.downsample.1.num_batches_tracked', 'encoder.layer1.1.conv1.weight', 'encoder.layer1.1.bn1.weight', 'encoder.layer1.1.bn1.bias', 'encoder.layer1.1.bn1.running_mean', 'encoder.layer1.1.bn1.running_var', 'encoder.layer1.1.bn1.num_batches_tracked', 'encoder.layer1.1.conv2.weight', 'encoder.layer1.1.bn2.weight', 'encoder.layer1.1.bn2.bias', 'encoder.layer1.1.bn2.running_mean', 'encoder.layer1.1.bn2.running_var', 'encoder.layer1.1.bn2.num_batches_tracked', 'encoder.layer1.1.conv3.weight', 'encoder.layer1.1.bn3.weight', 'encoder.layer1.1.bn3.bias', 'encoder.layer1.1.bn3.running_mean', 'encoder.layer1.1.bn3.running_var', 'encoder.layer1.1.bn3.num_batches_tracked', 'encoder.layer1.2.conv1.weight', 'encoder.layer1.2.bn1.weight', 'encoder.layer1.2.bn1.bias', 'encoder.layer1.2.bn1.running_mean', 'encoder.layer1.2.bn1.running_var', 'encoder.layer1.2.bn1.num_batches_tracked', 'encoder.layer1.2.conv2.weight', 'encoder.layer1.2.bn2.weight', 'encoder.layer1.2.bn2.bias', 'encoder.layer1.2.bn2.running_mean', 'encoder.layer1.2.bn2.running_var', 'encoder.layer1.2.bn2.num_batches_tracked', 'encoder.layer1.2.conv3.weight', 'encoder.layer1.2.bn3.weight', 'encoder.layer1.2.bn3.bias', 'encoder.layer1.2.bn3.running_mean', 'encoder.layer1.2.bn3.running_var', 'encoder.layer1.2.bn3.num_batches_tracked', 'encoder.layer2.0.conv1.weight', 'encoder.layer2.0.bn1.weight', 'encoder.layer2.0.bn1.bias', 'encoder.layer2.0.bn1.running_mean', 'encoder.layer2.0.bn1.running_var', 'encoder.layer2.0.bn1.num_batches_tracked', 'encoder.layer2.0.conv2.weight', 'encoder.layer2.0.bn2.weight', 'encoder.layer2.0.bn2.bias', 'encoder.layer2.0.bn2.running_mean', 'encoder.layer2.0.bn2.running_var', 'encoder.layer2.0.bn2.num_batches_tracked', 'encoder.layer2.0.conv3.weight', 'encoder.layer2.0.bn3.weight', 'encoder.layer2.0.bn3.bias', 'encoder.layer2.0.bn3.running_mean', 'encoder.layer2.0.bn3.running_var', 'encoder.layer2.0.bn3.num_batches_tracked', 'encoder.layer2.0.downsample.0.weight', 'encoder.layer2.0.downsample.1.weight', 'encoder.layer2.0.downsample.1.bias', 'encoder.layer2.0.downsample.1.running_mean', 'encoder.layer2.0.downsample.1.running_var', 'encoder.layer2.0.downsample.1.num_batches_tracked', 'encoder.layer2.1.conv1.weight', 'encoder.layer2.1.bn1.weight', 'encoder.layer2.1.bn1.bias', 'encoder.layer2.1.bn1.running_mean', 'encoder.layer2.1.bn1.running_var', 'encoder.layer2.1.bn1.num_batches_tracked', 'encoder.layer2.1.conv2.weight', 'encoder.layer2.1.bn2.weight', 'encoder.layer2.1.bn2.bias', 'encoder.layer2.1.bn2.running_mean', 'encoder.layer2.1.bn2.running_var', 'encoder.layer2.1.bn2.num_batches_tracked', 'encoder.layer2.1.conv3.weight', 'encoder.layer2.1.bn3.weight', 'encoder.layer2.1.bn3.bias', 'encoder.layer2.1.bn3.running_mean', 'encoder.layer2.1.bn3.running_var', 'encoder.layer2.1.bn3.num_batches_tracked', 'encoder.layer2.2.conv1.weight', 'encoder.layer2.2.bn1.weight', 'encoder.layer2.2.bn1.bias', 'encoder.layer2.2.bn1.running_mean', 'encoder.layer2.2.bn1.running_var', 'encoder.layer2.2.bn1.num_batches_tracked', 'encoder.layer2.2.conv2.weight', 'encoder.layer2.2.bn2.weight', 'encoder.layer2.2.bn2.bias', 'encoder.layer2.2.bn2.running_mean', 'encoder.layer2.2.bn2.running_var', 'encoder.layer2.2.bn2.num_batches_tracked', 'encoder.layer2.2.conv3.weight', 'encoder.layer2.2.bn3.weight', 'encoder.layer2.2.bn3.bias', 'encoder.layer2.2.bn3.running_mean', 'encoder.layer2.2.bn3.running_var', 'encoder.layer2.2.bn3.num_batches_tracked', 'encoder.layer2.3.conv1.weight', 'encoder.layer2.3.bn1.weight', 'encoder.layer2.3.bn1.bias', 'encoder.layer2.3.bn1.running_mean', 'encoder.layer2.3.bn1.running_var', 'encoder.layer2.3.bn1.num_batches_tracked', 'encoder.layer2.3.conv2.weight', 'encoder.layer2.3.bn2.weight', 'encoder.layer2.3.bn2.bias', 'encoder.layer2.3.bn2.running_mean', 'encoder.layer2.3.bn2.running_var', 'encoder.layer2.3.bn2.num_batches_tracked', 'encoder.layer2.3.conv3.weight', 'encoder.layer2.3.bn3.weight', 'encoder.layer2.3.bn3.bias', 'encoder.layer2.3.bn3.running_mean', 'encoder.layer2.3.bn3.running_var', 'encoder.layer2.3.bn3.num_batches_tracked', 'encoder.layer3.0.conv1.weight', 'encoder.layer3.0.bn1.weight', 'encoder.layer3.0.bn1.bias', 'encoder.layer3.0.bn1.running_mean', 'encoder.layer3.0.bn1.running_var', 'encoder.layer3.0.bn1.num_batches_tracked', 'encoder.layer3.0.conv2.weight', 'encoder.layer3.0.bn2.weight', 'encoder.layer3.0.bn2.bias', 'encoder.layer3.0.bn2.running_mean', 'encoder.layer3.0.bn2.running_var', 'encoder.layer3.0.bn2.num_batches_tracked', 'encoder.layer3.0.conv3.weight', 'encoder.layer3.0.bn3.weight', 'encoder.layer3.0.bn3.bias', 'encoder.layer3.0.bn3.running_mean', 'encoder.layer3.0.bn3.running_var', 'encoder.layer3.0.bn3.num_batches_tracked', 'encoder.layer3.0.downsample.0.weight', 'encoder.layer3.0.downsample.1.weight', 'encoder.layer3.0.downsample.1.bias', 'encoder.layer3.0.downsample.1.running_mean', 'encoder.layer3.0.downsample.1.running_var', 'encoder.layer3.0.downsample.1.num_batches_tracked', 'encoder.layer3.1.conv1.weight', 'encoder.layer3.1.bn1.weight', 'encoder.layer3.1.bn1.bias', 'encoder.layer3.1.bn1.running_mean', 'encoder.layer3.1.bn1.running_var', 'encoder.layer3.1.bn1.num_batches_tracked', 'encoder.layer3.1.conv2.weight', 'encoder.layer3.1.bn2.weight', 'encoder.layer3.1.bn2.bias', 'encoder.layer3.1.bn2.running_mean', 'encoder.layer3.1.bn2.running_var', 'encoder.layer3.1.bn2.num_batches_tracked', 'encoder.layer3.1.conv3.weight', 'encoder.layer3.1.bn3.weight', 'encoder.layer3.1.bn3.bias', 'encoder.layer3.1.bn3.running_mean', 'encoder.layer3.1.bn3.running_var', 'encoder.layer3.1.bn3.num_batches_tracked', 'encoder.layer3.2.conv1.weight', 'encoder.layer3.2.bn1.weight', 'encoder.layer3.2.bn1.bias', 'encoder.layer3.2.bn1.running_mean', 'encoder.layer3.2.bn1.running_var', 'encoder.layer3.2.bn1.num_batches_tracked', 'encoder.layer3.2.conv2.weight', 'encoder.layer3.2.bn2.weight', 'encoder.layer3.2.bn2.bias', 'encoder.layer3.2.bn2.running_mean', 'encoder.layer3.2.bn2.running_var', 'encoder.layer3.2.bn2.num_batches_tracked', 'encoder.layer3.2.conv3.weight', 'encoder.layer3.2.bn3.weight', 'encoder.layer3.2.bn3.bias', 'encoder.layer3.2.bn3.running_mean', 'encoder.layer3.2.bn3.running_var', 'encoder.layer3.2.bn3.num_batches_tracked', 'encoder.layer3.3.conv1.weight', 'encoder.layer3.3.bn1.weight', 'encoder.layer3.3.bn1.bias', 'encoder.layer3.3.bn1.running_mean', 'encoder.layer3.3.bn1.running_var', 'encoder.layer3.3.bn1.num_batches_tracked', 'encoder.layer3.3.conv2.weight', 'encoder.layer3.3.bn2.weight', 'encoder.layer3.3.bn2.bias', 'encoder.layer3.3.bn2.running_mean', 'encoder.layer3.3.bn2.running_var', 'encoder.layer3.3.bn2.num_batches_tracked', 'encoder.layer3.3.conv3.weight', 'encoder.layer3.3.bn3.weight', 'encoder.layer3.3.bn3.bias', 'encoder.layer3.3.bn3.running_mean', 'encoder.layer3.3.bn3.running_var', 'encoder.layer3.3.bn3.num_batches_tracked', 'encoder.layer3.4.conv1.weight', 'encoder.layer3.4.bn1.weight', 'encoder.layer3.4.bn1.bias', 'encoder.layer3.4.bn1.running_mean', 'encoder.layer3.4.bn1.running_var', 'encoder.layer3.4.bn1.num_batches_tracked', 'encoder.layer3.4.conv2.weight', 'encoder.layer3.4.bn2.weight', 'encoder.layer3.4.bn2.bias', 'encoder.layer3.4.bn2.running_mean', 'encoder.layer3.4.bn2.running_var', 'encoder.layer3.4.bn2.num_batches_tracked', 'encoder.layer3.4.conv3.weight', 'encoder.layer3.4.bn3.weight', 'encoder.layer3.4.bn3.bias', 'encoder.layer3.4.bn3.running_mean', 'encoder.layer3.4.bn3.running_var', 'encoder.layer3.4.bn3.num_batches_tracked', 'encoder.layer3.5.conv1.weight', 'encoder.layer3.5.bn1.weight', 'encoder.layer3.5.bn1.bias', 'encoder.layer3.5.bn1.running_mean', 'encoder.layer3.5.bn1.running_var', 'encoder.layer3.5.bn1.num_batches_tracked', 'encoder.layer3.5.conv2.weight', 'encoder.layer3.5.bn2.weight', 'encoder.layer3.5.bn2.bias', 'encoder.layer3.5.bn2.running_mean', 'encoder.layer3.5.bn2.running_var', 'encoder.layer3.5.bn2.num_batches_tracked', 'encoder.layer3.5.conv3.weight', 'encoder.layer3.5.bn3.weight', 'encoder.layer3.5.bn3.bias', 'encoder.layer3.5.bn3.running_mean', 'encoder.layer3.5.bn3.running_var', 'encoder.layer3.5.bn3.num_batches_tracked', 'encoder.layer4.0.conv1.weight', 'encoder.layer4.0.bn1.weight', 'encoder.layer4.0.bn1.bias', 'encoder.layer4.0.bn1.running_mean', 'encoder.layer4.0.bn1.running_var', 'encoder.layer4.0.bn1.num_batches_tracked', 'encoder.layer4.0.conv2.weight', 'encoder.layer4.0.bn2.weight', 'encoder.layer4.0.bn2.bias', 'encoder.layer4.0.bn2.running_mean', 'encoder.layer4.0.bn2.running_var', 'encoder.layer4.0.bn2.num_batches_tracked', 'encoder.layer4.0.conv3.weight', 'encoder.layer4.0.bn3.weight', 'encoder.layer4.0.bn3.bias', 'encoder.layer4.0.bn3.running_mean', 'encoder.layer4.0.bn3.running_var', 'encoder.layer4.0.bn3.num_batches_tracked', 'encoder.layer4.0.downsample.0.weight', 'encoder.layer4.0.downsample.1.weight', 'encoder.layer4.0.downsample.1.bias', 'encoder.layer4.0.downsample.1.running_mean', 'encoder.layer4.0.downsample.1.running_var', 'encoder.layer4.0.downsample.1.num_batches_tracked', 'encoder.layer4.1.conv1.weight', 'encoder.layer4.1.bn1.weight', 'encoder.layer4.1.bn1.bias', 'encoder.layer4.1.bn1.running_mean', 'encoder.layer4.1.bn1.running_var', 'encoder.layer4.1.bn1.num_batches_tracked', 'encoder.layer4.1.conv2.weight', 'encoder.layer4.1.bn2.weight', 'encoder.layer4.1.bn2.bias', 'encoder.layer4.1.bn2.running_mean', 'encoder.layer4.1.bn2.running_var', 'encoder.layer4.1.bn2.num_batches_tracked', 'encoder.layer4.1.conv3.weight', 'encoder.layer4.1.bn3.weight', 'encoder.layer4.1.bn3.bias', 'encoder.layer4.1.bn3.running_mean', 'encoder.layer4.1.bn3.running_var', 'encoder.layer4.1.bn3.num_batches_tracked', 'encoder.layer4.2.conv1.weight', 'encoder.layer4.2.bn1.weight', 'encoder.layer4.2.bn1.bias', 'encoder.layer4.2.bn1.running_mean', 'encoder.layer4.2.bn1.running_var', 'encoder.layer4.2.bn1.num_batches_tracked', 'encoder.layer4.2.conv2.weight', 'encoder.layer4.2.bn2.weight', 'encoder.layer4.2.bn2.bias', 'encoder.layer4.2.bn2.running_mean', 'encoder.layer4.2.bn2.running_var', 'encoder.layer4.2.bn2.num_batches_tracked', 'encoder.layer4.2.conv3.weight', 'encoder.layer4.2.bn3.weight', 'encoder.layer4.2.bn3.bias', 'encoder.layer4.2.bn3.running_mean', 'encoder.layer4.2.bn3.running_var', 'encoder.layer4.2.bn3.num_batches_tracked', 'projection_head.mlp.0.weight', 'projection_head.mlp.0.bias', 'projection_head.mlp.2.weight', 'projection_head.mlp.2.bias', 'projection_head.mlp.4.weight', 'projection_head.mlp.4.bias', 'projection_head.mlp.6.weight', 'projection_head.mlp.6.bias', 'projection_head.mlp.9.weight', 'projection_head.mlp.9.bias', 'classifier.classifier.0.weight', 'classifier.classifier.0.bias', 'classifier.classifier.2.weight', 'classifier.classifier.2.bias', 'classifier.classifier.4.weight', 'classifier.classifier.4.bias', 'classifier.classifier.6.weight', 'classifier.classifier.6.bias', 'classifier.classifier.8.weight', 'classifier.classifier.8.bias', 'classifier.classifier.10.weight', 'classifier.classifier.10.bias', 'classifier.classifier.12.weight', 'classifier.classifier.12.bias', 'classifier.classifier.14.weight', 'classifier.classifier.14.bias'])\n",
            "✓ It's a model file - loading directly\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/SSCLNet-Implementation/train_supervised.py\", line 223, in <module>\n",
            "    train_supervised()\n",
            "  File \"/content/SSCLNet-Implementation/train_supervised.py\", line 37, in train_supervised\n",
            "    model.load_state_dict(checkpoint, strict=False)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 2624, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for SSCLNet:\n",
            "\tsize mismatch for classifier.classifier.14.weight: copying a param with shape torch.Size([5, 16]) from checkpoint, the shape in current model is torch.Size([2, 16]).\n",
            "\tsize mismatch for classifier.classifier.14.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([2]).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model Evaluation (2-class dataset)"
      ],
      "metadata": {
        "id": "i6o5ua1WvVC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing / model evaluation:\n",
        "!python \"/content/SSCLNet-Implementation/eval.py\""
      ],
      "metadata": {
        "id": "0enTVxdYvUv7",
        "outputId": "412f9c49-87a6-4494-a37e-fd672b2afcce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-01 14:11:59.013368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762006319.044785   49718 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762006319.054319   49718 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762006319.077162   49718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762006319.077197   49718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762006319.077204   49718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762006319.077207   49718 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-01 14:11:59.084034: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✓ All directories created\n",
            "✓ All directories created\n",
            "2025-11-01 14:12:05,587 - model_evaluation - INFO - === Model Evaluation on Test Set ===\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/SSCLNet-Implementation/eval.py\", line 216, in <module>\n",
            "    results = evaluate_model()\n",
            "              ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SSCLNet-Implementation/eval.py\", line 30, in evaluate_model\n",
            "    model.load_state_dict(torch.load(config.SUPERVISED_SAVE_PATH))\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 2624, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for SSCLNet:\n",
            "\tsize mismatch for classifier.classifier.14.weight: copying a param with shape torch.Size([5, 16]) from checkpoint, the shape in current model is torch.Size([2, 16]).\n",
            "\tsize mismatch for classifier.classifier.14.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([2]).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Saving all the results and downloading it"
      ],
      "metadata": {
        "id": "EpoehPFIv-78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving it all for backup\n",
        "import os\n",
        "\n",
        "# Make a new folder in Colab’s /content directory\n",
        "new_folder = \"/content/SSCLNet-implementation-outputs/2-class\"\n",
        "os.makedirs(new_folder, exist_ok=True)\n",
        "\n",
        "print(f\"Created: {new_folder}\")"
      ],
      "metadata": {
        "id": "moy9a0cE2eUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/configs \"/content/SSCLNet-implementation-outputs/2-class\"\n",
        "!cp -r /content/logs \"/content/SSCLNet-implementation-outputs/2-class\"\n",
        "!cp -r /content/models \"/content/SSCLNet-implementation-outputs/2-class\"\n",
        "!cp -r /content/plots \"/content/SSCLNet-implementation-outputs/2-class\"\n",
        "!cp -r /content/results \"/content/SSCLNet-implementation-outputs/2-class\"\n",
        "!cp -r /content/runs \"/content/SSCLNet-implementation-outputs/2-class\"\n",
        "!cp -r /content/training_output \"/content/SSCLNet-implementation-outputs/2-class\"\n",
        "\n",
        "!zip -r SSCLNet_implementation_outputs_2Class.zip \"/content/SSCLNet-implementation-outputs/2-class\""
      ],
      "metadata": {
        "id": "zE1D5Tqq2kLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv SSCLNet_implementation_outputs_5Class.zip \"/content/drive/MyDrive/MS-AI/Project-Datasets/SSCLNet-Implementation/\"\n",
        "!mv SSCLNet_implementation_outputs_2Class.zip \"/content/drive/MyDrive/MS-AI/Project-Datasets/SSCLNet-Implementation/\""
      ],
      "metadata": {
        "id": "8_kG0lVdzM25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}