{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJRl+5IeDCs9JESr55sfc4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tselane2110/SSCLNet-Implementation/blob/main/implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sbIIjUI3Nd8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13520f14-b52f-41fa-ede9-e27fe51f2c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SSCLNet-Implementation'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 238 (delta 52), reused 44 (delta 18), pack-reused 143 (from 1)\u001b[K\n",
            "Receiving objects: 100% (238/238), 207.16 KiB | 1.56 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tselane2110/SSCLNet-Implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/SSCLNet-Implementation"
      ],
      "metadata": {
        "id": "VDBJRkIFBIt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading & Preprocessing"
      ],
      "metadata": {
        "id": "bp8ivRpez212"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dataset"
      ],
      "metadata": {
        "id": "Ks6yLvOFN1Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.load_data(\"https://drive.google.com/file/d/1QI9_a1qjLyKOsj8IOFdRAZVOGs3W51jL/view?usp=drive_link\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "gSSgcKkYBAvF",
        "outputId": "142bf529-f974-42a6-dea8-04bedc75bfb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1QI9_a1qjLyKOsj8IOFdRAZVOGs3W51jL\n",
            "From (redirected): https://drive.google.com/uc?id=1QI9_a1qjLyKOsj8IOFdRAZVOGs3W51jL&confirm=t&uuid=0333eb9c-3b49-48fe-a889-22e3cf4bdc06\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 209M/209M [00:02<00:00, 96.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Dataset ready at: /content/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.split_data_disjoint_pretrain(\"/content/Dataset-Brain-MRI\", \"/content/splitted-dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzkojWZpBffN",
        "outputId": "9f0812fb-b292-4514-c378-984a5481b3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DISJOINT dataset splitting (70% pretrain, 20% train, 10% test)...\n",
            "Input directory: /content/Dataset-Brain-MRI\n",
            "Output directory: /content/splitted-dataset\n",
            "Created: /content/splitted-dataset/pretrain\n",
            "Created: /content/splitted-dataset/train/2-class/yes\n",
            "Created: /content/splitted-dataset/train/2-class/no\n",
            "Created: /content/splitted-dataset/train/5-class/Glioblastoma\n",
            "Created: /content/splitted-dataset/train/5-class/glioma_tumor\n",
            "Created: /content/splitted-dataset/train/5-class/meningioma_tumor\n",
            "Created: /content/splitted-dataset/train/5-class/no_tumor\n",
            "Created: /content/splitted-dataset/train/5-class/pituitary_tumor\n",
            "Created: /content/splitted-dataset/test/2-class/yes\n",
            "Created: /content/splitted-dataset/test/2-class/no\n",
            "Created: /content/splitted-dataset/test/5-class/Glioblastoma\n",
            "Created: /content/splitted-dataset/test/5-class/glioma_tumor\n",
            "Created: /content/splitted-dataset/test/5-class/meningioma_tumor\n",
            "Created: /content/splitted-dataset/test/5-class/no_tumor\n",
            "Created: /content/splitted-dataset/test/5-class/pituitary_tumor\n",
            "\n",
            "Total images collected: 6763\n",
            "\n",
            "Copying files...\n",
            "\n",
            "=== SPLITTING COMPLETED ===\n",
            "Pre-train: 4734 images (70% of total)\n",
            "Train: 1352 images (20% of total)\n",
            "Test: 677 images (10% of total)\n",
            "Total: 6763 images\n",
            "\n",
            "Final structure:\n",
            "/content/splitted-dataset/\n",
            "├── pretrain/          # 70% of ALL data (no labels needed)\n",
            "├── train/\n",
            "│   ├── 2-class/       # 20% of original 2-class data\n",
            "│   │   ├── yes/\n",
            "│   │   └── no/\n",
            "│   └── 5-class/       # 20% of original 5-class data\n",
            "│       ├── Glioblastoma/\n",
            "│       ├── glioma_tumor/\n",
            "│       ├── meningioma_tumor/\n",
            "│       ├── no_tumor/\n",
            "│       └── pituitary_tumor/\n",
            "└── test/\n",
            "    ├── 2-class/       # 10% of original 2-class data\n",
            "    │   ├── yes/\n",
            "    │   └── no/\n",
            "    └── 5-class/       # 10% of original 5-class data\n",
            "        ├── Glioblastoma/\n",
            "        ├── glioma_tumor/\n",
            "        ├── meningioma_tumor/\n",
            "        ├── no_tumor/\n",
            "        └── pituitary_tumor/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.preprocess_split_data(\"/content/splitted-dataset\", \"/content/Preprocessed-splitted-data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FCmgjjJB7rD",
        "outputId": "baefd080-eaee-415c-f64a-37cac89c2e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing split dataset with new structure...\n",
            "Input path: /content/splitted-dataset\n",
            "Output path: /content/Preprocessed-splitted-data\n",
            "\n",
            "Processing pretrain folder...\n",
            "  Pretrain: 4556 images processed\n",
            "\n",
            "Processing train folder...\n",
            "  Train/2-class/yes: 40 images processed\n",
            "  Train/2-class/no: 25 images processed\n",
            "  Train/5-class/Glioblastoma: 194 images processed\n",
            "  Train/5-class/glioma_tumor: 282 images processed\n",
            "  Train/5-class/meningioma_tumor: 330 images processed\n",
            "  Train/5-class/no_tumor: 193 images processed\n",
            "  Train/5-class/pituitary_tumor: 288 images processed\n",
            "\n",
            "Processing test folder...\n",
            "  Test/2-class/yes: 14 images processed\n",
            "  Test/2-class/no: 8 images processed\n",
            "  Test/5-class/Glioblastoma: 90 images processed\n",
            "  Test/5-class/glioma_tumor: 149 images processed\n",
            "  Test/5-class/meningioma_tumor: 163 images processed\n",
            "  Test/5-class/no_tumor: 95 images processed\n",
            "  Test/5-class/pituitary_tumor: 158 images processed\n",
            "\n",
            "=== PREPROCESSING COMPLETED ===\n",
            "Total images successfully processed: 6585\n",
            "Total errors: 0\n",
            "Preprocessed dataset saved to: /content/Preprocessed-splitted-data\n",
            "\n",
            "Final structure:\n",
            "/content/Preprocessed-splitted-data/\n",
            "├── pretrain/          # 70% of ALL data (preprocessed, no labels)\n",
            "├── train/\n",
            "│   ├── 2-class/       # 20% of 2-class data (preprocessed)\n",
            "│   │   ├── yes/\n",
            "│   │   └── no/\n",
            "│   └── 5-class/       # 20% of 5-class data (preprocessed)\n",
            "│       ├── Glioblastoma/\n",
            "│       ├── glioma_tumor/\n",
            "│       ├── meningioma_tumor/\n",
            "│       ├── no_tumor/\n",
            "│       └── pituitary_tumor/\n",
            "└── test/\n",
            "    ├── 2-class/       # 10% of 2-class data (preprocessed)\n",
            "    │   ├── yes/\n",
            "    │   └── no/\n",
            "    └── 5-class/       # 10% of 5-class data (preprocessed)\n",
            "        ├── Glioblastoma/\n",
            "        ├── glioma_tumor/\n",
            "        ├── meningioma_tumor/\n",
            "        ├── no_tumor/\n",
            "        └── pituitary_tumor/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"/content/splitted_data.zip\" \"/content/splitted-dataset\""
      ],
      "metadata": {
        "id": "Vo2o41JLH2k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Downloaded the `splitted_data.zip` file manually and uploaded it to the gdrive folder."
      ],
      "metadata": {
        "id": "KRuhcRMkIxMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"/content/preprocessed_splitted_data.zip\" \"/content/Preprocessed-splitted-data\""
      ],
      "metadata": {
        "id": "kzOnki8CCJnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Downloaded the `preprocessed_splitted_data.zip` file manually and uploaded it to the gdrive folder."
      ],
      "metadata": {
        "id": "J7J_owlgIFz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training Phase 1 - Contrastive Learning"
      ],
      "metadata": {
        "id": "FbOAd7E-0B6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tselane2110/SSCLNet-Implementation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TiXxFBM9ItD",
        "outputId": "f756f281-b45d-4b8d-9c34-71b22bb68063"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SSCLNet-Implementation'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "Receiving objects: 100% (238/238), 207.16 KiB | 1.61 MiB/s, done.\n",
            "remote: Total 238 (delta 52), reused 44 (delta 18), pack-reused 143 (from 1)\u001b[K\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y/view?usp=drive_link\"\n",
        "!unzip -q /content/Preprocessed-splitted-data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj4fJAf413LZ",
        "outputId": "2281606b-5160-4e62-d120-10320b11e911"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y\n",
            "From (redirected): https://drive.google.com/uc?id=16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y&confirm=t&uuid=2e660e9f-2264-442d-93db-fc3a0b08f7f0\n",
            "To: /content/Preprocessed-splitted-data.zip\n",
            "100% 84.7M/84.7M [00:00<00:00, 111MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# updating file paths for pretrain and train data in the config.py file\n",
        "# PRETRAIN_DATA_PATH = \"/content/Preprocessed-splitted-data/pretrain\"\n",
        "# TRAIN_DATA_PATH = \"/content/Preprocessed-splitted-data/train/5-class\"\n",
        "# we will perform the supervised training again on 2-class data"
      ],
      "metadata": {
        "id": "5eYOmyxK2VzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/SSCLNet-Implementation/train_contrastive.py\""
      ],
      "metadata": {
        "id": "nv_YUV2GIui8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cd2d58-afb3-41b2-a3d5-dc448e005478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-30 06:46:31.800789: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761806791.820603     911 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761806791.826357     911 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761806791.840866     911 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761806791.840893     911 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761806791.840897     911 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761806791.840902     911 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-30 06:46:31.847293: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✓ All directories created\n",
            "✓ All directories created\n",
            "✓ Random seed set to: 42\n",
            "2025-10-30 06:46:35,013 - contrastive_pretraining - INFO - === Starting Contrastive Pre-training ===\n",
            "================================================================================\n",
            "MODEL SUMMARY\n",
            "================================================================================\n",
            "Model: SSCLNet\n",
            "Total trainable parameters: 25,669,541\n",
            "Model size: 98.12 MB\n",
            "\n",
            "Layer breakdown:\n",
            "  encoder.conv1.weight: 3,136\n",
            "  encoder.bn1.weight: 64\n",
            "  encoder.bn1.bias: 64\n",
            "  encoder.layer1.0.conv1.weight: 4,096\n",
            "  encoder.layer1.0.bn1.weight: 64\n",
            "  encoder.layer1.0.bn1.bias: 64\n",
            "  encoder.layer1.0.conv2.weight: 36,864\n",
            "  encoder.layer1.0.bn2.weight: 64\n",
            "  encoder.layer1.0.bn2.bias: 64\n",
            "  encoder.layer1.0.conv3.weight: 16,384\n",
            "  encoder.layer1.0.bn3.weight: 256\n",
            "  encoder.layer1.0.bn3.bias: 256\n",
            "  encoder.layer1.0.downsample.0.weight: 16,384\n",
            "  encoder.layer1.0.downsample.1.weight: 256\n",
            "  encoder.layer1.0.downsample.1.bias: 256\n",
            "  encoder.layer1.1.conv1.weight: 16,384\n",
            "  encoder.layer1.1.bn1.weight: 64\n",
            "  encoder.layer1.1.bn1.bias: 64\n",
            "  encoder.layer1.1.conv2.weight: 36,864\n",
            "  encoder.layer1.1.bn2.weight: 64\n",
            "  encoder.layer1.1.bn2.bias: 64\n",
            "  encoder.layer1.1.conv3.weight: 16,384\n",
            "  encoder.layer1.1.bn3.weight: 256\n",
            "  encoder.layer1.1.bn3.bias: 256\n",
            "  encoder.layer1.2.conv1.weight: 16,384\n",
            "  encoder.layer1.2.bn1.weight: 64\n",
            "  encoder.layer1.2.bn1.bias: 64\n",
            "  encoder.layer1.2.conv2.weight: 36,864\n",
            "  encoder.layer1.2.bn2.weight: 64\n",
            "  encoder.layer1.2.bn2.bias: 64\n",
            "  encoder.layer1.2.conv3.weight: 16,384\n",
            "  encoder.layer1.2.bn3.weight: 256\n",
            "  encoder.layer1.2.bn3.bias: 256\n",
            "  encoder.layer2.0.conv1.weight: 32,768\n",
            "  encoder.layer2.0.bn1.weight: 128\n",
            "  encoder.layer2.0.bn1.bias: 128\n",
            "  encoder.layer2.0.conv2.weight: 147,456\n",
            "  encoder.layer2.0.bn2.weight: 128\n",
            "  encoder.layer2.0.bn2.bias: 128\n",
            "  encoder.layer2.0.conv3.weight: 65,536\n",
            "  encoder.layer2.0.bn3.weight: 512\n",
            "  encoder.layer2.0.bn3.bias: 512\n",
            "  encoder.layer2.0.downsample.0.weight: 131,072\n",
            "  encoder.layer2.0.downsample.1.weight: 512\n",
            "  encoder.layer2.0.downsample.1.bias: 512\n",
            "  encoder.layer2.1.conv1.weight: 65,536\n",
            "  encoder.layer2.1.bn1.weight: 128\n",
            "  encoder.layer2.1.bn1.bias: 128\n",
            "  encoder.layer2.1.conv2.weight: 147,456\n",
            "  encoder.layer2.1.bn2.weight: 128\n",
            "  encoder.layer2.1.bn2.bias: 128\n",
            "  encoder.layer2.1.conv3.weight: 65,536\n",
            "  encoder.layer2.1.bn3.weight: 512\n",
            "  encoder.layer2.1.bn3.bias: 512\n",
            "  encoder.layer2.2.conv1.weight: 65,536\n",
            "  encoder.layer2.2.bn1.weight: 128\n",
            "  encoder.layer2.2.bn1.bias: 128\n",
            "  encoder.layer2.2.conv2.weight: 147,456\n",
            "  encoder.layer2.2.bn2.weight: 128\n",
            "  encoder.layer2.2.bn2.bias: 128\n",
            "  encoder.layer2.2.conv3.weight: 65,536\n",
            "  encoder.layer2.2.bn3.weight: 512\n",
            "  encoder.layer2.2.bn3.bias: 512\n",
            "  encoder.layer2.3.conv1.weight: 65,536\n",
            "  encoder.layer2.3.bn1.weight: 128\n",
            "  encoder.layer2.3.bn1.bias: 128\n",
            "  encoder.layer2.3.conv2.weight: 147,456\n",
            "  encoder.layer2.3.bn2.weight: 128\n",
            "  encoder.layer2.3.bn2.bias: 128\n",
            "  encoder.layer2.3.conv3.weight: 65,536\n",
            "  encoder.layer2.3.bn3.weight: 512\n",
            "  encoder.layer2.3.bn3.bias: 512\n",
            "  encoder.layer3.0.conv1.weight: 131,072\n",
            "  encoder.layer3.0.bn1.weight: 256\n",
            "  encoder.layer3.0.bn1.bias: 256\n",
            "  encoder.layer3.0.conv2.weight: 589,824\n",
            "  encoder.layer3.0.bn2.weight: 256\n",
            "  encoder.layer3.0.bn2.bias: 256\n",
            "  encoder.layer3.0.conv3.weight: 262,144\n",
            "  encoder.layer3.0.bn3.weight: 1,024\n",
            "  encoder.layer3.0.bn3.bias: 1,024\n",
            "  encoder.layer3.0.downsample.0.weight: 524,288\n",
            "  encoder.layer3.0.downsample.1.weight: 1,024\n",
            "  encoder.layer3.0.downsample.1.bias: 1,024\n",
            "  encoder.layer3.1.conv1.weight: 262,144\n",
            "  encoder.layer3.1.bn1.weight: 256\n",
            "  encoder.layer3.1.bn1.bias: 256\n",
            "  encoder.layer3.1.conv2.weight: 589,824\n",
            "  encoder.layer3.1.bn2.weight: 256\n",
            "  encoder.layer3.1.bn2.bias: 256\n",
            "  encoder.layer3.1.conv3.weight: 262,144\n",
            "  encoder.layer3.1.bn3.weight: 1,024\n",
            "  encoder.layer3.1.bn3.bias: 1,024\n",
            "  encoder.layer3.2.conv1.weight: 262,144\n",
            "  encoder.layer3.2.bn1.weight: 256\n",
            "  encoder.layer3.2.bn1.bias: 256\n",
            "  encoder.layer3.2.conv2.weight: 589,824\n",
            "  encoder.layer3.2.bn2.weight: 256\n",
            "  encoder.layer3.2.bn2.bias: 256\n",
            "  encoder.layer3.2.conv3.weight: 262,144\n",
            "  encoder.layer3.2.bn3.weight: 1,024\n",
            "  encoder.layer3.2.bn3.bias: 1,024\n",
            "  encoder.layer3.3.conv1.weight: 262,144\n",
            "  encoder.layer3.3.bn1.weight: 256\n",
            "  encoder.layer3.3.bn1.bias: 256\n",
            "  encoder.layer3.3.conv2.weight: 589,824\n",
            "  encoder.layer3.3.bn2.weight: 256\n",
            "  encoder.layer3.3.bn2.bias: 256\n",
            "  encoder.layer3.3.conv3.weight: 262,144\n",
            "  encoder.layer3.3.bn3.weight: 1,024\n",
            "  encoder.layer3.3.bn3.bias: 1,024\n",
            "  encoder.layer3.4.conv1.weight: 262,144\n",
            "  encoder.layer3.4.bn1.weight: 256\n",
            "  encoder.layer3.4.bn1.bias: 256\n",
            "  encoder.layer3.4.conv2.weight: 589,824\n",
            "  encoder.layer3.4.bn2.weight: 256\n",
            "  encoder.layer3.4.bn2.bias: 256\n",
            "  encoder.layer3.4.conv3.weight: 262,144\n",
            "  encoder.layer3.4.bn3.weight: 1,024\n",
            "  encoder.layer3.4.bn3.bias: 1,024\n",
            "  encoder.layer3.5.conv1.weight: 262,144\n",
            "  encoder.layer3.5.bn1.weight: 256\n",
            "  encoder.layer3.5.bn1.bias: 256\n",
            "  encoder.layer3.5.conv2.weight: 589,824\n",
            "  encoder.layer3.5.bn2.weight: 256\n",
            "  encoder.layer3.5.bn2.bias: 256\n",
            "  encoder.layer3.5.conv3.weight: 262,144\n",
            "  encoder.layer3.5.bn3.weight: 1,024\n",
            "  encoder.layer3.5.bn3.bias: 1,024\n",
            "  encoder.layer4.0.conv1.weight: 524,288\n",
            "  encoder.layer4.0.bn1.weight: 512\n",
            "  encoder.layer4.0.bn1.bias: 512\n",
            "  encoder.layer4.0.conv2.weight: 2,359,296\n",
            "  encoder.layer4.0.bn2.weight: 512\n",
            "  encoder.layer4.0.bn2.bias: 512\n",
            "  encoder.layer4.0.conv3.weight: 1,048,576\n",
            "  encoder.layer4.0.bn3.weight: 2,048\n",
            "  encoder.layer4.0.bn3.bias: 2,048\n",
            "  encoder.layer4.0.downsample.0.weight: 2,097,152\n",
            "  encoder.layer4.0.downsample.1.weight: 2,048\n",
            "  encoder.layer4.0.downsample.1.bias: 2,048\n",
            "  encoder.layer4.1.conv1.weight: 1,048,576\n",
            "  encoder.layer4.1.bn1.weight: 512\n",
            "  encoder.layer4.1.bn1.bias: 512\n",
            "  encoder.layer4.1.conv2.weight: 2,359,296\n",
            "  encoder.layer4.1.bn2.weight: 512\n",
            "  encoder.layer4.1.bn2.bias: 512\n",
            "  encoder.layer4.1.conv3.weight: 1,048,576\n",
            "  encoder.layer4.1.bn3.weight: 2,048\n",
            "  encoder.layer4.1.bn3.bias: 2,048\n",
            "  encoder.layer4.2.conv1.weight: 1,048,576\n",
            "  encoder.layer4.2.bn1.weight: 512\n",
            "  encoder.layer4.2.bn1.bias: 512\n",
            "  encoder.layer4.2.conv2.weight: 2,359,296\n",
            "  encoder.layer4.2.bn2.weight: 512\n",
            "  encoder.layer4.2.bn2.bias: 512\n",
            "  encoder.layer4.2.conv3.weight: 1,048,576\n",
            "  encoder.layer4.2.bn3.weight: 2,048\n",
            "  encoder.layer4.2.bn3.bias: 2,048\n",
            "  projection_head.mlp.0.weight: 1,048,576\n",
            "  projection_head.mlp.0.bias: 512\n",
            "  projection_head.mlp.2.weight: 262,144\n",
            "  projection_head.mlp.2.bias: 512\n",
            "  projection_head.mlp.4.weight: 131,072\n",
            "  projection_head.mlp.4.bias: 256\n",
            "  projection_head.mlp.6.weight: 65,536\n",
            "  projection_head.mlp.6.bias: 256\n",
            "  projection_head.mlp.9.weight: 8,192\n",
            "  projection_head.mlp.9.bias: 32\n",
            "  classifier.classifier.0.weight: 524,288\n",
            "  classifier.classifier.0.bias: 256\n",
            "  classifier.classifier.2.weight: 65,536\n",
            "  classifier.classifier.2.bias: 256\n",
            "  classifier.classifier.4.weight: 32,768\n",
            "  classifier.classifier.4.bias: 128\n",
            "  classifier.classifier.6.weight: 16,384\n",
            "  classifier.classifier.6.bias: 128\n",
            "  classifier.classifier.8.weight: 8,192\n",
            "  classifier.classifier.8.bias: 64\n",
            "  classifier.classifier.10.weight: 2,048\n",
            "  classifier.classifier.10.bias: 32\n",
            "  classifier.classifier.12.weight: 512\n",
            "  classifier.classifier.12.bias: 16\n",
            "  classifier.classifier.14.weight: 80\n",
            "  classifier.classifier.14.bias: 5\n",
            "================================================================================\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Created pretrain DataLoader:\n",
            "  - Batch size: 64\n",
            "  - Shuffle: True\n",
            "  - Workers: 4\n",
            "  - Augmentation pool: random_cropping, random_brightness, random_contrast, random_noise\n",
            "  - Number of samples: 4556\n",
            "  - Returns: (augmented_image1, augmented_image2) pairs\n",
            "2025-10-30 06:46:35,602 - contrastive_pretraining - INFO - Pre-training with 4556 unlabeled images\n",
            "Epoch 1/100:   0% 0/72 [00:05<?, ?it/s, Loss=4.8866]2025-10-30 06:46:41,381 - contrastive_pretraining - INFO - Epoch 0 - contrastive_loss/batch: 4.8866\n",
            "2025-10-30 06:46:41,382 - contrastive_pretraining - INFO - Epoch 0 - learning_rate: 0.0003\n",
            "Epoch 1/100:  69% 50/72 [01:10<00:30,  1.38s/it, Loss=2.8974]2025-10-30 06:47:46,300 - contrastive_pretraining - INFO - Epoch 50 - contrastive_loss/batch: 2.8974\n",
            "2025-10-30 06:47:46,300 - contrastive_pretraining - INFO - Epoch 50 - learning_rate: 0.0003\n",
            "Epoch 1/100: 100% 72/72 [01:37<00:00,  1.36s/it, Loss=1.1804]\n",
            "2025-10-30 06:48:13,374 - contrastive_pretraining - INFO - Epoch 1 - Average Loss: 3.4468\n",
            "2025-10-30 06:48:13,374 - contrastive_pretraining - INFO - Epoch 0 - contrastive_loss/epoch: 3.4468\n",
            "Epoch 2/100:   0% 0/72 [00:02<?, ?it/s, Loss=2.9278]2025-10-30 06:48:15,682 - contrastive_pretraining - INFO - Epoch 72 - contrastive_loss/batch: 2.9278\n",
            "2025-10-30 06:48:15,683 - contrastive_pretraining - INFO - Epoch 72 - learning_rate: 0.0003\n",
            "Epoch 2/100:  69% 50/72 [01:08<00:29,  1.33s/it, Loss=2.2762]2025-10-30 06:49:21,963 - contrastive_pretraining - INFO - Epoch 122 - contrastive_loss/batch: 2.2762\n",
            "2025-10-30 06:49:21,963 - contrastive_pretraining - INFO - Epoch 122 - learning_rate: 0.0003\n",
            "Epoch 2/100: 100% 72/72 [01:35<00:00,  1.32s/it, Loss=0.9564]\n",
            "2025-10-30 06:49:48,750 - contrastive_pretraining - INFO - Epoch 2 - Average Loss: 2.3498\n",
            "2025-10-30 06:49:48,750 - contrastive_pretraining - INFO - Epoch 1 - contrastive_loss/epoch: 2.3498\n",
            "Epoch 3/100:   0% 0/72 [00:02<?, ?it/s, Loss=2.2132]2025-10-30 06:49:51,661 - contrastive_pretraining - INFO - Epoch 144 - contrastive_loss/batch: 2.2132\n",
            "2025-10-30 06:49:51,661 - contrastive_pretraining - INFO - Epoch 144 - learning_rate: 0.0003\n",
            "Epoch 3/100:  69% 50/72 [01:09<00:29,  1.33s/it, Loss=1.8801]2025-10-30 06:50:58,041 - contrastive_pretraining - INFO - Epoch 194 - contrastive_loss/batch: 1.8801\n",
            "2025-10-30 06:50:58,041 - contrastive_pretraining - INFO - Epoch 194 - learning_rate: 0.0003\n",
            "Epoch 3/100: 100% 72/72 [01:36<00:00,  1.33s/it, Loss=0.8038]\n",
            "2025-10-30 06:51:24,836 - contrastive_pretraining - INFO - Epoch 3 - Average Loss: 2.0225\n",
            "2025-10-30 06:51:24,837 - contrastive_pretraining - INFO - Epoch 2 - contrastive_loss/epoch: 2.0225\n",
            "Epoch 4/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.7769]2025-10-30 06:51:27,449 - contrastive_pretraining - INFO - Epoch 216 - contrastive_loss/batch: 1.7769\n",
            "2025-10-30 06:51:27,449 - contrastive_pretraining - INFO - Epoch 216 - learning_rate: 0.0003\n",
            "Epoch 4/100:  69% 50/72 [01:08<00:29,  1.33s/it, Loss=1.7426]2025-10-30 06:52:33,821 - contrastive_pretraining - INFO - Epoch 266 - contrastive_loss/batch: 1.7426\n",
            "2025-10-30 06:52:33,822 - contrastive_pretraining - INFO - Epoch 266 - learning_rate: 0.0003\n",
            "Epoch 4/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.3328]\n",
            "2025-10-30 06:53:00,765 - contrastive_pretraining - INFO - Epoch 4 - Average Loss: 1.7351\n",
            "2025-10-30 06:53:00,765 - contrastive_pretraining - INFO - Epoch 3 - contrastive_loss/epoch: 1.7351\n",
            "Epoch 5/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.6840]2025-10-30 06:53:03,070 - contrastive_pretraining - INFO - Epoch 288 - contrastive_loss/batch: 1.6840\n",
            "2025-10-30 06:53:03,070 - contrastive_pretraining - INFO - Epoch 288 - learning_rate: 0.0003\n",
            "Epoch 5/100:  69% 50/72 [01:08<00:29,  1.33s/it, Loss=1.3297]2025-10-30 06:54:09,592 - contrastive_pretraining - INFO - Epoch 338 - contrastive_loss/batch: 1.3297\n",
            "2025-10-30 06:54:09,592 - contrastive_pretraining - INFO - Epoch 338 - learning_rate: 0.0003\n",
            "Epoch 5/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.1964]\n",
            "2025-10-30 06:54:36,465 - contrastive_pretraining - INFO - Epoch 5 - Average Loss: 1.4898\n",
            "2025-10-30 06:54:36,466 - contrastive_pretraining - INFO - Epoch 4 - contrastive_loss/epoch: 1.4898\n",
            "Epoch 6/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.4725]2025-10-30 06:54:39,130 - contrastive_pretraining - INFO - Epoch 360 - contrastive_loss/batch: 1.4725\n",
            "2025-10-30 06:54:39,131 - contrastive_pretraining - INFO - Epoch 360 - learning_rate: 0.0003\n",
            "Epoch 6/100:  69% 50/72 [01:08<00:29,  1.33s/it, Loss=1.1178]2025-10-30 06:55:45,341 - contrastive_pretraining - INFO - Epoch 410 - contrastive_loss/batch: 1.1178\n",
            "2025-10-30 06:55:45,341 - contrastive_pretraining - INFO - Epoch 410 - learning_rate: 0.0003\n",
            "Epoch 6/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.2701]\n",
            "2025-10-30 06:56:12,277 - contrastive_pretraining - INFO - Epoch 6 - Average Loss: 1.2737\n",
            "2025-10-30 06:56:12,278 - contrastive_pretraining - INFO - Epoch 5 - contrastive_loss/epoch: 1.2737\n",
            "Epoch 7/100:   0% 0/72 [00:03<?, ?it/s, Loss=1.3746]2025-10-30 06:56:15,553 - contrastive_pretraining - INFO - Epoch 432 - contrastive_loss/batch: 1.3746\n",
            "2025-10-30 06:56:15,554 - contrastive_pretraining - INFO - Epoch 432 - learning_rate: 0.0003\n",
            "Epoch 7/100:  69% 50/72 [01:09<00:29,  1.32s/it, Loss=1.0711]2025-10-30 06:57:21,921 - contrastive_pretraining - INFO - Epoch 482 - contrastive_loss/batch: 1.0711\n",
            "2025-10-30 06:57:21,922 - contrastive_pretraining - INFO - Epoch 482 - learning_rate: 0.0003\n",
            "Epoch 7/100: 100% 72/72 [01:36<00:00,  1.34s/it, Loss=0.5294]\n",
            "2025-10-30 06:57:48,658 - contrastive_pretraining - INFO - Epoch 7 - Average Loss: 1.1570\n",
            "2025-10-30 06:57:48,658 - contrastive_pretraining - INFO - Epoch 6 - contrastive_loss/epoch: 1.1570\n",
            "Epoch 8/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.1471]2025-10-30 06:57:51,344 - contrastive_pretraining - INFO - Epoch 504 - contrastive_loss/batch: 1.1471\n",
            "2025-10-30 06:57:51,345 - contrastive_pretraining - INFO - Epoch 504 - learning_rate: 0.0003\n",
            "Epoch 8/100:  69% 50/72 [01:08<00:29,  1.33s/it, Loss=1.0571]2025-10-30 06:58:57,657 - contrastive_pretraining - INFO - Epoch 554 - contrastive_loss/batch: 1.0571\n",
            "2025-10-30 06:58:57,657 - contrastive_pretraining - INFO - Epoch 554 - learning_rate: 0.0003\n",
            "Epoch 8/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.1586]\n",
            "2025-10-30 06:59:24,427 - contrastive_pretraining - INFO - Epoch 8 - Average Loss: 1.0589\n",
            "2025-10-30 06:59:24,427 - contrastive_pretraining - INFO - Epoch 7 - contrastive_loss/epoch: 1.0589\n",
            "Epoch 9/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8189]2025-10-30 06:59:27,157 - contrastive_pretraining - INFO - Epoch 576 - contrastive_loss/batch: 0.8189\n",
            "2025-10-30 06:59:27,158 - contrastive_pretraining - INFO - Epoch 576 - learning_rate: 0.0003\n",
            "Epoch 9/100:  69% 50/72 [01:09<00:29,  1.32s/it, Loss=0.9366]2025-10-30 07:00:33,457 - contrastive_pretraining - INFO - Epoch 626 - contrastive_loss/batch: 0.9366\n",
            "2025-10-30 07:00:33,457 - contrastive_pretraining - INFO - Epoch 626 - learning_rate: 0.0003\n",
            "Epoch 9/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.0876]\n",
            "2025-10-30 07:01:00,221 - contrastive_pretraining - INFO - Epoch 9 - Average Loss: 1.0192\n",
            "2025-10-30 07:01:00,221 - contrastive_pretraining - INFO - Epoch 8 - contrastive_loss/epoch: 1.0192\n",
            "Epoch 10/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.1443]2025-10-30 07:01:02,802 - contrastive_pretraining - INFO - Epoch 648 - contrastive_loss/batch: 1.1443\n",
            "2025-10-30 07:01:02,802 - contrastive_pretraining - INFO - Epoch 648 - learning_rate: 0.0003\n",
            "Epoch 10/100:  69% 50/72 [01:08<00:29,  1.33s/it, Loss=1.0864]2025-10-30 07:02:09,101 - contrastive_pretraining - INFO - Epoch 698 - contrastive_loss/batch: 1.0864\n",
            "2025-10-30 07:02:09,101 - contrastive_pretraining - INFO - Epoch 698 - learning_rate: 0.0003\n",
            "Epoch 10/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.1061]\n",
            "2025-10-30 07:02:36,049 - contrastive_pretraining - INFO - Epoch 10 - Average Loss: 1.0023\n",
            "2025-10-30 07:02:36,050 - contrastive_pretraining - INFO - Epoch 9 - contrastive_loss/epoch: 1.0023\n",
            "Epoch 11/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.0991]2025-10-30 07:02:38,537 - contrastive_pretraining - INFO - Epoch 720 - contrastive_loss/batch: 1.0991\n",
            "2025-10-30 07:02:38,537 - contrastive_pretraining - INFO - Epoch 720 - learning_rate: 0.0003\n",
            "Epoch 11/100:  69% 50/72 [01:08<00:29,  1.33s/it, Loss=0.8589]2025-10-30 07:03:44,836 - contrastive_pretraining - INFO - Epoch 770 - contrastive_loss/batch: 0.8589\n",
            "2025-10-30 07:03:44,836 - contrastive_pretraining - INFO - Epoch 770 - learning_rate: 0.0003\n",
            "Epoch 11/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.2186]\n",
            "2025-10-30 07:04:11,671 - contrastive_pretraining - INFO - Epoch 11 - Average Loss: 0.9329\n",
            "2025-10-30 07:04:11,671 - contrastive_pretraining - INFO - Epoch 10 - contrastive_loss/epoch: 0.9329\n",
            "Epoch 12/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.8726]2025-10-30 07:04:14,931 - contrastive_pretraining - INFO - Epoch 792 - contrastive_loss/batch: 0.8726\n",
            "2025-10-30 07:04:14,937 - contrastive_pretraining - INFO - Epoch 792 - learning_rate: 0.0003\n",
            "Epoch 12/100:  69% 50/72 [01:09<00:29,  1.33s/it, Loss=1.0120]2025-10-30 07:05:21,183 - contrastive_pretraining - INFO - Epoch 842 - contrastive_loss/batch: 1.0120\n",
            "2025-10-30 07:05:21,183 - contrastive_pretraining - INFO - Epoch 842 - learning_rate: 0.0003\n",
            "Epoch 12/100: 100% 72/72 [01:36<00:00,  1.34s/it, Loss=0.2110]\n",
            "2025-10-30 07:05:48,034 - contrastive_pretraining - INFO - Epoch 12 - Average Loss: 0.9190\n",
            "2025-10-30 07:05:48,035 - contrastive_pretraining - INFO - Epoch 11 - contrastive_loss/epoch: 0.9190\n",
            "Epoch 13/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8019]2025-10-30 07:05:50,751 - contrastive_pretraining - INFO - Epoch 864 - contrastive_loss/batch: 0.8019\n",
            "2025-10-30 07:05:50,752 - contrastive_pretraining - INFO - Epoch 864 - learning_rate: 0.0003\n",
            "Epoch 13/100:  69% 50/72 [01:08<00:29,  1.33s/it, Loss=0.7987]2025-10-30 07:06:56,817 - contrastive_pretraining - INFO - Epoch 914 - contrastive_loss/batch: 0.7987\n",
            "2025-10-30 07:06:56,818 - contrastive_pretraining - INFO - Epoch 914 - learning_rate: 0.0003\n",
            "Epoch 13/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.1972]\n",
            "2025-10-30 07:07:23,617 - contrastive_pretraining - INFO - Epoch 13 - Average Loss: 0.8633\n",
            "2025-10-30 07:07:23,617 - contrastive_pretraining - INFO - Epoch 12 - contrastive_loss/epoch: 0.8633\n",
            "Epoch 14/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8512]2025-10-30 07:07:26,121 - contrastive_pretraining - INFO - Epoch 936 - contrastive_loss/batch: 0.8512\n",
            "2025-10-30 07:07:26,121 - contrastive_pretraining - INFO - Epoch 936 - learning_rate: 0.0003\n",
            "Epoch 14/100:  69% 50/72 [01:08<00:29,  1.32s/it, Loss=1.0214]2025-10-30 07:08:32,187 - contrastive_pretraining - INFO - Epoch 986 - contrastive_loss/batch: 1.0214\n",
            "2025-10-30 07:08:32,188 - contrastive_pretraining - INFO - Epoch 986 - learning_rate: 0.0003\n",
            "Epoch 14/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.3337]\n",
            "2025-10-30 07:08:59,151 - contrastive_pretraining - INFO - Epoch 14 - Average Loss: 0.8644\n",
            "2025-10-30 07:08:59,152 - contrastive_pretraining - INFO - Epoch 13 - contrastive_loss/epoch: 0.8644\n",
            "Epoch 15/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.9025]2025-10-30 07:09:01,610 - contrastive_pretraining - INFO - Epoch 1008 - contrastive_loss/batch: 0.9025\n",
            "2025-10-30 07:09:01,611 - contrastive_pretraining - INFO - Epoch 1008 - learning_rate: 0.0003\n",
            "Epoch 15/100:  69% 50/72 [01:08<00:29,  1.32s/it, Loss=0.7287]2025-10-30 07:10:07,535 - contrastive_pretraining - INFO - Epoch 1058 - contrastive_loss/batch: 0.7287\n",
            "2025-10-30 07:10:07,536 - contrastive_pretraining - INFO - Epoch 1058 - learning_rate: 0.0003\n",
            "Epoch 15/100: 100% 72/72 [01:35<00:00,  1.32s/it, Loss=0.2268]\n",
            "2025-10-30 07:10:34,265 - contrastive_pretraining - INFO - Epoch 15 - Average Loss: 0.8456\n",
            "2025-10-30 07:10:34,265 - contrastive_pretraining - INFO - Epoch 14 - contrastive_loss/epoch: 0.8456\n",
            "Epoch 16/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.8030]2025-10-30 07:10:37,409 - contrastive_pretraining - INFO - Epoch 1080 - contrastive_loss/batch: 0.8030\n",
            "2025-10-30 07:10:37,411 - contrastive_pretraining - INFO - Epoch 1080 - learning_rate: 0.0003\n",
            "Epoch 16/100:  69% 50/72 [01:09<00:29,  1.32s/it, Loss=0.8620]2025-10-30 07:11:43,607 - contrastive_pretraining - INFO - Epoch 1130 - contrastive_loss/batch: 0.8620\n",
            "2025-10-30 07:11:43,607 - contrastive_pretraining - INFO - Epoch 1130 - learning_rate: 0.0003\n",
            "Epoch 16/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.0318]\n",
            "2025-10-30 07:12:10,251 - contrastive_pretraining - INFO - Epoch 16 - Average Loss: 0.7873\n",
            "2025-10-30 07:12:10,251 - contrastive_pretraining - INFO - Epoch 15 - contrastive_loss/epoch: 0.7873\n",
            "Epoch 17/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.9293]2025-10-30 07:12:12,514 - contrastive_pretraining - INFO - Epoch 1152 - contrastive_loss/batch: 0.9293\n",
            "2025-10-30 07:12:12,514 - contrastive_pretraining - INFO - Epoch 1152 - learning_rate: 0.0003\n",
            "Epoch 17/100:  69% 50/72 [01:08<00:29,  1.32s/it, Loss=0.8753]2025-10-30 07:13:18,789 - contrastive_pretraining - INFO - Epoch 1202 - contrastive_loss/batch: 0.8753\n",
            "2025-10-30 07:13:18,789 - contrastive_pretraining - INFO - Epoch 1202 - learning_rate: 0.0003\n",
            "Epoch 17/100: 100% 72/72 [01:35<00:00,  1.32s/it, Loss=0.2477]\n",
            "2025-10-30 07:13:45,451 - contrastive_pretraining - INFO - Epoch 17 - Average Loss: 0.7936\n",
            "2025-10-30 07:13:45,451 - contrastive_pretraining - INFO - Epoch 16 - contrastive_loss/epoch: 0.7936\n",
            "Epoch 18/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.7937]2025-10-30 07:13:48,191 - contrastive_pretraining - INFO - Epoch 1224 - contrastive_loss/batch: 0.7937\n",
            "2025-10-30 07:13:48,191 - contrastive_pretraining - INFO - Epoch 1224 - learning_rate: 0.0003\n",
            "Epoch 18/100:  69% 50/72 [01:09<00:29,  1.33s/it, Loss=0.7610]2025-10-30 07:14:54,505 - contrastive_pretraining - INFO - Epoch 1274 - contrastive_loss/batch: 0.7610\n",
            "2025-10-30 07:14:54,506 - contrastive_pretraining - INFO - Epoch 1274 - learning_rate: 0.0003\n",
            "Epoch 18/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.1766]\n",
            "2025-10-30 07:15:21,350 - contrastive_pretraining - INFO - Epoch 18 - Average Loss: 0.7676\n",
            "2025-10-30 07:15:21,351 - contrastive_pretraining - INFO - Epoch 17 - contrastive_loss/epoch: 0.7676\n",
            "Epoch 19/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8610]2025-10-30 07:15:23,946 - contrastive_pretraining - INFO - Epoch 1296 - contrastive_loss/batch: 0.8610\n",
            "2025-10-30 07:15:23,948 - contrastive_pretraining - INFO - Epoch 1296 - learning_rate: 0.0003\n",
            "Epoch 19/100:  69% 50/72 [01:08<00:29,  1.33s/it, Loss=0.7411]2025-10-30 07:16:30,150 - contrastive_pretraining - INFO - Epoch 1346 - contrastive_loss/batch: 0.7411\n",
            "2025-10-30 07:16:30,150 - contrastive_pretraining - INFO - Epoch 1346 - learning_rate: 0.0003\n",
            "Epoch 19/100: 100% 72/72 [01:35<00:00,  1.33s/it, Loss=0.1679]\n",
            "2025-10-30 07:16:57,099 - contrastive_pretraining - INFO - Epoch 19 - Average Loss: 0.7537\n",
            "2025-10-30 07:16:57,100 - contrastive_pretraining - INFO - Epoch 18 - contrastive_loss/epoch: 0.7537\n",
            "Epoch 20/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.8875]2025-10-30 07:17:00,296 - contrastive_pretraining - INFO - Epoch 1368 - contrastive_loss/batch: 0.8875\n",
            "2025-10-30 07:17:00,296 - contrastive_pretraining - INFO - Epoch 1368 - learning_rate: 0.0003\n",
            "Epoch 20/100:  69% 50/72 [01:09<00:29,  1.33s/it, Loss=0.8079]2025-10-30 07:18:06,677 - contrastive_pretraining - INFO - Epoch 1418 - contrastive_loss/batch: 0.8079\n",
            "2025-10-30 07:18:06,678 - contrastive_pretraining - INFO - Epoch 1418 - learning_rate: 0.0003\n",
            "Epoch 20/100: 100% 72/72 [01:36<00:00,  1.34s/it, Loss=0.0863]\n",
            "2025-10-30 07:18:33,556 - contrastive_pretraining - INFO - Epoch 20 - Average Loss: 0.7426\n",
            "2025-10-30 07:18:33,557 - contrastive_pretraining - INFO - Epoch 19 - contrastive_loss/epoch: 0.7426\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-10-30 07:18:34,113 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_20.pth\n",
            "Epoch 21/100:   0% 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 21/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.7679]2025-10-30 07:18:36,641 - contrastive_pretraining - INFO - Epoch 1440 - contrastive_loss/batch: 0.7679\n",
            "2025-10-30 07:18:36,647 - contrastive_pretraining - INFO - Epoch 1440 - learning_rate: 0.0003\n",
            "Epoch 21/100:  22% 16/72 [00:22<01:13,  1.32s/it, Loss=0.6366]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the output\n",
        "# !zip -r contrastive_training_output.zip /content/contrastive_training_output"
      ],
      "metadata": {
        "id": "YXy-l8IA38En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training Phase 2 - Supervised Learning"
      ],
      "metadata": {
        "id": "wzbTABHGQ263"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/tselane2110/SSCLNet-Implementation"
      ],
      "metadata": {
        "id": "EVnQlQej2EpC",
        "outputId": "39cf05ef-2a55-41eb-bd43-993707822a74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SSCLNet-Implementation'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 238 (delta 52), reused 44 (delta 18), pack-reused 143 (from 1)\u001b[K\n",
            "Receiving objects: 100% (238/238), 207.16 KiB | 1.64 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "# !gdown --fuzzy \"https://drive.google.com/file/d/16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y/view?usp=drive_link\"\n",
        "# !unzip -q /content/Preprocessed-splitted-data.zip"
      ],
      "metadata": {
        "id": "NU8QtVAo2GYL",
        "outputId": "37e432e9-9eb4-4d71-9d83-76a78fd2997a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y\n",
            "From (redirected): https://drive.google.com/uc?id=16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y&confirm=t&uuid=3dd4e7fb-cb7e-47b8-92c7-9edd0183c755\n",
            "To: /content/Preprocessed-splitted-data.zip\n",
            "100% 84.7M/84.7M [00:00<00:00, 97.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/SSCLNet-Implementation/train_supervised.py\""
      ],
      "metadata": {
        "id": "vYoNIzhXQT-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving it all for backup\n",
        "import os\n",
        "\n",
        "# Make a new folder in Colab’s /content directory\n",
        "new_folder = \"/content/SSCLNet-implementation-outputs\"\n",
        "os.makedirs(new_folder, exist_ok=True)\n",
        "\n",
        "print(f\"Created: {new_folder}\")\n"
      ],
      "metadata": {
        "id": "E8A-xGk59EGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/checkpoints \"/content/SSCLNet-implementation-outputs\"\n",
        "!cp -r /content/configs \"/content/SSCLNet-implementation-outputs\"\n",
        "!cp -r /content/logs \"/content/SSCLNet-implementation-outputs\"\n",
        "!cp -r /content/models \"/content/SSCLNet-implementation-outputs\"\n",
        "!cp -r /content/plots \"/content/SSCLNet-implementation-outputs\"\n",
        "!cp -r /content/results \"/content/SSCLNet-implementation-outputs\"\n",
        "!cp -r /content/runs \"/content/SSCLNet-implementation-outputs\"\n",
        "!cp -r /content/contrastive_training \"/content/SSCLNet-implementation-outputs\"\n",
        "!cp -r /content/supervised_training \"/content/SSCLNet-implementation-outputs\"\n",
        "\n",
        "!zip -r SSCLNet_implementation_outputs.zip \"/content/SSCLNet-implementation-outputs\""
      ],
      "metadata": {
        "id": "UbTOpWny9av5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp SSCLNet_implementation_outputs.zip /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "pW3zUVVX99t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation"
      ],
      "metadata": {
        "id": "eLS_xJ6dRy8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing phase maybe:\n",
        "#python \"/content/SSCLNet-Implementation/eval.py\""
      ],
      "metadata": {
        "id": "c9GFNgJXRHML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}