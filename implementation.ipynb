{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVg4Trq0o38wjuZIq9ggl9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tselane2110/SSCLNet-Implementation/blob/main/module-based-implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sbIIjUI3Nd8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c77bd1fc-4a4d-4cf6-c357-1d66df1b3f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SSCLNet-Implementation'...\n",
            "remote: Enumerating objects: 173, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 173 (delta 15), reused 16 (delta 6), pack-reused 143 (from 1)\u001b[K\n",
            "Receiving objects: 100% (173/173), 171.33 KiB | 8.57 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tselane2110/SSCLNet-Implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/SSCLNet-Implementation"
      ],
      "metadata": {
        "id": "VDBJRkIFBIt0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading & Preprocessing"
      ],
      "metadata": {
        "id": "bp8ivRpez212"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dataset"
      ],
      "metadata": {
        "id": "Ks6yLvOFN1Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.load_data(\"https://drive.google.com/file/d/1QI9_a1qjLyKOsj8IOFdRAZVOGs3W51jL/view?usp=drive_link\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "gSSgcKkYBAvF",
        "outputId": "142bf529-f974-42a6-dea8-04bedc75bfb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1QI9_a1qjLyKOsj8IOFdRAZVOGs3W51jL\n",
            "From (redirected): https://drive.google.com/uc?id=1QI9_a1qjLyKOsj8IOFdRAZVOGs3W51jL&confirm=t&uuid=0333eb9c-3b49-48fe-a889-22e3cf4bdc06\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 209M/209M [00:02<00:00, 96.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Dataset ready at: /content/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.split_data_disjoint_pretrain(\"/content/Dataset-Brain-MRI\", \"/content/splitted-dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzkojWZpBffN",
        "outputId": "9f0812fb-b292-4514-c378-984a5481b3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DISJOINT dataset splitting (70% pretrain, 20% train, 10% test)...\n",
            "Input directory: /content/Dataset-Brain-MRI\n",
            "Output directory: /content/splitted-dataset\n",
            "Created: /content/splitted-dataset/pretrain\n",
            "Created: /content/splitted-dataset/train/2-class/yes\n",
            "Created: /content/splitted-dataset/train/2-class/no\n",
            "Created: /content/splitted-dataset/train/5-class/Glioblastoma\n",
            "Created: /content/splitted-dataset/train/5-class/glioma_tumor\n",
            "Created: /content/splitted-dataset/train/5-class/meningioma_tumor\n",
            "Created: /content/splitted-dataset/train/5-class/no_tumor\n",
            "Created: /content/splitted-dataset/train/5-class/pituitary_tumor\n",
            "Created: /content/splitted-dataset/test/2-class/yes\n",
            "Created: /content/splitted-dataset/test/2-class/no\n",
            "Created: /content/splitted-dataset/test/5-class/Glioblastoma\n",
            "Created: /content/splitted-dataset/test/5-class/glioma_tumor\n",
            "Created: /content/splitted-dataset/test/5-class/meningioma_tumor\n",
            "Created: /content/splitted-dataset/test/5-class/no_tumor\n",
            "Created: /content/splitted-dataset/test/5-class/pituitary_tumor\n",
            "\n",
            "Total images collected: 6763\n",
            "\n",
            "Copying files...\n",
            "\n",
            "=== SPLITTING COMPLETED ===\n",
            "Pre-train: 4734 images (70% of total)\n",
            "Train: 1352 images (20% of total)\n",
            "Test: 677 images (10% of total)\n",
            "Total: 6763 images\n",
            "\n",
            "Final structure:\n",
            "/content/splitted-dataset/\n",
            "├── pretrain/          # 70% of ALL data (no labels needed)\n",
            "├── train/\n",
            "│   ├── 2-class/       # 20% of original 2-class data\n",
            "│   │   ├── yes/\n",
            "│   │   └── no/\n",
            "│   └── 5-class/       # 20% of original 5-class data\n",
            "│       ├── Glioblastoma/\n",
            "│       ├── glioma_tumor/\n",
            "│       ├── meningioma_tumor/\n",
            "│       ├── no_tumor/\n",
            "│       └── pituitary_tumor/\n",
            "└── test/\n",
            "    ├── 2-class/       # 10% of original 2-class data\n",
            "    │   ├── yes/\n",
            "    │   └── no/\n",
            "    └── 5-class/       # 10% of original 5-class data\n",
            "        ├── Glioblastoma/\n",
            "        ├── glioma_tumor/\n",
            "        ├── meningioma_tumor/\n",
            "        ├── no_tumor/\n",
            "        └── pituitary_tumor/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.preprocess_split_data(\"/content/splitted-dataset\", \"/content/Preprocessed-splitted-data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FCmgjjJB7rD",
        "outputId": "baefd080-eaee-415c-f64a-37cac89c2e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing split dataset with new structure...\n",
            "Input path: /content/splitted-dataset\n",
            "Output path: /content/Preprocessed-splitted-data\n",
            "\n",
            "Processing pretrain folder...\n",
            "  Pretrain: 4556 images processed\n",
            "\n",
            "Processing train folder...\n",
            "  Train/2-class/yes: 40 images processed\n",
            "  Train/2-class/no: 25 images processed\n",
            "  Train/5-class/Glioblastoma: 194 images processed\n",
            "  Train/5-class/glioma_tumor: 282 images processed\n",
            "  Train/5-class/meningioma_tumor: 330 images processed\n",
            "  Train/5-class/no_tumor: 193 images processed\n",
            "  Train/5-class/pituitary_tumor: 288 images processed\n",
            "\n",
            "Processing test folder...\n",
            "  Test/2-class/yes: 14 images processed\n",
            "  Test/2-class/no: 8 images processed\n",
            "  Test/5-class/Glioblastoma: 90 images processed\n",
            "  Test/5-class/glioma_tumor: 149 images processed\n",
            "  Test/5-class/meningioma_tumor: 163 images processed\n",
            "  Test/5-class/no_tumor: 95 images processed\n",
            "  Test/5-class/pituitary_tumor: 158 images processed\n",
            "\n",
            "=== PREPROCESSING COMPLETED ===\n",
            "Total images successfully processed: 6585\n",
            "Total errors: 0\n",
            "Preprocessed dataset saved to: /content/Preprocessed-splitted-data\n",
            "\n",
            "Final structure:\n",
            "/content/Preprocessed-splitted-data/\n",
            "├── pretrain/          # 70% of ALL data (preprocessed, no labels)\n",
            "├── train/\n",
            "│   ├── 2-class/       # 20% of 2-class data (preprocessed)\n",
            "│   │   ├── yes/\n",
            "│   │   └── no/\n",
            "│   └── 5-class/       # 20% of 5-class data (preprocessed)\n",
            "│       ├── Glioblastoma/\n",
            "│       ├── glioma_tumor/\n",
            "│       ├── meningioma_tumor/\n",
            "│       ├── no_tumor/\n",
            "│       └── pituitary_tumor/\n",
            "└── test/\n",
            "    ├── 2-class/       # 10% of 2-class data (preprocessed)\n",
            "    │   ├── yes/\n",
            "    │   └── no/\n",
            "    └── 5-class/       # 10% of 5-class data (preprocessed)\n",
            "        ├── Glioblastoma/\n",
            "        ├── glioma_tumor/\n",
            "        ├── meningioma_tumor/\n",
            "        ├── no_tumor/\n",
            "        └── pituitary_tumor/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"/content/splitted_data.zip\" \"/content/splitted-dataset\""
      ],
      "metadata": {
        "id": "Vo2o41JLH2k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Downloaded the `splitted_data.zip` file manually and uploaded it to the gdrive folder."
      ],
      "metadata": {
        "id": "KRuhcRMkIxMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r \"/content/preprocessed_splitted_data.zip\" \"/content/Preprocessed-splitted-data\""
      ],
      "metadata": {
        "id": "kzOnki8CCJnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Downloaded the `preprocessed_splitted_data.zip` file manually and uploaded it to the gdrive folder."
      ],
      "metadata": {
        "id": "J7J_owlgIFz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training Phase 1 - Contrastive Learning"
      ],
      "metadata": {
        "id": "FbOAd7E-0B6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tselane2110/SSCLNet-Implementation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TiXxFBM9ItD",
        "outputId": "d45b7c6f-564c-4f6f-83da-3ae4002556be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SSCLNet-Implementation'...\n",
            "remote: Enumerating objects: 196, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 196 (delta 27), reused 27 (delta 10), pack-reused 143 (from 1)\u001b[K\n",
            "Receiving objects: 100% (196/196), 183.52 KiB | 1.42 MiB/s, done.\n",
            "Resolving deltas: 100% (102/102), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y/view?usp=drive_link\"\n",
        "!unzip -q /content/Preprocessed-splitted-data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj4fJAf413LZ",
        "outputId": "eb57b849-9480-4301-94ec-74a5fdcaaceb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y\n",
            "From (redirected): https://drive.google.com/uc?id=16-v8HBPN_Nnj8JnQSDRSnfq5C1YqD-8y&confirm=t&uuid=4166a99e-8843-475d-b5ba-62ace3a1bdf2\n",
            "To: /content/Preprocessed-splitted-data.zip\n",
            "100% 84.7M/84.7M [00:01<00:00, 57.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# updating file paths for pretrain and train data in the config.py file\n",
        "# PRETRAIN_DATA_PATH = \"/content/Preprocessed-splitted-data/pretrain\"\n",
        "# TRAIN_DATA_PATH = \"/content/Preprocessed-splitted-data/train/5-class\"\n",
        "# we will perform the supervised training again on 2-class data"
      ],
      "metadata": {
        "id": "5eYOmyxK2VzA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/SSCLNet-Implementation/train_contrastive.py\""
      ],
      "metadata": {
        "id": "nv_YUV2GIui8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906478a3-f2ba-4a3f-fa4b-6a4ea5090d38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-28 16:33:53.462979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761669233.502815    1237 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761669233.514144    1237 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761669233.542086    1237 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761669233.542116    1237 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761669233.542124    1237 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761669233.542129    1237 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-28 16:33:53.548998: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✓ All directories created\n",
            "✓ All directories created\n",
            "✓ Random seed set to: 42\n",
            "2025-10-28 16:33:55,930 - contrastive_pretraining - INFO - === Starting Contrastive Pre-training ===\n",
            "================================================================================\n",
            "MODEL SUMMARY\n",
            "================================================================================\n",
            "Model: SSCLNet\n",
            "Total trainable parameters: 25,669,541\n",
            "Model size: 98.12 MB\n",
            "\n",
            "Layer breakdown:\n",
            "  encoder.conv1.weight: 3,136\n",
            "  encoder.bn1.weight: 64\n",
            "  encoder.bn1.bias: 64\n",
            "  encoder.layer1.0.conv1.weight: 4,096\n",
            "  encoder.layer1.0.bn1.weight: 64\n",
            "  encoder.layer1.0.bn1.bias: 64\n",
            "  encoder.layer1.0.conv2.weight: 36,864\n",
            "  encoder.layer1.0.bn2.weight: 64\n",
            "  encoder.layer1.0.bn2.bias: 64\n",
            "  encoder.layer1.0.conv3.weight: 16,384\n",
            "  encoder.layer1.0.bn3.weight: 256\n",
            "  encoder.layer1.0.bn3.bias: 256\n",
            "  encoder.layer1.0.downsample.0.weight: 16,384\n",
            "  encoder.layer1.0.downsample.1.weight: 256\n",
            "  encoder.layer1.0.downsample.1.bias: 256\n",
            "  encoder.layer1.1.conv1.weight: 16,384\n",
            "  encoder.layer1.1.bn1.weight: 64\n",
            "  encoder.layer1.1.bn1.bias: 64\n",
            "  encoder.layer1.1.conv2.weight: 36,864\n",
            "  encoder.layer1.1.bn2.weight: 64\n",
            "  encoder.layer1.1.bn2.bias: 64\n",
            "  encoder.layer1.1.conv3.weight: 16,384\n",
            "  encoder.layer1.1.bn3.weight: 256\n",
            "  encoder.layer1.1.bn3.bias: 256\n",
            "  encoder.layer1.2.conv1.weight: 16,384\n",
            "  encoder.layer1.2.bn1.weight: 64\n",
            "  encoder.layer1.2.bn1.bias: 64\n",
            "  encoder.layer1.2.conv2.weight: 36,864\n",
            "  encoder.layer1.2.bn2.weight: 64\n",
            "  encoder.layer1.2.bn2.bias: 64\n",
            "  encoder.layer1.2.conv3.weight: 16,384\n",
            "  encoder.layer1.2.bn3.weight: 256\n",
            "  encoder.layer1.2.bn3.bias: 256\n",
            "  encoder.layer2.0.conv1.weight: 32,768\n",
            "  encoder.layer2.0.bn1.weight: 128\n",
            "  encoder.layer2.0.bn1.bias: 128\n",
            "  encoder.layer2.0.conv2.weight: 147,456\n",
            "  encoder.layer2.0.bn2.weight: 128\n",
            "  encoder.layer2.0.bn2.bias: 128\n",
            "  encoder.layer2.0.conv3.weight: 65,536\n",
            "  encoder.layer2.0.bn3.weight: 512\n",
            "  encoder.layer2.0.bn3.bias: 512\n",
            "  encoder.layer2.0.downsample.0.weight: 131,072\n",
            "  encoder.layer2.0.downsample.1.weight: 512\n",
            "  encoder.layer2.0.downsample.1.bias: 512\n",
            "  encoder.layer2.1.conv1.weight: 65,536\n",
            "  encoder.layer2.1.bn1.weight: 128\n",
            "  encoder.layer2.1.bn1.bias: 128\n",
            "  encoder.layer2.1.conv2.weight: 147,456\n",
            "  encoder.layer2.1.bn2.weight: 128\n",
            "  encoder.layer2.1.bn2.bias: 128\n",
            "  encoder.layer2.1.conv3.weight: 65,536\n",
            "  encoder.layer2.1.bn3.weight: 512\n",
            "  encoder.layer2.1.bn3.bias: 512\n",
            "  encoder.layer2.2.conv1.weight: 65,536\n",
            "  encoder.layer2.2.bn1.weight: 128\n",
            "  encoder.layer2.2.bn1.bias: 128\n",
            "  encoder.layer2.2.conv2.weight: 147,456\n",
            "  encoder.layer2.2.bn2.weight: 128\n",
            "  encoder.layer2.2.bn2.bias: 128\n",
            "  encoder.layer2.2.conv3.weight: 65,536\n",
            "  encoder.layer2.2.bn3.weight: 512\n",
            "  encoder.layer2.2.bn3.bias: 512\n",
            "  encoder.layer2.3.conv1.weight: 65,536\n",
            "  encoder.layer2.3.bn1.weight: 128\n",
            "  encoder.layer2.3.bn1.bias: 128\n",
            "  encoder.layer2.3.conv2.weight: 147,456\n",
            "  encoder.layer2.3.bn2.weight: 128\n",
            "  encoder.layer2.3.bn2.bias: 128\n",
            "  encoder.layer2.3.conv3.weight: 65,536\n",
            "  encoder.layer2.3.bn3.weight: 512\n",
            "  encoder.layer2.3.bn3.bias: 512\n",
            "  encoder.layer3.0.conv1.weight: 131,072\n",
            "  encoder.layer3.0.bn1.weight: 256\n",
            "  encoder.layer3.0.bn1.bias: 256\n",
            "  encoder.layer3.0.conv2.weight: 589,824\n",
            "  encoder.layer3.0.bn2.weight: 256\n",
            "  encoder.layer3.0.bn2.bias: 256\n",
            "  encoder.layer3.0.conv3.weight: 262,144\n",
            "  encoder.layer3.0.bn3.weight: 1,024\n",
            "  encoder.layer3.0.bn3.bias: 1,024\n",
            "  encoder.layer3.0.downsample.0.weight: 524,288\n",
            "  encoder.layer3.0.downsample.1.weight: 1,024\n",
            "  encoder.layer3.0.downsample.1.bias: 1,024\n",
            "  encoder.layer3.1.conv1.weight: 262,144\n",
            "  encoder.layer3.1.bn1.weight: 256\n",
            "  encoder.layer3.1.bn1.bias: 256\n",
            "  encoder.layer3.1.conv2.weight: 589,824\n",
            "  encoder.layer3.1.bn2.weight: 256\n",
            "  encoder.layer3.1.bn2.bias: 256\n",
            "  encoder.layer3.1.conv3.weight: 262,144\n",
            "  encoder.layer3.1.bn3.weight: 1,024\n",
            "  encoder.layer3.1.bn3.bias: 1,024\n",
            "  encoder.layer3.2.conv1.weight: 262,144\n",
            "  encoder.layer3.2.bn1.weight: 256\n",
            "  encoder.layer3.2.bn1.bias: 256\n",
            "  encoder.layer3.2.conv2.weight: 589,824\n",
            "  encoder.layer3.2.bn2.weight: 256\n",
            "  encoder.layer3.2.bn2.bias: 256\n",
            "  encoder.layer3.2.conv3.weight: 262,144\n",
            "  encoder.layer3.2.bn3.weight: 1,024\n",
            "  encoder.layer3.2.bn3.bias: 1,024\n",
            "  encoder.layer3.3.conv1.weight: 262,144\n",
            "  encoder.layer3.3.bn1.weight: 256\n",
            "  encoder.layer3.3.bn1.bias: 256\n",
            "  encoder.layer3.3.conv2.weight: 589,824\n",
            "  encoder.layer3.3.bn2.weight: 256\n",
            "  encoder.layer3.3.bn2.bias: 256\n",
            "  encoder.layer3.3.conv3.weight: 262,144\n",
            "  encoder.layer3.3.bn3.weight: 1,024\n",
            "  encoder.layer3.3.bn3.bias: 1,024\n",
            "  encoder.layer3.4.conv1.weight: 262,144\n",
            "  encoder.layer3.4.bn1.weight: 256\n",
            "  encoder.layer3.4.bn1.bias: 256\n",
            "  encoder.layer3.4.conv2.weight: 589,824\n",
            "  encoder.layer3.4.bn2.weight: 256\n",
            "  encoder.layer3.4.bn2.bias: 256\n",
            "  encoder.layer3.4.conv3.weight: 262,144\n",
            "  encoder.layer3.4.bn3.weight: 1,024\n",
            "  encoder.layer3.4.bn3.bias: 1,024\n",
            "  encoder.layer3.5.conv1.weight: 262,144\n",
            "  encoder.layer3.5.bn1.weight: 256\n",
            "  encoder.layer3.5.bn1.bias: 256\n",
            "  encoder.layer3.5.conv2.weight: 589,824\n",
            "  encoder.layer3.5.bn2.weight: 256\n",
            "  encoder.layer3.5.bn2.bias: 256\n",
            "  encoder.layer3.5.conv3.weight: 262,144\n",
            "  encoder.layer3.5.bn3.weight: 1,024\n",
            "  encoder.layer3.5.bn3.bias: 1,024\n",
            "  encoder.layer4.0.conv1.weight: 524,288\n",
            "  encoder.layer4.0.bn1.weight: 512\n",
            "  encoder.layer4.0.bn1.bias: 512\n",
            "  encoder.layer4.0.conv2.weight: 2,359,296\n",
            "  encoder.layer4.0.bn2.weight: 512\n",
            "  encoder.layer4.0.bn2.bias: 512\n",
            "  encoder.layer4.0.conv3.weight: 1,048,576\n",
            "  encoder.layer4.0.bn3.weight: 2,048\n",
            "  encoder.layer4.0.bn3.bias: 2,048\n",
            "  encoder.layer4.0.downsample.0.weight: 2,097,152\n",
            "  encoder.layer4.0.downsample.1.weight: 2,048\n",
            "  encoder.layer4.0.downsample.1.bias: 2,048\n",
            "  encoder.layer4.1.conv1.weight: 1,048,576\n",
            "  encoder.layer4.1.bn1.weight: 512\n",
            "  encoder.layer4.1.bn1.bias: 512\n",
            "  encoder.layer4.1.conv2.weight: 2,359,296\n",
            "  encoder.layer4.1.bn2.weight: 512\n",
            "  encoder.layer4.1.bn2.bias: 512\n",
            "  encoder.layer4.1.conv3.weight: 1,048,576\n",
            "  encoder.layer4.1.bn3.weight: 2,048\n",
            "  encoder.layer4.1.bn3.bias: 2,048\n",
            "  encoder.layer4.2.conv1.weight: 1,048,576\n",
            "  encoder.layer4.2.bn1.weight: 512\n",
            "  encoder.layer4.2.bn1.bias: 512\n",
            "  encoder.layer4.2.conv2.weight: 2,359,296\n",
            "  encoder.layer4.2.bn2.weight: 512\n",
            "  encoder.layer4.2.bn2.bias: 512\n",
            "  encoder.layer4.2.conv3.weight: 1,048,576\n",
            "  encoder.layer4.2.bn3.weight: 2,048\n",
            "  encoder.layer4.2.bn3.bias: 2,048\n",
            "  projection_head.mlp.0.weight: 1,048,576\n",
            "  projection_head.mlp.0.bias: 512\n",
            "  projection_head.mlp.2.weight: 262,144\n",
            "  projection_head.mlp.2.bias: 512\n",
            "  projection_head.mlp.4.weight: 131,072\n",
            "  projection_head.mlp.4.bias: 256\n",
            "  projection_head.mlp.6.weight: 65,536\n",
            "  projection_head.mlp.6.bias: 256\n",
            "  projection_head.mlp.9.weight: 8,192\n",
            "  projection_head.mlp.9.bias: 32\n",
            "  classifier.classifier.0.weight: 524,288\n",
            "  classifier.classifier.0.bias: 256\n",
            "  classifier.classifier.2.weight: 65,536\n",
            "  classifier.classifier.2.bias: 256\n",
            "  classifier.classifier.4.weight: 32,768\n",
            "  classifier.classifier.4.bias: 128\n",
            "  classifier.classifier.6.weight: 16,384\n",
            "  classifier.classifier.6.bias: 128\n",
            "  classifier.classifier.8.weight: 8,192\n",
            "  classifier.classifier.8.bias: 64\n",
            "  classifier.classifier.10.weight: 2,048\n",
            "  classifier.classifier.10.bias: 32\n",
            "  classifier.classifier.12.weight: 512\n",
            "  classifier.classifier.12.bias: 16\n",
            "  classifier.classifier.14.weight: 80\n",
            "  classifier.classifier.14.bias: 5\n",
            "================================================================================\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Created pretrain DataLoader:\n",
            "  - Batch size: 64\n",
            "  - Shuffle: True\n",
            "  - Workers: 4\n",
            "  - Augmentation pool: random_cropping, random_brightness, random_contrast, random_noise\n",
            "  - Number of samples: 4556\n",
            "  - Returns: (augmented_image1, augmented_image2) pairs\n",
            "2025-10-28 16:33:56,436 - contrastive_pretraining - INFO - Pre-training with 4556 unlabeled images\n",
            "Epoch 1/100:   0% 0/72 [00:04<?, ?it/s, Loss=4.8746]2025-10-28 16:34:01,122 - contrastive_pretraining - INFO - Epoch 0 - contrastive_loss/batch: 4.8746\n",
            "2025-10-28 16:34:01,123 - contrastive_pretraining - INFO - Epoch 0 - learning_rate: 0.0003\n",
            "Epoch 1/100:  69% 50/72 [01:05<00:28,  1.29s/it, Loss=2.9357]2025-10-28 16:35:02,317 - contrastive_pretraining - INFO - Epoch 50 - contrastive_loss/batch: 2.9357\n",
            "2025-10-28 16:35:02,317 - contrastive_pretraining - INFO - Epoch 50 - learning_rate: 0.0003\n",
            "Epoch 1/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=1.5964]\n",
            "2025-10-28 16:35:29,224 - contrastive_pretraining - INFO - Epoch 1 - Average Loss: 3.5442\n",
            "2025-10-28 16:35:29,224 - contrastive_pretraining - INFO - Epoch 0 - contrastive_loss/epoch: 3.5442\n",
            "Epoch 2/100:   0% 0/72 [00:02<?, ?it/s, Loss=2.8644]2025-10-28 16:35:31,605 - contrastive_pretraining - INFO - Epoch 72 - contrastive_loss/batch: 2.8644\n",
            "2025-10-28 16:35:31,606 - contrastive_pretraining - INFO - Epoch 72 - learning_rate: 0.0003\n",
            "Epoch 2/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=2.2980]2025-10-28 16:36:35,500 - contrastive_pretraining - INFO - Epoch 122 - contrastive_loss/batch: 2.2980\n",
            "2025-10-28 16:36:35,501 - contrastive_pretraining - INFO - Epoch 122 - learning_rate: 0.0003\n",
            "Epoch 2/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.5913]\n",
            "2025-10-28 16:37:01,650 - contrastive_pretraining - INFO - Epoch 2 - Average Loss: 2.3569\n",
            "2025-10-28 16:37:01,651 - contrastive_pretraining - INFO - Epoch 1 - contrastive_loss/epoch: 2.3569\n",
            "Epoch 3/100:   0% 0/72 [00:02<?, ?it/s, Loss=2.1032]2025-10-28 16:37:04,231 - contrastive_pretraining - INFO - Epoch 144 - contrastive_loss/batch: 2.1032\n",
            "2025-10-28 16:37:04,232 - contrastive_pretraining - INFO - Epoch 144 - learning_rate: 0.0003\n",
            "Epoch 3/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=1.8941]2025-10-28 16:38:08,374 - contrastive_pretraining - INFO - Epoch 194 - contrastive_loss/batch: 1.8941\n",
            "2025-10-28 16:38:08,374 - contrastive_pretraining - INFO - Epoch 194 - learning_rate: 0.0003\n",
            "Epoch 3/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.3797]\n",
            "2025-10-28 16:38:34,416 - contrastive_pretraining - INFO - Epoch 3 - Average Loss: 2.0057\n",
            "2025-10-28 16:38:34,416 - contrastive_pretraining - INFO - Epoch 2 - contrastive_loss/epoch: 2.0057\n",
            "Epoch 4/100:   0% 0/72 [00:03<?, ?it/s, Loss=1.8503]2025-10-28 16:38:38,172 - contrastive_pretraining - INFO - Epoch 216 - contrastive_loss/batch: 1.8503\n",
            "2025-10-28 16:38:38,173 - contrastive_pretraining - INFO - Epoch 216 - learning_rate: 0.0003\n",
            "Epoch 4/100:  69% 50/72 [01:08<00:28,  1.28s/it, Loss=1.8369]2025-10-28 16:39:42,855 - contrastive_pretraining - INFO - Epoch 266 - contrastive_loss/batch: 1.8369\n",
            "2025-10-28 16:39:42,856 - contrastive_pretraining - INFO - Epoch 266 - learning_rate: 0.0003\n",
            "Epoch 4/100: 100% 72/72 [01:34<00:00,  1.31s/it, Loss=0.2804]\n",
            "2025-10-28 16:40:08,862 - contrastive_pretraining - INFO - Epoch 4 - Average Loss: 1.7562\n",
            "2025-10-28 16:40:08,862 - contrastive_pretraining - INFO - Epoch 3 - contrastive_loss/epoch: 1.7562\n",
            "Epoch 5/100:   0% 0/72 [00:02<?, ?it/s, Loss=2.0122]2025-10-28 16:40:11,011 - contrastive_pretraining - INFO - Epoch 288 - contrastive_loss/batch: 2.0122\n",
            "2025-10-28 16:40:11,012 - contrastive_pretraining - INFO - Epoch 288 - learning_rate: 0.0003\n",
            "Epoch 5/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=1.7046]2025-10-28 16:41:15,399 - contrastive_pretraining - INFO - Epoch 338 - contrastive_loss/batch: 1.7046\n",
            "2025-10-28 16:41:15,400 - contrastive_pretraining - INFO - Epoch 338 - learning_rate: 0.0003\n",
            "Epoch 5/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.2213]\n",
            "2025-10-28 16:41:41,504 - contrastive_pretraining - INFO - Epoch 5 - Average Loss: 1.6219\n",
            "2025-10-28 16:41:41,505 - contrastive_pretraining - INFO - Epoch 4 - contrastive_loss/epoch: 1.6219\n",
            "Epoch 6/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.4435]2025-10-28 16:41:44,004 - contrastive_pretraining - INFO - Epoch 360 - contrastive_loss/batch: 1.4435\n",
            "2025-10-28 16:41:44,012 - contrastive_pretraining - INFO - Epoch 360 - learning_rate: 0.0003\n",
            "Epoch 6/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=1.4559]2025-10-28 16:42:48,351 - contrastive_pretraining - INFO - Epoch 410 - contrastive_loss/batch: 1.4559\n",
            "2025-10-28 16:42:48,352 - contrastive_pretraining - INFO - Epoch 410 - learning_rate: 0.0003\n",
            "Epoch 6/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.4625]\n",
            "2025-10-28 16:43:14,460 - contrastive_pretraining - INFO - Epoch 6 - Average Loss: 1.4306\n",
            "2025-10-28 16:43:14,460 - contrastive_pretraining - INFO - Epoch 5 - contrastive_loss/epoch: 1.4306\n",
            "Epoch 7/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.4454]2025-10-28 16:43:16,679 - contrastive_pretraining - INFO - Epoch 432 - contrastive_loss/batch: 1.4454\n",
            "2025-10-28 16:43:16,686 - contrastive_pretraining - INFO - Epoch 432 - learning_rate: 0.0003\n",
            "Epoch 7/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=1.3877]2025-10-28 16:44:21,163 - contrastive_pretraining - INFO - Epoch 482 - contrastive_loss/batch: 1.3877\n",
            "2025-10-28 16:44:21,163 - contrastive_pretraining - INFO - Epoch 482 - learning_rate: 0.0003\n",
            "Epoch 7/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1912]\n",
            "2025-10-28 16:44:47,256 - contrastive_pretraining - INFO - Epoch 7 - Average Loss: 1.3290\n",
            "2025-10-28 16:44:47,256 - contrastive_pretraining - INFO - Epoch 6 - contrastive_loss/epoch: 1.3290\n",
            "Epoch 8/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.2787]2025-10-28 16:44:49,606 - contrastive_pretraining - INFO - Epoch 504 - contrastive_loss/batch: 1.2787\n",
            "2025-10-28 16:44:49,614 - contrastive_pretraining - INFO - Epoch 504 - learning_rate: 0.0003\n",
            "Epoch 8/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=1.1512]2025-10-28 16:45:53,883 - contrastive_pretraining - INFO - Epoch 554 - contrastive_loss/batch: 1.1512\n",
            "2025-10-28 16:45:53,883 - contrastive_pretraining - INFO - Epoch 554 - learning_rate: 0.0003\n",
            "Epoch 8/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.2920]\n",
            "2025-10-28 16:46:19,967 - contrastive_pretraining - INFO - Epoch 8 - Average Loss: 1.1540\n",
            "2025-10-28 16:46:19,967 - contrastive_pretraining - INFO - Epoch 7 - contrastive_loss/epoch: 1.1540\n",
            "Epoch 9/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.3550]2025-10-28 16:46:22,394 - contrastive_pretraining - INFO - Epoch 576 - contrastive_loss/batch: 1.3550\n",
            "2025-10-28 16:46:22,396 - contrastive_pretraining - INFO - Epoch 576 - learning_rate: 0.0003\n",
            "Epoch 9/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=1.0022]2025-10-28 16:47:26,785 - contrastive_pretraining - INFO - Epoch 626 - contrastive_loss/batch: 1.0022\n",
            "2025-10-28 16:47:26,785 - contrastive_pretraining - INFO - Epoch 626 - learning_rate: 0.0003\n",
            "Epoch 9/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.3315]\n",
            "2025-10-28 16:47:52,794 - contrastive_pretraining - INFO - Epoch 9 - Average Loss: 1.0890\n",
            "2025-10-28 16:47:52,794 - contrastive_pretraining - INFO - Epoch 8 - contrastive_loss/epoch: 1.0890\n",
            "Epoch 10/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.0091]2025-10-28 16:47:55,383 - contrastive_pretraining - INFO - Epoch 648 - contrastive_loss/batch: 1.0091\n",
            "2025-10-28 16:47:55,384 - contrastive_pretraining - INFO - Epoch 648 - learning_rate: 0.0003\n",
            "Epoch 10/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=1.0386]2025-10-28 16:48:59,665 - contrastive_pretraining - INFO - Epoch 698 - contrastive_loss/batch: 1.0386\n",
            "2025-10-28 16:48:59,666 - contrastive_pretraining - INFO - Epoch 698 - learning_rate: 0.0003\n",
            "Epoch 10/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1862]\n",
            "2025-10-28 16:49:25,785 - contrastive_pretraining - INFO - Epoch 10 - Average Loss: 1.0254\n",
            "2025-10-28 16:49:25,786 - contrastive_pretraining - INFO - Epoch 9 - contrastive_loss/epoch: 1.0254\n",
            "Epoch 11/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.0031]2025-10-28 16:49:28,115 - contrastive_pretraining - INFO - Epoch 720 - contrastive_loss/batch: 1.0031\n",
            "2025-10-28 16:49:28,118 - contrastive_pretraining - INFO - Epoch 720 - learning_rate: 0.0003\n",
            "Epoch 11/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=1.0349]2025-10-28 16:50:32,451 - contrastive_pretraining - INFO - Epoch 770 - contrastive_loss/batch: 1.0349\n",
            "2025-10-28 16:50:32,451 - contrastive_pretraining - INFO - Epoch 770 - learning_rate: 0.0003\n",
            "Epoch 11/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1806]\n",
            "2025-10-28 16:50:58,554 - contrastive_pretraining - INFO - Epoch 11 - Average Loss: 0.9469\n",
            "2025-10-28 16:50:58,554 - contrastive_pretraining - INFO - Epoch 10 - contrastive_loss/epoch: 0.9469\n",
            "Epoch 12/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.9345]2025-10-28 16:51:01,016 - contrastive_pretraining - INFO - Epoch 792 - contrastive_loss/batch: 0.9345\n",
            "2025-10-28 16:51:01,017 - contrastive_pretraining - INFO - Epoch 792 - learning_rate: 0.0003\n",
            "Epoch 12/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.8704]2025-10-28 16:52:05,402 - contrastive_pretraining - INFO - Epoch 842 - contrastive_loss/batch: 0.8704\n",
            "2025-10-28 16:52:05,402 - contrastive_pretraining - INFO - Epoch 842 - learning_rate: 0.0003\n",
            "Epoch 12/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1120]\n",
            "2025-10-28 16:52:31,487 - contrastive_pretraining - INFO - Epoch 12 - Average Loss: 0.9434\n",
            "2025-10-28 16:52:31,487 - contrastive_pretraining - INFO - Epoch 11 - contrastive_loss/epoch: 0.9434\n",
            "Epoch 13/100:   0% 0/72 [00:02<?, ?it/s, Loss=1.0878]2025-10-28 16:52:34,154 - contrastive_pretraining - INFO - Epoch 864 - contrastive_loss/batch: 1.0878\n",
            "2025-10-28 16:52:34,154 - contrastive_pretraining - INFO - Epoch 864 - learning_rate: 0.0003\n",
            "Epoch 13/100:  69% 50/72 [01:07<00:28,  1.28s/it, Loss=0.8374]2025-10-28 16:53:38,503 - contrastive_pretraining - INFO - Epoch 914 - contrastive_loss/batch: 0.8374\n",
            "2025-10-28 16:53:38,504 - contrastive_pretraining - INFO - Epoch 914 - learning_rate: 0.0003\n",
            "Epoch 13/100: 100% 72/72 [01:33<00:00,  1.29s/it, Loss=0.1423]\n",
            "2025-10-28 16:54:04,505 - contrastive_pretraining - INFO - Epoch 13 - Average Loss: 0.8899\n",
            "2025-10-28 16:54:04,505 - contrastive_pretraining - INFO - Epoch 12 - contrastive_loss/epoch: 0.8899\n",
            "Epoch 14/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8472]2025-10-28 16:54:06,953 - contrastive_pretraining - INFO - Epoch 936 - contrastive_loss/batch: 0.8472\n",
            "2025-10-28 16:54:06,953 - contrastive_pretraining - INFO - Epoch 936 - learning_rate: 0.0003\n",
            "Epoch 14/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.8137]2025-10-28 16:55:11,187 - contrastive_pretraining - INFO - Epoch 986 - contrastive_loss/batch: 0.8137\n",
            "2025-10-28 16:55:11,187 - contrastive_pretraining - INFO - Epoch 986 - learning_rate: 0.0003\n",
            "Epoch 14/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.3457]\n",
            "2025-10-28 16:55:37,258 - contrastive_pretraining - INFO - Epoch 14 - Average Loss: 0.8842\n",
            "2025-10-28 16:55:37,258 - contrastive_pretraining - INFO - Epoch 13 - contrastive_loss/epoch: 0.8842\n",
            "Epoch 15/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8856]2025-10-28 16:55:39,897 - contrastive_pretraining - INFO - Epoch 1008 - contrastive_loss/batch: 0.8856\n",
            "2025-10-28 16:55:39,898 - contrastive_pretraining - INFO - Epoch 1008 - learning_rate: 0.0003\n",
            "Epoch 15/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.8042]2025-10-28 16:56:43,940 - contrastive_pretraining - INFO - Epoch 1058 - contrastive_loss/batch: 0.8042\n",
            "2025-10-28 16:56:43,941 - contrastive_pretraining - INFO - Epoch 1058 - learning_rate: 0.0003\n",
            "Epoch 15/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1609]\n",
            "2025-10-28 16:57:09,948 - contrastive_pretraining - INFO - Epoch 15 - Average Loss: 0.8691\n",
            "2025-10-28 16:57:09,949 - contrastive_pretraining - INFO - Epoch 14 - contrastive_loss/epoch: 0.8691\n",
            "Epoch 16/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.9643]2025-10-28 16:57:12,566 - contrastive_pretraining - INFO - Epoch 1080 - contrastive_loss/batch: 0.9643\n",
            "2025-10-28 16:57:12,567 - contrastive_pretraining - INFO - Epoch 1080 - learning_rate: 0.0003\n",
            "Epoch 16/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.7309]2025-10-28 16:58:16,941 - contrastive_pretraining - INFO - Epoch 1130 - contrastive_loss/batch: 0.7309\n",
            "2025-10-28 16:58:16,942 - contrastive_pretraining - INFO - Epoch 1130 - learning_rate: 0.0003\n",
            "Epoch 16/100: 100% 72/72 [01:33<00:00,  1.29s/it, Loss=0.1909]\n",
            "2025-10-28 16:58:42,983 - contrastive_pretraining - INFO - Epoch 16 - Average Loss: 0.8198\n",
            "2025-10-28 16:58:42,983 - contrastive_pretraining - INFO - Epoch 15 - contrastive_loss/epoch: 0.8198\n",
            "Epoch 17/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6999]2025-10-28 16:58:45,229 - contrastive_pretraining - INFO - Epoch 1152 - contrastive_loss/batch: 0.6999\n",
            "2025-10-28 16:58:45,231 - contrastive_pretraining - INFO - Epoch 1152 - learning_rate: 0.0003\n",
            "Epoch 17/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.8792]2025-10-28 16:59:49,207 - contrastive_pretraining - INFO - Epoch 1202 - contrastive_loss/batch: 0.8792\n",
            "2025-10-28 16:59:49,208 - contrastive_pretraining - INFO - Epoch 1202 - learning_rate: 0.0003\n",
            "Epoch 17/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.3715]\n",
            "2025-10-28 17:00:15,197 - contrastive_pretraining - INFO - Epoch 17 - Average Loss: 0.8568\n",
            "2025-10-28 17:00:15,197 - contrastive_pretraining - INFO - Epoch 16 - contrastive_loss/epoch: 0.8568\n",
            "Epoch 18/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6645]2025-10-28 17:00:17,535 - contrastive_pretraining - INFO - Epoch 1224 - contrastive_loss/batch: 0.6645\n",
            "2025-10-28 17:00:17,540 - contrastive_pretraining - INFO - Epoch 1224 - learning_rate: 0.0003\n",
            "Epoch 18/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.7637]2025-10-28 17:01:21,529 - contrastive_pretraining - INFO - Epoch 1274 - contrastive_loss/batch: 0.7637\n",
            "2025-10-28 17:01:21,530 - contrastive_pretraining - INFO - Epoch 1274 - learning_rate: 0.0003\n",
            "Epoch 18/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0628]\n",
            "2025-10-28 17:01:47,548 - contrastive_pretraining - INFO - Epoch 18 - Average Loss: 0.7978\n",
            "2025-10-28 17:01:47,549 - contrastive_pretraining - INFO - Epoch 17 - contrastive_loss/epoch: 0.7978\n",
            "Epoch 19/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8357]2025-10-28 17:01:50,405 - contrastive_pretraining - INFO - Epoch 1296 - contrastive_loss/batch: 0.8357\n",
            "2025-10-28 17:01:50,406 - contrastive_pretraining - INFO - Epoch 1296 - learning_rate: 0.0003\n",
            "Epoch 19/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.8179]2025-10-28 17:02:54,640 - contrastive_pretraining - INFO - Epoch 1346 - contrastive_loss/batch: 0.8179\n",
            "2025-10-28 17:02:54,640 - contrastive_pretraining - INFO - Epoch 1346 - learning_rate: 0.0003\n",
            "Epoch 19/100: 100% 72/72 [01:33<00:00,  1.29s/it, Loss=0.0834]\n",
            "2025-10-28 17:03:20,742 - contrastive_pretraining - INFO - Epoch 19 - Average Loss: 0.7473\n",
            "2025-10-28 17:03:20,743 - contrastive_pretraining - INFO - Epoch 18 - contrastive_loss/epoch: 0.7473\n",
            "Epoch 20/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.7214]2025-10-28 17:03:22,962 - contrastive_pretraining - INFO - Epoch 1368 - contrastive_loss/batch: 0.7214\n",
            "2025-10-28 17:03:22,963 - contrastive_pretraining - INFO - Epoch 1368 - learning_rate: 0.0003\n",
            "Epoch 20/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.6811]2025-10-28 17:04:27,222 - contrastive_pretraining - INFO - Epoch 1418 - contrastive_loss/batch: 0.6811\n",
            "2025-10-28 17:04:27,222 - contrastive_pretraining - INFO - Epoch 1418 - learning_rate: 0.0003\n",
            "Epoch 20/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.3386]\n",
            "2025-10-28 17:04:53,348 - contrastive_pretraining - INFO - Epoch 20 - Average Loss: 0.7551\n",
            "2025-10-28 17:04:53,348 - contrastive_pretraining - INFO - Epoch 19 - contrastive_loss/epoch: 0.7551\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-10-28 17:04:59,470 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_20.pth\n",
            "Epoch 21/100:   0% 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 21/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.7037]2025-10-28 17:05:01,767 - contrastive_pretraining - INFO - Epoch 1440 - contrastive_loss/batch: 0.7037\n",
            "2025-10-28 17:05:01,768 - contrastive_pretraining - INFO - Epoch 1440 - learning_rate: 0.0003\n",
            "Epoch 21/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.8447]2025-10-28 17:06:06,287 - contrastive_pretraining - INFO - Epoch 1490 - contrastive_loss/batch: 0.8447\n",
            "2025-10-28 17:06:06,287 - contrastive_pretraining - INFO - Epoch 1490 - learning_rate: 0.0003\n",
            "Epoch 21/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1926]\n",
            "2025-10-28 17:06:32,227 - contrastive_pretraining - INFO - Epoch 21 - Average Loss: 0.7625\n",
            "2025-10-28 17:06:32,227 - contrastive_pretraining - INFO - Epoch 20 - contrastive_loss/epoch: 0.7625\n",
            "Epoch 22/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.8286]2025-10-28 17:06:35,720 - contrastive_pretraining - INFO - Epoch 1512 - contrastive_loss/batch: 0.8286\n",
            "2025-10-28 17:06:35,721 - contrastive_pretraining - INFO - Epoch 1512 - learning_rate: 0.0003\n",
            "Epoch 22/100:  69% 50/72 [01:07<00:28,  1.28s/it, Loss=0.7431]2025-10-28 17:07:40,068 - contrastive_pretraining - INFO - Epoch 1562 - contrastive_loss/batch: 0.7431\n",
            "2025-10-28 17:07:40,068 - contrastive_pretraining - INFO - Epoch 1562 - learning_rate: 0.0003\n",
            "Epoch 22/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.1418]\n",
            "2025-10-28 17:08:05,985 - contrastive_pretraining - INFO - Epoch 22 - Average Loss: 0.7568\n",
            "2025-10-28 17:08:05,986 - contrastive_pretraining - INFO - Epoch 21 - contrastive_loss/epoch: 0.7568\n",
            "Epoch 23/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6119]2025-10-28 17:08:08,402 - contrastive_pretraining - INFO - Epoch 1584 - contrastive_loss/batch: 0.6119\n",
            "2025-10-28 17:08:08,403 - contrastive_pretraining - INFO - Epoch 1584 - learning_rate: 0.0003\n",
            "Epoch 23/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.7883]2025-10-28 17:09:12,598 - contrastive_pretraining - INFO - Epoch 1634 - contrastive_loss/batch: 0.7883\n",
            "2025-10-28 17:09:12,598 - contrastive_pretraining - INFO - Epoch 1634 - learning_rate: 0.0003\n",
            "Epoch 23/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0349]\n",
            "2025-10-28 17:09:38,788 - contrastive_pretraining - INFO - Epoch 23 - Average Loss: 0.7500\n",
            "2025-10-28 17:09:38,788 - contrastive_pretraining - INFO - Epoch 22 - contrastive_loss/epoch: 0.7500\n",
            "Epoch 24/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.8905]2025-10-28 17:09:42,232 - contrastive_pretraining - INFO - Epoch 1656 - contrastive_loss/batch: 0.8905\n",
            "2025-10-28 17:09:42,232 - contrastive_pretraining - INFO - Epoch 1656 - learning_rate: 0.0003\n",
            "Epoch 24/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.7149]2025-10-28 17:10:46,596 - contrastive_pretraining - INFO - Epoch 1706 - contrastive_loss/batch: 0.7149\n",
            "2025-10-28 17:10:46,597 - contrastive_pretraining - INFO - Epoch 1706 - learning_rate: 0.0003\n",
            "Epoch 24/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.2285]\n",
            "2025-10-28 17:11:12,676 - contrastive_pretraining - INFO - Epoch 24 - Average Loss: 0.7254\n",
            "2025-10-28 17:11:12,676 - contrastive_pretraining - INFO - Epoch 23 - contrastive_loss/epoch: 0.7254\n",
            "Epoch 25/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8078]2025-10-28 17:11:14,941 - contrastive_pretraining - INFO - Epoch 1728 - contrastive_loss/batch: 0.8078\n",
            "2025-10-28 17:11:14,942 - contrastive_pretraining - INFO - Epoch 1728 - learning_rate: 0.0003\n",
            "Epoch 25/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.6973]2025-10-28 17:12:19,162 - contrastive_pretraining - INFO - Epoch 1778 - contrastive_loss/batch: 0.6973\n",
            "2025-10-28 17:12:19,162 - contrastive_pretraining - INFO - Epoch 1778 - learning_rate: 0.0003\n",
            "Epoch 25/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.2847]\n",
            "2025-10-28 17:12:45,180 - contrastive_pretraining - INFO - Epoch 25 - Average Loss: 0.7290\n",
            "2025-10-28 17:12:45,181 - contrastive_pretraining - INFO - Epoch 24 - contrastive_loss/epoch: 0.7290\n",
            "Epoch 26/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.9560]2025-10-28 17:12:48,261 - contrastive_pretraining - INFO - Epoch 1800 - contrastive_loss/batch: 0.9560\n",
            "2025-10-28 17:12:48,261 - contrastive_pretraining - INFO - Epoch 1800 - learning_rate: 0.0003\n",
            "Epoch 26/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.7755]2025-10-28 17:13:52,503 - contrastive_pretraining - INFO - Epoch 1850 - contrastive_loss/batch: 0.7755\n",
            "2025-10-28 17:13:52,503 - contrastive_pretraining - INFO - Epoch 1850 - learning_rate: 0.0003\n",
            "Epoch 26/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0588]\n",
            "2025-10-28 17:14:18,506 - contrastive_pretraining - INFO - Epoch 26 - Average Loss: 0.7297\n",
            "2025-10-28 17:14:18,507 - contrastive_pretraining - INFO - Epoch 25 - contrastive_loss/epoch: 0.7297\n",
            "Epoch 27/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6044]2025-10-28 17:14:21,041 - contrastive_pretraining - INFO - Epoch 1872 - contrastive_loss/batch: 0.6044\n",
            "2025-10-28 17:14:21,042 - contrastive_pretraining - INFO - Epoch 1872 - learning_rate: 0.0003\n",
            "Epoch 27/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.8223]2025-10-28 17:15:25,029 - contrastive_pretraining - INFO - Epoch 1922 - contrastive_loss/batch: 0.8223\n",
            "2025-10-28 17:15:25,029 - contrastive_pretraining - INFO - Epoch 1922 - learning_rate: 0.0003\n",
            "Epoch 27/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.1234]\n",
            "2025-10-28 17:15:50,985 - contrastive_pretraining - INFO - Epoch 27 - Average Loss: 0.6898\n",
            "2025-10-28 17:15:50,985 - contrastive_pretraining - INFO - Epoch 26 - contrastive_loss/epoch: 0.6898\n",
            "Epoch 28/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6799]2025-10-28 17:15:53,611 - contrastive_pretraining - INFO - Epoch 1944 - contrastive_loss/batch: 0.6799\n",
            "2025-10-28 17:15:53,612 - contrastive_pretraining - INFO - Epoch 1944 - learning_rate: 0.0003\n",
            "Epoch 28/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.6582]2025-10-28 17:16:57,826 - contrastive_pretraining - INFO - Epoch 1994 - contrastive_loss/batch: 0.6582\n",
            "2025-10-28 17:16:57,827 - contrastive_pretraining - INFO - Epoch 1994 - learning_rate: 0.0003\n",
            "Epoch 28/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.3980]\n",
            "2025-10-28 17:17:23,762 - contrastive_pretraining - INFO - Epoch 28 - Average Loss: 0.6949\n",
            "2025-10-28 17:17:23,763 - contrastive_pretraining - INFO - Epoch 27 - contrastive_loss/epoch: 0.6949\n",
            "Epoch 29/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.7487]2025-10-28 17:17:26,374 - contrastive_pretraining - INFO - Epoch 2016 - contrastive_loss/batch: 0.7487\n",
            "2025-10-28 17:17:26,375 - contrastive_pretraining - INFO - Epoch 2016 - learning_rate: 0.0003\n",
            "Epoch 29/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.6265]2025-10-28 17:18:30,418 - contrastive_pretraining - INFO - Epoch 2066 - contrastive_loss/batch: 0.6265\n",
            "2025-10-28 17:18:30,419 - contrastive_pretraining - INFO - Epoch 2066 - learning_rate: 0.0003\n",
            "Epoch 29/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0346]\n",
            "2025-10-28 17:18:56,390 - contrastive_pretraining - INFO - Epoch 29 - Average Loss: 0.6861\n",
            "2025-10-28 17:18:56,390 - contrastive_pretraining - INFO - Epoch 28 - contrastive_loss/epoch: 0.6861\n",
            "Epoch 30/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.7322]2025-10-28 17:18:58,543 - contrastive_pretraining - INFO - Epoch 2088 - contrastive_loss/batch: 0.7322\n",
            "2025-10-28 17:18:58,550 - contrastive_pretraining - INFO - Epoch 2088 - learning_rate: 0.0003\n",
            "Epoch 30/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.6292]2025-10-28 17:20:02,803 - contrastive_pretraining - INFO - Epoch 2138 - contrastive_loss/batch: 0.6292\n",
            "2025-10-28 17:20:02,804 - contrastive_pretraining - INFO - Epoch 2138 - learning_rate: 0.0003\n",
            "Epoch 30/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.3668]\n",
            "2025-10-28 17:20:28,754 - contrastive_pretraining - INFO - Epoch 30 - Average Loss: 0.6879\n",
            "2025-10-28 17:20:28,755 - contrastive_pretraining - INFO - Epoch 29 - contrastive_loss/epoch: 0.6879\n",
            "Epoch 31/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.6564]2025-10-28 17:20:31,779 - contrastive_pretraining - INFO - Epoch 2160 - contrastive_loss/batch: 0.6564\n",
            "2025-10-28 17:20:31,780 - contrastive_pretraining - INFO - Epoch 2160 - learning_rate: 0.0003\n",
            "Epoch 31/100:  69% 50/72 [01:07<00:28,  1.28s/it, Loss=0.8191]2025-10-28 17:21:36,052 - contrastive_pretraining - INFO - Epoch 2210 - contrastive_loss/batch: 0.8191\n",
            "2025-10-28 17:21:36,053 - contrastive_pretraining - INFO - Epoch 2210 - learning_rate: 0.0003\n",
            "Epoch 31/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.2354]\n",
            "2025-10-28 17:22:02,051 - contrastive_pretraining - INFO - Epoch 31 - Average Loss: 0.6941\n",
            "2025-10-28 17:22:02,052 - contrastive_pretraining - INFO - Epoch 30 - contrastive_loss/epoch: 0.6941\n",
            "Epoch 32/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.8354]2025-10-28 17:22:04,767 - contrastive_pretraining - INFO - Epoch 2232 - contrastive_loss/batch: 0.8354\n",
            "2025-10-28 17:22:04,770 - contrastive_pretraining - INFO - Epoch 2232 - learning_rate: 0.0003\n",
            "Epoch 32/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.5564]2025-10-28 17:23:09,104 - contrastive_pretraining - INFO - Epoch 2282 - contrastive_loss/batch: 0.5564\n",
            "2025-10-28 17:23:09,105 - contrastive_pretraining - INFO - Epoch 2282 - learning_rate: 0.0003\n",
            "Epoch 32/100: 100% 72/72 [01:33<00:00,  1.29s/it, Loss=0.0140]\n",
            "2025-10-28 17:23:35,204 - contrastive_pretraining - INFO - Epoch 32 - Average Loss: 0.6730\n",
            "2025-10-28 17:23:35,204 - contrastive_pretraining - INFO - Epoch 31 - contrastive_loss/epoch: 0.6730\n",
            "Epoch 33/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.6066]2025-10-28 17:23:38,639 - contrastive_pretraining - INFO - Epoch 2304 - contrastive_loss/batch: 0.6066\n",
            "2025-10-28 17:23:38,640 - contrastive_pretraining - INFO - Epoch 2304 - learning_rate: 0.0003\n",
            "Epoch 33/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.6285]2025-10-28 17:24:42,944 - contrastive_pretraining - INFO - Epoch 2354 - contrastive_loss/batch: 0.6285\n",
            "2025-10-28 17:24:42,944 - contrastive_pretraining - INFO - Epoch 2354 - learning_rate: 0.0003\n",
            "Epoch 33/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0143]\n",
            "2025-10-28 17:25:08,897 - contrastive_pretraining - INFO - Epoch 33 - Average Loss: 0.6454\n",
            "2025-10-28 17:25:08,898 - contrastive_pretraining - INFO - Epoch 32 - contrastive_loss/epoch: 0.6454\n",
            "Epoch 34/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6876]2025-10-28 17:25:11,649 - contrastive_pretraining - INFO - Epoch 2376 - contrastive_loss/batch: 0.6876\n",
            "2025-10-28 17:25:11,650 - contrastive_pretraining - INFO - Epoch 2376 - learning_rate: 0.0003\n",
            "Epoch 34/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.5902]2025-10-28 17:26:15,852 - contrastive_pretraining - INFO - Epoch 2426 - contrastive_loss/batch: 0.5902\n",
            "2025-10-28 17:26:15,852 - contrastive_pretraining - INFO - Epoch 2426 - learning_rate: 0.0003\n",
            "Epoch 34/100: 100% 72/72 [01:33<00:00,  1.29s/it, Loss=0.0400]\n",
            "2025-10-28 17:26:41,905 - contrastive_pretraining - INFO - Epoch 34 - Average Loss: 0.6400\n",
            "2025-10-28 17:26:41,906 - contrastive_pretraining - INFO - Epoch 33 - contrastive_loss/epoch: 0.6400\n",
            "Epoch 35/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.7949]2025-10-28 17:26:45,410 - contrastive_pretraining - INFO - Epoch 2448 - contrastive_loss/batch: 0.7949\n",
            "2025-10-28 17:26:45,411 - contrastive_pretraining - INFO - Epoch 2448 - learning_rate: 0.0003\n",
            "Epoch 35/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.5751]2025-10-28 17:27:49,662 - contrastive_pretraining - INFO - Epoch 2498 - contrastive_loss/batch: 0.5751\n",
            "2025-10-28 17:27:49,663 - contrastive_pretraining - INFO - Epoch 2498 - learning_rate: 0.0003\n",
            "Epoch 35/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0934]\n",
            "2025-10-28 17:28:15,675 - contrastive_pretraining - INFO - Epoch 35 - Average Loss: 0.6222\n",
            "2025-10-28 17:28:15,675 - contrastive_pretraining - INFO - Epoch 34 - contrastive_loss/epoch: 0.6222\n",
            "Epoch 36/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6263]2025-10-28 17:28:18,007 - contrastive_pretraining - INFO - Epoch 2520 - contrastive_loss/batch: 0.6263\n",
            "2025-10-28 17:28:18,010 - contrastive_pretraining - INFO - Epoch 2520 - learning_rate: 0.0003\n",
            "Epoch 36/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.5884]2025-10-28 17:29:22,103 - contrastive_pretraining - INFO - Epoch 2570 - contrastive_loss/batch: 0.5884\n",
            "2025-10-28 17:29:22,103 - contrastive_pretraining - INFO - Epoch 2570 - learning_rate: 0.0003\n",
            "Epoch 36/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0631]\n",
            "2025-10-28 17:29:48,123 - contrastive_pretraining - INFO - Epoch 36 - Average Loss: 0.5895\n",
            "2025-10-28 17:29:48,123 - contrastive_pretraining - INFO - Epoch 35 - contrastive_loss/epoch: 0.5895\n",
            "Epoch 37/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.6270]2025-10-28 17:29:51,030 - contrastive_pretraining - INFO - Epoch 2592 - contrastive_loss/batch: 0.6270\n",
            "2025-10-28 17:29:51,031 - contrastive_pretraining - INFO - Epoch 2592 - learning_rate: 0.0003\n",
            "Epoch 37/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.7297]2025-10-28 17:30:55,327 - contrastive_pretraining - INFO - Epoch 2642 - contrastive_loss/batch: 0.7297\n",
            "2025-10-28 17:30:55,327 - contrastive_pretraining - INFO - Epoch 2642 - learning_rate: 0.0003\n",
            "Epoch 37/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0613]\n",
            "2025-10-28 17:31:21,399 - contrastive_pretraining - INFO - Epoch 37 - Average Loss: 0.6273\n",
            "2025-10-28 17:31:21,399 - contrastive_pretraining - INFO - Epoch 36 - contrastive_loss/epoch: 0.6273\n",
            "Epoch 38/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5982]2025-10-28 17:31:23,683 - contrastive_pretraining - INFO - Epoch 2664 - contrastive_loss/batch: 0.5982\n",
            "2025-10-28 17:31:23,684 - contrastive_pretraining - INFO - Epoch 2664 - learning_rate: 0.0003\n",
            "Epoch 38/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.6551]2025-10-28 17:32:27,784 - contrastive_pretraining - INFO - Epoch 2714 - contrastive_loss/batch: 0.6551\n",
            "2025-10-28 17:32:27,784 - contrastive_pretraining - INFO - Epoch 2714 - learning_rate: 0.0003\n",
            "Epoch 38/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0594]\n",
            "2025-10-28 17:32:53,769 - contrastive_pretraining - INFO - Epoch 38 - Average Loss: 0.5911\n",
            "2025-10-28 17:32:53,770 - contrastive_pretraining - INFO - Epoch 37 - contrastive_loss/epoch: 0.5911\n",
            "Epoch 39/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5966]2025-10-28 17:32:56,040 - contrastive_pretraining - INFO - Epoch 2736 - contrastive_loss/batch: 0.5966\n",
            "2025-10-28 17:32:56,046 - contrastive_pretraining - INFO - Epoch 2736 - learning_rate: 0.0003\n",
            "Epoch 39/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.4748]2025-10-28 17:34:00,234 - contrastive_pretraining - INFO - Epoch 2786 - contrastive_loss/batch: 0.4748\n",
            "2025-10-28 17:34:00,234 - contrastive_pretraining - INFO - Epoch 2786 - learning_rate: 0.0003\n",
            "Epoch 39/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0773]\n",
            "2025-10-28 17:34:26,334 - contrastive_pretraining - INFO - Epoch 39 - Average Loss: 0.5918\n",
            "2025-10-28 17:34:26,334 - contrastive_pretraining - INFO - Epoch 38 - contrastive_loss/epoch: 0.5918\n",
            "Epoch 40/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5925]2025-10-28 17:34:28,808 - contrastive_pretraining - INFO - Epoch 2808 - contrastive_loss/batch: 0.5925\n",
            "2025-10-28 17:34:28,809 - contrastive_pretraining - INFO - Epoch 2808 - learning_rate: 0.0003\n",
            "Epoch 40/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.5717]2025-10-28 17:35:33,190 - contrastive_pretraining - INFO - Epoch 2858 - contrastive_loss/batch: 0.5717\n",
            "2025-10-28 17:35:33,190 - contrastive_pretraining - INFO - Epoch 2858 - learning_rate: 0.0003\n",
            "Epoch 40/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1097]\n",
            "2025-10-28 17:35:59,238 - contrastive_pretraining - INFO - Epoch 40 - Average Loss: 0.5917\n",
            "2025-10-28 17:35:59,238 - contrastive_pretraining - INFO - Epoch 39 - contrastive_loss/epoch: 0.5917\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-10-28 17:36:02,666 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_40.pth\n",
            "Epoch 41/100:   0% 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 41/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4785]2025-10-28 17:36:05,641 - contrastive_pretraining - INFO - Epoch 2880 - contrastive_loss/batch: 0.4785\n",
            "2025-10-28 17:36:05,642 - contrastive_pretraining - INFO - Epoch 2880 - learning_rate: 0.0003\n",
            "Epoch 41/100:  69% 50/72 [01:07<00:28,  1.28s/it, Loss=0.4907]2025-10-28 17:37:09,959 - contrastive_pretraining - INFO - Epoch 2930 - contrastive_loss/batch: 0.4907\n",
            "2025-10-28 17:37:09,960 - contrastive_pretraining - INFO - Epoch 2930 - learning_rate: 0.0003\n",
            "Epoch 41/100: 100% 72/72 [01:33<00:00,  1.29s/it, Loss=0.0638]\n",
            "2025-10-28 17:37:35,853 - contrastive_pretraining - INFO - Epoch 41 - Average Loss: 0.5641\n",
            "2025-10-28 17:37:35,854 - contrastive_pretraining - INFO - Epoch 40 - contrastive_loss/epoch: 0.5641\n",
            "Epoch 42/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5229]2025-10-28 17:37:38,379 - contrastive_pretraining - INFO - Epoch 2952 - contrastive_loss/batch: 0.5229\n",
            "2025-10-28 17:37:38,379 - contrastive_pretraining - INFO - Epoch 2952 - learning_rate: 0.0003\n",
            "Epoch 42/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.4837]2025-10-28 17:38:42,670 - contrastive_pretraining - INFO - Epoch 3002 - contrastive_loss/batch: 0.4837\n",
            "2025-10-28 17:38:42,671 - contrastive_pretraining - INFO - Epoch 3002 - learning_rate: 0.0003\n",
            "Epoch 42/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.2256]\n",
            "2025-10-28 17:39:08,637 - contrastive_pretraining - INFO - Epoch 42 - Average Loss: 0.5570\n",
            "2025-10-28 17:39:08,637 - contrastive_pretraining - INFO - Epoch 41 - contrastive_loss/epoch: 0.5570\n",
            "Epoch 43/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3825]2025-10-28 17:39:11,519 - contrastive_pretraining - INFO - Epoch 3024 - contrastive_loss/batch: 0.3825\n",
            "2025-10-28 17:39:11,522 - contrastive_pretraining - INFO - Epoch 3024 - learning_rate: 0.0003\n",
            "Epoch 43/100:  69% 50/72 [01:07<00:28,  1.28s/it, Loss=0.6383]2025-10-28 17:40:15,706 - contrastive_pretraining - INFO - Epoch 3074 - contrastive_loss/batch: 0.6383\n",
            "2025-10-28 17:40:15,707 - contrastive_pretraining - INFO - Epoch 3074 - learning_rate: 0.0003\n",
            "Epoch 43/100: 100% 72/72 [01:33<00:00,  1.29s/it, Loss=0.0643]\n",
            "2025-10-28 17:40:41,681 - contrastive_pretraining - INFO - Epoch 43 - Average Loss: 0.5666\n",
            "2025-10-28 17:40:41,681 - contrastive_pretraining - INFO - Epoch 42 - contrastive_loss/epoch: 0.5666\n",
            "Epoch 44/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5595]2025-10-28 17:40:44,093 - contrastive_pretraining - INFO - Epoch 3096 - contrastive_loss/batch: 0.5595\n",
            "2025-10-28 17:40:44,096 - contrastive_pretraining - INFO - Epoch 3096 - learning_rate: 0.0003\n",
            "Epoch 44/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.5006]2025-10-28 17:41:48,060 - contrastive_pretraining - INFO - Epoch 3146 - contrastive_loss/batch: 0.5006\n",
            "2025-10-28 17:41:48,061 - contrastive_pretraining - INFO - Epoch 3146 - learning_rate: 0.0003\n",
            "Epoch 44/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0378]\n",
            "2025-10-28 17:42:14,120 - contrastive_pretraining - INFO - Epoch 44 - Average Loss: 0.5431\n",
            "2025-10-28 17:42:14,120 - contrastive_pretraining - INFO - Epoch 43 - contrastive_loss/epoch: 0.5431\n",
            "Epoch 45/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5705]2025-10-28 17:42:17,056 - contrastive_pretraining - INFO - Epoch 3168 - contrastive_loss/batch: 0.5705\n",
            "2025-10-28 17:42:17,062 - contrastive_pretraining - INFO - Epoch 3168 - learning_rate: 0.0003\n",
            "Epoch 45/100:  69% 50/72 [01:07<00:28,  1.28s/it, Loss=0.4833]2025-10-28 17:43:21,314 - contrastive_pretraining - INFO - Epoch 3218 - contrastive_loss/batch: 0.4833\n",
            "2025-10-28 17:43:21,315 - contrastive_pretraining - INFO - Epoch 3218 - learning_rate: 0.0003\n",
            "Epoch 45/100: 100% 72/72 [01:33<00:00,  1.29s/it, Loss=0.1413]\n",
            "2025-10-28 17:43:47,229 - contrastive_pretraining - INFO - Epoch 45 - Average Loss: 0.5234\n",
            "2025-10-28 17:43:47,229 - contrastive_pretraining - INFO - Epoch 44 - contrastive_loss/epoch: 0.5234\n",
            "Epoch 46/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3982]2025-10-28 17:43:49,615 - contrastive_pretraining - INFO - Epoch 3240 - contrastive_loss/batch: 0.3982\n",
            "2025-10-28 17:43:49,616 - contrastive_pretraining - INFO - Epoch 3240 - learning_rate: 0.0003\n",
            "Epoch 46/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.5363]2025-10-28 17:44:53,837 - contrastive_pretraining - INFO - Epoch 3290 - contrastive_loss/batch: 0.5363\n",
            "2025-10-28 17:44:53,837 - contrastive_pretraining - INFO - Epoch 3290 - learning_rate: 0.0003\n",
            "Epoch 46/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1081]\n",
            "2025-10-28 17:45:19,820 - contrastive_pretraining - INFO - Epoch 46 - Average Loss: 0.5105\n",
            "2025-10-28 17:45:19,820 - contrastive_pretraining - INFO - Epoch 45 - contrastive_loss/epoch: 0.5105\n",
            "Epoch 47/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4707]2025-10-28 17:45:22,100 - contrastive_pretraining - INFO - Epoch 3312 - contrastive_loss/batch: 0.4707\n",
            "2025-10-28 17:45:22,101 - contrastive_pretraining - INFO - Epoch 3312 - learning_rate: 0.0003\n",
            "Epoch 47/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.4415]2025-10-28 17:46:26,183 - contrastive_pretraining - INFO - Epoch 3362 - contrastive_loss/batch: 0.4415\n",
            "2025-10-28 17:46:26,183 - contrastive_pretraining - INFO - Epoch 3362 - learning_rate: 0.0003\n",
            "Epoch 47/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0264]\n",
            "2025-10-28 17:46:52,156 - contrastive_pretraining - INFO - Epoch 47 - Average Loss: 0.5196\n",
            "2025-10-28 17:46:52,156 - contrastive_pretraining - INFO - Epoch 46 - contrastive_loss/epoch: 0.5196\n",
            "Epoch 48/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5543]2025-10-28 17:46:54,718 - contrastive_pretraining - INFO - Epoch 3384 - contrastive_loss/batch: 0.5543\n",
            "2025-10-28 17:46:54,719 - contrastive_pretraining - INFO - Epoch 3384 - learning_rate: 0.0003\n",
            "Epoch 48/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.4966]2025-10-28 17:47:58,917 - contrastive_pretraining - INFO - Epoch 3434 - contrastive_loss/batch: 0.4966\n",
            "2025-10-28 17:47:58,918 - contrastive_pretraining - INFO - Epoch 3434 - learning_rate: 0.0003\n",
            "Epoch 48/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1895]\n",
            "2025-10-28 17:48:24,984 - contrastive_pretraining - INFO - Epoch 48 - Average Loss: 0.5199\n",
            "2025-10-28 17:48:24,984 - contrastive_pretraining - INFO - Epoch 47 - contrastive_loss/epoch: 0.5199\n",
            "Epoch 49/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5970]2025-10-28 17:48:27,466 - contrastive_pretraining - INFO - Epoch 3456 - contrastive_loss/batch: 0.5970\n",
            "2025-10-28 17:48:27,467 - contrastive_pretraining - INFO - Epoch 3456 - learning_rate: 0.0003\n",
            "Epoch 49/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.5639]2025-10-28 17:49:31,731 - contrastive_pretraining - INFO - Epoch 3506 - contrastive_loss/batch: 0.5639\n",
            "2025-10-28 17:49:31,731 - contrastive_pretraining - INFO - Epoch 3506 - learning_rate: 0.0003\n",
            "Epoch 49/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.3589]\n",
            "2025-10-28 17:49:57,843 - contrastive_pretraining - INFO - Epoch 49 - Average Loss: 0.5119\n",
            "2025-10-28 17:49:57,843 - contrastive_pretraining - INFO - Epoch 48 - contrastive_loss/epoch: 0.5119\n",
            "Epoch 50/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5307]2025-10-28 17:50:00,459 - contrastive_pretraining - INFO - Epoch 3528 - contrastive_loss/batch: 0.5307\n",
            "2025-10-28 17:50:00,460 - contrastive_pretraining - INFO - Epoch 3528 - learning_rate: 0.0003\n",
            "Epoch 50/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.5143]2025-10-28 17:51:04,757 - contrastive_pretraining - INFO - Epoch 3578 - contrastive_loss/batch: 0.5143\n",
            "2025-10-28 17:51:04,757 - contrastive_pretraining - INFO - Epoch 3578 - learning_rate: 0.0003\n",
            "Epoch 50/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1671]\n",
            "2025-10-28 17:51:30,741 - contrastive_pretraining - INFO - Epoch 50 - Average Loss: 0.5102\n",
            "2025-10-28 17:51:30,741 - contrastive_pretraining - INFO - Epoch 49 - contrastive_loss/epoch: 0.5102\n",
            "Epoch 51/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5369]2025-10-28 17:51:33,239 - contrastive_pretraining - INFO - Epoch 3600 - contrastive_loss/batch: 0.5369\n",
            "2025-10-28 17:51:33,240 - contrastive_pretraining - INFO - Epoch 3600 - learning_rate: 0.0003\n",
            "Epoch 51/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.5092]2025-10-28 17:52:37,188 - contrastive_pretraining - INFO - Epoch 3650 - contrastive_loss/batch: 0.5092\n",
            "2025-10-28 17:52:37,188 - contrastive_pretraining - INFO - Epoch 3650 - learning_rate: 0.0003\n",
            "Epoch 51/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.1396]\n",
            "2025-10-28 17:53:03,188 - contrastive_pretraining - INFO - Epoch 51 - Average Loss: 0.4840\n",
            "2025-10-28 17:53:03,188 - contrastive_pretraining - INFO - Epoch 50 - contrastive_loss/epoch: 0.4840\n",
            "Epoch 52/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.3888]2025-10-28 17:53:06,635 - contrastive_pretraining - INFO - Epoch 3672 - contrastive_loss/batch: 0.3888\n",
            "2025-10-28 17:53:06,636 - contrastive_pretraining - INFO - Epoch 3672 - learning_rate: 0.0003\n",
            "Epoch 52/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.6657]2025-10-28 17:54:10,906 - contrastive_pretraining - INFO - Epoch 3722 - contrastive_loss/batch: 0.6657\n",
            "2025-10-28 17:54:10,907 - contrastive_pretraining - INFO - Epoch 3722 - learning_rate: 0.0003\n",
            "Epoch 52/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0465]\n",
            "2025-10-28 17:54:36,933 - contrastive_pretraining - INFO - Epoch 52 - Average Loss: 0.4710\n",
            "2025-10-28 17:54:36,933 - contrastive_pretraining - INFO - Epoch 51 - contrastive_loss/epoch: 0.4710\n",
            "Epoch 53/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4527]2025-10-28 17:54:39,493 - contrastive_pretraining - INFO - Epoch 3744 - contrastive_loss/batch: 0.4527\n",
            "2025-10-28 17:54:39,493 - contrastive_pretraining - INFO - Epoch 3744 - learning_rate: 0.0003\n",
            "Epoch 53/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.4240]2025-10-28 17:55:43,728 - contrastive_pretraining - INFO - Epoch 3794 - contrastive_loss/batch: 0.4240\n",
            "2025-10-28 17:55:43,729 - contrastive_pretraining - INFO - Epoch 3794 - learning_rate: 0.0003\n",
            "Epoch 53/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0784]\n",
            "2025-10-28 17:56:09,906 - contrastive_pretraining - INFO - Epoch 53 - Average Loss: 0.4830\n",
            "2025-10-28 17:56:09,906 - contrastive_pretraining - INFO - Epoch 52 - contrastive_loss/epoch: 0.4830\n",
            "Epoch 54/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4033]2025-10-28 17:56:13,152 - contrastive_pretraining - INFO - Epoch 3816 - contrastive_loss/batch: 0.4033\n",
            "2025-10-28 17:56:13,153 - contrastive_pretraining - INFO - Epoch 3816 - learning_rate: 0.0003\n",
            "Epoch 54/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.5196]2025-10-28 17:57:17,486 - contrastive_pretraining - INFO - Epoch 3866 - contrastive_loss/batch: 0.5196\n",
            "2025-10-28 17:57:17,486 - contrastive_pretraining - INFO - Epoch 3866 - learning_rate: 0.0003\n",
            "Epoch 54/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0296]\n",
            "2025-10-28 17:57:43,513 - contrastive_pretraining - INFO - Epoch 54 - Average Loss: 0.4732\n",
            "2025-10-28 17:57:43,514 - contrastive_pretraining - INFO - Epoch 53 - contrastive_loss/epoch: 0.4732\n",
            "Epoch 55/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4907]2025-10-28 17:57:45,849 - contrastive_pretraining - INFO - Epoch 3888 - contrastive_loss/batch: 0.4907\n",
            "2025-10-28 17:57:45,849 - contrastive_pretraining - INFO - Epoch 3888 - learning_rate: 0.0003\n",
            "Epoch 55/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.4671]2025-10-28 17:58:50,030 - contrastive_pretraining - INFO - Epoch 3938 - contrastive_loss/batch: 0.4671\n",
            "2025-10-28 17:58:50,031 - contrastive_pretraining - INFO - Epoch 3938 - learning_rate: 0.0003\n",
            "Epoch 55/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0411]\n",
            "2025-10-28 17:59:16,178 - contrastive_pretraining - INFO - Epoch 55 - Average Loss: 0.4546\n",
            "2025-10-28 17:59:16,179 - contrastive_pretraining - INFO - Epoch 54 - contrastive_loss/epoch: 0.4546\n",
            "Epoch 56/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4584]2025-10-28 17:59:19,322 - contrastive_pretraining - INFO - Epoch 3960 - contrastive_loss/batch: 0.4584\n",
            "2025-10-28 17:59:19,323 - contrastive_pretraining - INFO - Epoch 3960 - learning_rate: 0.0003\n",
            "Epoch 56/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.4848]2025-10-28 18:00:23,639 - contrastive_pretraining - INFO - Epoch 4010 - contrastive_loss/batch: 0.4848\n",
            "2025-10-28 18:00:23,640 - contrastive_pretraining - INFO - Epoch 4010 - learning_rate: 0.0003\n",
            "Epoch 56/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0166]\n",
            "2025-10-28 18:00:49,720 - contrastive_pretraining - INFO - Epoch 56 - Average Loss: 0.4643\n",
            "2025-10-28 18:00:49,721 - contrastive_pretraining - INFO - Epoch 55 - contrastive_loss/epoch: 0.4643\n",
            "Epoch 57/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4732]2025-10-28 18:00:52,156 - contrastive_pretraining - INFO - Epoch 4032 - contrastive_loss/batch: 0.4732\n",
            "2025-10-28 18:00:52,156 - contrastive_pretraining - INFO - Epoch 4032 - learning_rate: 0.0003\n",
            "Epoch 57/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.5406]2025-10-28 18:01:56,459 - contrastive_pretraining - INFO - Epoch 4082 - contrastive_loss/batch: 0.5406\n",
            "2025-10-28 18:01:56,460 - contrastive_pretraining - INFO - Epoch 4082 - learning_rate: 0.0003\n",
            "Epoch 57/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0440]\n",
            "2025-10-28 18:02:22,516 - contrastive_pretraining - INFO - Epoch 57 - Average Loss: 0.4709\n",
            "2025-10-28 18:02:22,516 - contrastive_pretraining - INFO - Epoch 56 - contrastive_loss/epoch: 0.4709\n",
            "Epoch 58/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.5313]2025-10-28 18:02:25,953 - contrastive_pretraining - INFO - Epoch 4104 - contrastive_loss/batch: 0.5313\n",
            "2025-10-28 18:02:25,956 - contrastive_pretraining - INFO - Epoch 4104 - learning_rate: 0.0003\n",
            "Epoch 58/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.4543]2025-10-28 18:03:30,280 - contrastive_pretraining - INFO - Epoch 4154 - contrastive_loss/batch: 0.4543\n",
            "2025-10-28 18:03:30,280 - contrastive_pretraining - INFO - Epoch 4154 - learning_rate: 0.0003\n",
            "Epoch 58/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0370]\n",
            "2025-10-28 18:03:56,329 - contrastive_pretraining - INFO - Epoch 58 - Average Loss: 0.4579\n",
            "2025-10-28 18:03:56,330 - contrastive_pretraining - INFO - Epoch 57 - contrastive_loss/epoch: 0.4579\n",
            "Epoch 59/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4041]2025-10-28 18:03:58,845 - contrastive_pretraining - INFO - Epoch 4176 - contrastive_loss/batch: 0.4041\n",
            "2025-10-28 18:03:58,846 - contrastive_pretraining - INFO - Epoch 4176 - learning_rate: 0.0003\n",
            "Epoch 59/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.4754]2025-10-28 18:05:03,035 - contrastive_pretraining - INFO - Epoch 4226 - contrastive_loss/batch: 0.4754\n",
            "2025-10-28 18:05:03,036 - contrastive_pretraining - INFO - Epoch 4226 - learning_rate: 0.0003\n",
            "Epoch 59/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0385]\n",
            "2025-10-28 18:05:29,133 - contrastive_pretraining - INFO - Epoch 59 - Average Loss: 0.4652\n",
            "2025-10-28 18:05:29,133 - contrastive_pretraining - INFO - Epoch 58 - contrastive_loss/epoch: 0.4652\n",
            "Epoch 60/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4233]2025-10-28 18:05:32,648 - contrastive_pretraining - INFO - Epoch 4248 - contrastive_loss/batch: 0.4233\n",
            "2025-10-28 18:05:32,648 - contrastive_pretraining - INFO - Epoch 4248 - learning_rate: 0.0003\n",
            "Epoch 60/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.5538]2025-10-28 18:06:36,970 - contrastive_pretraining - INFO - Epoch 4298 - contrastive_loss/batch: 0.5538\n",
            "2025-10-28 18:06:36,971 - contrastive_pretraining - INFO - Epoch 4298 - learning_rate: 0.0003\n",
            "Epoch 60/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0146]\n",
            "2025-10-28 18:07:03,007 - contrastive_pretraining - INFO - Epoch 60 - Average Loss: 0.4566\n",
            "2025-10-28 18:07:03,008 - contrastive_pretraining - INFO - Epoch 59 - contrastive_loss/epoch: 0.4566\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-10-28 18:07:05,030 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_60.pth\n",
            "Epoch 61/100:   0% 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 61/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4295]2025-10-28 18:07:07,741 - contrastive_pretraining - INFO - Epoch 4320 - contrastive_loss/batch: 0.4295\n",
            "2025-10-28 18:07:07,742 - contrastive_pretraining - INFO - Epoch 4320 - learning_rate: 0.0003\n",
            "Epoch 61/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.3405]2025-10-28 18:08:11,991 - contrastive_pretraining - INFO - Epoch 4370 - contrastive_loss/batch: 0.3405\n",
            "2025-10-28 18:08:11,992 - contrastive_pretraining - INFO - Epoch 4370 - learning_rate: 0.0003\n",
            "Epoch 61/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.2622]\n",
            "2025-10-28 18:08:37,872 - contrastive_pretraining - INFO - Epoch 61 - Average Loss: 0.4457\n",
            "2025-10-28 18:08:37,873 - contrastive_pretraining - INFO - Epoch 60 - contrastive_loss/epoch: 0.4457\n",
            "Epoch 62/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3973]2025-10-28 18:08:40,439 - contrastive_pretraining - INFO - Epoch 4392 - contrastive_loss/batch: 0.3973\n",
            "2025-10-28 18:08:40,439 - contrastive_pretraining - INFO - Epoch 4392 - learning_rate: 0.0003\n",
            "Epoch 62/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.5841]2025-10-28 18:09:44,687 - contrastive_pretraining - INFO - Epoch 4442 - contrastive_loss/batch: 0.5841\n",
            "2025-10-28 18:09:44,687 - contrastive_pretraining - INFO - Epoch 4442 - learning_rate: 0.0003\n",
            "Epoch 62/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0432]\n",
            "2025-10-28 18:10:10,659 - contrastive_pretraining - INFO - Epoch 62 - Average Loss: 0.4606\n",
            "2025-10-28 18:10:10,659 - contrastive_pretraining - INFO - Epoch 61 - contrastive_loss/epoch: 0.4606\n",
            "Epoch 63/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4543]2025-10-28 18:10:13,316 - contrastive_pretraining - INFO - Epoch 4464 - contrastive_loss/batch: 0.4543\n",
            "2025-10-28 18:10:13,317 - contrastive_pretraining - INFO - Epoch 4464 - learning_rate: 0.0003\n",
            "Epoch 63/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3616]2025-10-28 18:11:17,443 - contrastive_pretraining - INFO - Epoch 4514 - contrastive_loss/batch: 0.3616\n",
            "2025-10-28 18:11:17,444 - contrastive_pretraining - INFO - Epoch 4514 - learning_rate: 0.0003\n",
            "Epoch 63/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0482]\n",
            "2025-10-28 18:11:43,586 - contrastive_pretraining - INFO - Epoch 63 - Average Loss: 0.4427\n",
            "2025-10-28 18:11:43,586 - contrastive_pretraining - INFO - Epoch 62 - contrastive_loss/epoch: 0.4427\n",
            "Epoch 64/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3806]2025-10-28 18:11:46,008 - contrastive_pretraining - INFO - Epoch 4536 - contrastive_loss/batch: 0.3806\n",
            "2025-10-28 18:11:46,010 - contrastive_pretraining - INFO - Epoch 4536 - learning_rate: 0.0003\n",
            "Epoch 64/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3808]2025-10-28 18:12:50,316 - contrastive_pretraining - INFO - Epoch 4586 - contrastive_loss/batch: 0.3808\n",
            "2025-10-28 18:12:50,317 - contrastive_pretraining - INFO - Epoch 4586 - learning_rate: 0.0003\n",
            "Epoch 64/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1179]\n",
            "2025-10-28 18:13:16,340 - contrastive_pretraining - INFO - Epoch 64 - Average Loss: 0.4412\n",
            "2025-10-28 18:13:16,341 - contrastive_pretraining - INFO - Epoch 63 - contrastive_loss/epoch: 0.4412\n",
            "Epoch 65/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4686]2025-10-28 18:13:18,796 - contrastive_pretraining - INFO - Epoch 4608 - contrastive_loss/batch: 0.4686\n",
            "2025-10-28 18:13:18,797 - contrastive_pretraining - INFO - Epoch 4608 - learning_rate: 0.0003\n",
            "Epoch 65/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.4749]2025-10-28 18:14:23,008 - contrastive_pretraining - INFO - Epoch 4658 - contrastive_loss/batch: 0.4749\n",
            "2025-10-28 18:14:23,009 - contrastive_pretraining - INFO - Epoch 4658 - learning_rate: 0.0003\n",
            "Epoch 65/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1640]\n",
            "2025-10-28 18:14:49,122 - contrastive_pretraining - INFO - Epoch 65 - Average Loss: 0.4419\n",
            "2025-10-28 18:14:49,122 - contrastive_pretraining - INFO - Epoch 64 - contrastive_loss/epoch: 0.4419\n",
            "Epoch 66/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4446]2025-10-28 18:14:51,678 - contrastive_pretraining - INFO - Epoch 4680 - contrastive_loss/batch: 0.4446\n",
            "2025-10-28 18:14:51,682 - contrastive_pretraining - INFO - Epoch 4680 - learning_rate: 0.0003\n",
            "Epoch 66/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.5016]2025-10-28 18:15:56,059 - contrastive_pretraining - INFO - Epoch 4730 - contrastive_loss/batch: 0.5016\n",
            "2025-10-28 18:15:56,060 - contrastive_pretraining - INFO - Epoch 4730 - learning_rate: 0.0003\n",
            "Epoch 66/100: 100% 72/72 [01:33<00:00,  1.29s/it, Loss=0.0312]\n",
            "2025-10-28 18:16:22,134 - contrastive_pretraining - INFO - Epoch 66 - Average Loss: 0.4240\n",
            "2025-10-28 18:16:22,134 - contrastive_pretraining - INFO - Epoch 65 - contrastive_loss/epoch: 0.4240\n",
            "Epoch 67/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3972]2025-10-28 18:16:24,727 - contrastive_pretraining - INFO - Epoch 4752 - contrastive_loss/batch: 0.3972\n",
            "2025-10-28 18:16:24,728 - contrastive_pretraining - INFO - Epoch 4752 - learning_rate: 0.0003\n",
            "Epoch 67/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.4914]2025-10-28 18:17:28,999 - contrastive_pretraining - INFO - Epoch 4802 - contrastive_loss/batch: 0.4914\n",
            "2025-10-28 18:17:28,999 - contrastive_pretraining - INFO - Epoch 4802 - learning_rate: 0.0003\n",
            "Epoch 67/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0278]\n",
            "2025-10-28 18:17:55,045 - contrastive_pretraining - INFO - Epoch 67 - Average Loss: 0.4168\n",
            "2025-10-28 18:17:55,046 - contrastive_pretraining - INFO - Epoch 66 - contrastive_loss/epoch: 0.4168\n",
            "Epoch 68/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5086]2025-10-28 18:17:57,540 - contrastive_pretraining - INFO - Epoch 4824 - contrastive_loss/batch: 0.5086\n",
            "2025-10-28 18:17:57,542 - contrastive_pretraining - INFO - Epoch 4824 - learning_rate: 0.0003\n",
            "Epoch 68/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.4868]2025-10-28 18:19:01,779 - contrastive_pretraining - INFO - Epoch 4874 - contrastive_loss/batch: 0.4868\n",
            "2025-10-28 18:19:01,779 - contrastive_pretraining - INFO - Epoch 4874 - learning_rate: 0.0003\n",
            "Epoch 68/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.2053]\n",
            "2025-10-28 18:19:27,847 - contrastive_pretraining - INFO - Epoch 68 - Average Loss: 0.4354\n",
            "2025-10-28 18:19:27,847 - contrastive_pretraining - INFO - Epoch 67 - contrastive_loss/epoch: 0.4354\n",
            "Epoch 69/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4973]2025-10-28 18:19:30,012 - contrastive_pretraining - INFO - Epoch 4896 - contrastive_loss/batch: 0.4973\n",
            "2025-10-28 18:19:30,013 - contrastive_pretraining - INFO - Epoch 4896 - learning_rate: 0.0003\n",
            "Epoch 69/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.4057]2025-10-28 18:20:34,347 - contrastive_pretraining - INFO - Epoch 4946 - contrastive_loss/batch: 0.4057\n",
            "2025-10-28 18:20:34,348 - contrastive_pretraining - INFO - Epoch 4946 - learning_rate: 0.0003\n",
            "Epoch 69/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0126]\n",
            "2025-10-28 18:21:00,369 - contrastive_pretraining - INFO - Epoch 69 - Average Loss: 0.4241\n",
            "2025-10-28 18:21:00,370 - contrastive_pretraining - INFO - Epoch 68 - contrastive_loss/epoch: 0.4241\n",
            "Epoch 70/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3958]2025-10-28 18:21:02,600 - contrastive_pretraining - INFO - Epoch 4968 - contrastive_loss/batch: 0.3958\n",
            "2025-10-28 18:21:02,601 - contrastive_pretraining - INFO - Epoch 4968 - learning_rate: 0.0003\n",
            "Epoch 70/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.4738]2025-10-28 18:22:06,646 - contrastive_pretraining - INFO - Epoch 5018 - contrastive_loss/batch: 0.4738\n",
            "2025-10-28 18:22:06,647 - contrastive_pretraining - INFO - Epoch 5018 - learning_rate: 0.0003\n",
            "Epoch 70/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0391]\n",
            "2025-10-28 18:22:32,663 - contrastive_pretraining - INFO - Epoch 70 - Average Loss: 0.4247\n",
            "2025-10-28 18:22:32,663 - contrastive_pretraining - INFO - Epoch 69 - contrastive_loss/epoch: 0.4247\n",
            "Epoch 71/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4078]2025-10-28 18:22:35,777 - contrastive_pretraining - INFO - Epoch 5040 - contrastive_loss/batch: 0.4078\n",
            "2025-10-28 18:22:35,777 - contrastive_pretraining - INFO - Epoch 5040 - learning_rate: 0.0003\n",
            "Epoch 71/100:  69% 50/72 [01:07<00:28,  1.28s/it, Loss=0.4682]2025-10-28 18:23:40,006 - contrastive_pretraining - INFO - Epoch 5090 - contrastive_loss/batch: 0.4682\n",
            "2025-10-28 18:23:40,007 - contrastive_pretraining - INFO - Epoch 5090 - learning_rate: 0.0003\n",
            "Epoch 71/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0685]\n",
            "2025-10-28 18:24:05,983 - contrastive_pretraining - INFO - Epoch 71 - Average Loss: 0.4244\n",
            "2025-10-28 18:24:05,984 - contrastive_pretraining - INFO - Epoch 70 - contrastive_loss/epoch: 0.4244\n",
            "Epoch 72/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4512]2025-10-28 18:24:08,497 - contrastive_pretraining - INFO - Epoch 5112 - contrastive_loss/batch: 0.4512\n",
            "2025-10-28 18:24:08,497 - contrastive_pretraining - INFO - Epoch 5112 - learning_rate: 0.0003\n",
            "Epoch 72/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.4992]2025-10-28 18:25:12,631 - contrastive_pretraining - INFO - Epoch 5162 - contrastive_loss/batch: 0.4992\n",
            "2025-10-28 18:25:12,631 - contrastive_pretraining - INFO - Epoch 5162 - learning_rate: 0.0003\n",
            "Epoch 72/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0129]\n",
            "2025-10-28 18:25:38,767 - contrastive_pretraining - INFO - Epoch 72 - Average Loss: 0.4037\n",
            "2025-10-28 18:25:38,768 - contrastive_pretraining - INFO - Epoch 71 - contrastive_loss/epoch: 0.4037\n",
            "Epoch 73/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.4623]2025-10-28 18:25:42,137 - contrastive_pretraining - INFO - Epoch 5184 - contrastive_loss/batch: 0.4623\n",
            "2025-10-28 18:25:42,138 - contrastive_pretraining - INFO - Epoch 5184 - learning_rate: 0.0003\n",
            "Epoch 73/100:  69% 50/72 [01:07<00:28,  1.28s/it, Loss=0.4005]2025-10-28 18:26:46,358 - contrastive_pretraining - INFO - Epoch 5234 - contrastive_loss/batch: 0.4005\n",
            "2025-10-28 18:26:46,359 - contrastive_pretraining - INFO - Epoch 5234 - learning_rate: 0.0003\n",
            "Epoch 73/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.0589]\n",
            "2025-10-28 18:27:12,296 - contrastive_pretraining - INFO - Epoch 73 - Average Loss: 0.4102\n",
            "2025-10-28 18:27:12,296 - contrastive_pretraining - INFO - Epoch 72 - contrastive_loss/epoch: 0.4102\n",
            "Epoch 74/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3608]2025-10-28 18:27:14,780 - contrastive_pretraining - INFO - Epoch 5256 - contrastive_loss/batch: 0.3608\n",
            "2025-10-28 18:27:14,780 - contrastive_pretraining - INFO - Epoch 5256 - learning_rate: 0.0003\n",
            "Epoch 74/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3646]2025-10-28 18:28:18,771 - contrastive_pretraining - INFO - Epoch 5306 - contrastive_loss/batch: 0.3646\n",
            "2025-10-28 18:28:18,772 - contrastive_pretraining - INFO - Epoch 5306 - learning_rate: 0.0003\n",
            "Epoch 74/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0227]\n",
            "2025-10-28 18:28:44,797 - contrastive_pretraining - INFO - Epoch 74 - Average Loss: 0.4043\n",
            "2025-10-28 18:28:44,797 - contrastive_pretraining - INFO - Epoch 73 - contrastive_loss/epoch: 0.4043\n",
            "Epoch 75/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3833]2025-10-28 18:28:46,889 - contrastive_pretraining - INFO - Epoch 5328 - contrastive_loss/batch: 0.3833\n",
            "2025-10-28 18:28:46,894 - contrastive_pretraining - INFO - Epoch 5328 - learning_rate: 0.0003\n",
            "Epoch 75/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.4615]2025-10-28 18:29:51,221 - contrastive_pretraining - INFO - Epoch 5378 - contrastive_loss/batch: 0.4615\n",
            "2025-10-28 18:29:51,221 - contrastive_pretraining - INFO - Epoch 5378 - learning_rate: 0.0003\n",
            "Epoch 75/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0266]\n",
            "2025-10-28 18:30:17,232 - contrastive_pretraining - INFO - Epoch 75 - Average Loss: 0.4000\n",
            "2025-10-28 18:30:17,232 - contrastive_pretraining - INFO - Epoch 74 - contrastive_loss/epoch: 0.4000\n",
            "Epoch 76/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4092]2025-10-28 18:30:19,679 - contrastive_pretraining - INFO - Epoch 5400 - contrastive_loss/batch: 0.4092\n",
            "2025-10-28 18:30:19,690 - contrastive_pretraining - INFO - Epoch 5400 - learning_rate: 0.0003\n",
            "Epoch 76/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.5320]2025-10-28 18:31:23,716 - contrastive_pretraining - INFO - Epoch 5450 - contrastive_loss/batch: 0.5320\n",
            "2025-10-28 18:31:23,716 - contrastive_pretraining - INFO - Epoch 5450 - learning_rate: 0.0003\n",
            "Epoch 76/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0472]\n",
            "2025-10-28 18:31:49,710 - contrastive_pretraining - INFO - Epoch 76 - Average Loss: 0.4138\n",
            "2025-10-28 18:31:49,710 - contrastive_pretraining - INFO - Epoch 75 - contrastive_loss/epoch: 0.4138\n",
            "Epoch 77/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4223]2025-10-28 18:31:52,140 - contrastive_pretraining - INFO - Epoch 5472 - contrastive_loss/batch: 0.4223\n",
            "2025-10-28 18:31:52,144 - contrastive_pretraining - INFO - Epoch 5472 - learning_rate: 0.0003\n",
            "Epoch 77/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3762]2025-10-28 18:32:56,275 - contrastive_pretraining - INFO - Epoch 5522 - contrastive_loss/batch: 0.3762\n",
            "2025-10-28 18:32:56,275 - contrastive_pretraining - INFO - Epoch 5522 - learning_rate: 0.0003\n",
            "Epoch 77/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0409]\n",
            "2025-10-28 18:33:22,316 - contrastive_pretraining - INFO - Epoch 77 - Average Loss: 0.3944\n",
            "2025-10-28 18:33:22,316 - contrastive_pretraining - INFO - Epoch 76 - contrastive_loss/epoch: 0.3944\n",
            "Epoch 78/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4683]2025-10-28 18:33:24,676 - contrastive_pretraining - INFO - Epoch 5544 - contrastive_loss/batch: 0.4683\n",
            "2025-10-28 18:33:24,679 - contrastive_pretraining - INFO - Epoch 5544 - learning_rate: 0.0003\n",
            "Epoch 78/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.4299]2025-10-28 18:34:28,980 - contrastive_pretraining - INFO - Epoch 5594 - contrastive_loss/batch: 0.4299\n",
            "2025-10-28 18:34:28,981 - contrastive_pretraining - INFO - Epoch 5594 - learning_rate: 0.0003\n",
            "Epoch 78/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0201]\n",
            "2025-10-28 18:34:55,011 - contrastive_pretraining - INFO - Epoch 78 - Average Loss: 0.4028\n",
            "2025-10-28 18:34:55,011 - contrastive_pretraining - INFO - Epoch 77 - contrastive_loss/epoch: 0.4028\n",
            "Epoch 79/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4013]2025-10-28 18:34:57,512 - contrastive_pretraining - INFO - Epoch 5616 - contrastive_loss/batch: 0.4013\n",
            "2025-10-28 18:34:57,513 - contrastive_pretraining - INFO - Epoch 5616 - learning_rate: 0.0003\n",
            "Epoch 79/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3270]2025-10-28 18:36:01,696 - contrastive_pretraining - INFO - Epoch 5666 - contrastive_loss/batch: 0.3270\n",
            "2025-10-28 18:36:01,697 - contrastive_pretraining - INFO - Epoch 5666 - learning_rate: 0.0003\n",
            "Epoch 79/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0741]\n",
            "2025-10-28 18:36:27,800 - contrastive_pretraining - INFO - Epoch 79 - Average Loss: 0.3965\n",
            "2025-10-28 18:36:27,800 - contrastive_pretraining - INFO - Epoch 78 - contrastive_loss/epoch: 0.3965\n",
            "Epoch 80/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4472]2025-10-28 18:36:30,432 - contrastive_pretraining - INFO - Epoch 5688 - contrastive_loss/batch: 0.4472\n",
            "2025-10-28 18:36:30,433 - contrastive_pretraining - INFO - Epoch 5688 - learning_rate: 0.0003\n",
            "Epoch 80/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.4226]2025-10-28 18:37:34,526 - contrastive_pretraining - INFO - Epoch 5738 - contrastive_loss/batch: 0.4226\n",
            "2025-10-28 18:37:34,526 - contrastive_pretraining - INFO - Epoch 5738 - learning_rate: 0.0003\n",
            "Epoch 80/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0480]\n",
            "2025-10-28 18:38:00,557 - contrastive_pretraining - INFO - Epoch 80 - Average Loss: 0.3877\n",
            "2025-10-28 18:38:00,557 - contrastive_pretraining - INFO - Epoch 79 - contrastive_loss/epoch: 0.3877\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-10-28 18:38:03,188 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_80.pth\n",
            "Epoch 81/100:   0% 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 81/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4191]2025-10-28 18:38:05,664 - contrastive_pretraining - INFO - Epoch 5760 - contrastive_loss/batch: 0.4191\n",
            "2025-10-28 18:38:05,667 - contrastive_pretraining - INFO - Epoch 5760 - learning_rate: 0.0003\n",
            "Epoch 81/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.3663]2025-10-28 18:39:09,923 - contrastive_pretraining - INFO - Epoch 5810 - contrastive_loss/batch: 0.3663\n",
            "2025-10-28 18:39:09,924 - contrastive_pretraining - INFO - Epoch 5810 - learning_rate: 0.0003\n",
            "Epoch 81/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0241]\n",
            "2025-10-28 18:39:35,756 - contrastive_pretraining - INFO - Epoch 81 - Average Loss: 0.4018\n",
            "2025-10-28 18:39:35,756 - contrastive_pretraining - INFO - Epoch 80 - contrastive_loss/epoch: 0.4018\n",
            "Epoch 82/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3916]2025-10-28 18:39:38,395 - contrastive_pretraining - INFO - Epoch 5832 - contrastive_loss/batch: 0.3916\n",
            "2025-10-28 18:39:38,396 - contrastive_pretraining - INFO - Epoch 5832 - learning_rate: 0.0003\n",
            "Epoch 82/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3400]2025-10-28 18:40:42,630 - contrastive_pretraining - INFO - Epoch 5882 - contrastive_loss/batch: 0.3400\n",
            "2025-10-28 18:40:42,630 - contrastive_pretraining - INFO - Epoch 5882 - learning_rate: 0.0003\n",
            "Epoch 82/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0269]\n",
            "2025-10-28 18:41:08,571 - contrastive_pretraining - INFO - Epoch 82 - Average Loss: 0.3711\n",
            "2025-10-28 18:41:08,571 - contrastive_pretraining - INFO - Epoch 81 - contrastive_loss/epoch: 0.3711\n",
            "Epoch 83/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3663]2025-10-28 18:41:11,233 - contrastive_pretraining - INFO - Epoch 5904 - contrastive_loss/batch: 0.3663\n",
            "2025-10-28 18:41:11,234 - contrastive_pretraining - INFO - Epoch 5904 - learning_rate: 0.0003\n",
            "Epoch 83/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.4143]2025-10-28 18:42:15,335 - contrastive_pretraining - INFO - Epoch 5954 - contrastive_loss/batch: 0.4143\n",
            "2025-10-28 18:42:15,336 - contrastive_pretraining - INFO - Epoch 5954 - learning_rate: 0.0003\n",
            "Epoch 83/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0412]\n",
            "2025-10-28 18:42:41,407 - contrastive_pretraining - INFO - Epoch 83 - Average Loss: 0.3820\n",
            "2025-10-28 18:42:41,407 - contrastive_pretraining - INFO - Epoch 82 - contrastive_loss/epoch: 0.3820\n",
            "Epoch 84/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.5070]2025-10-28 18:42:44,403 - contrastive_pretraining - INFO - Epoch 5976 - contrastive_loss/batch: 0.5070\n",
            "2025-10-28 18:42:44,404 - contrastive_pretraining - INFO - Epoch 5976 - learning_rate: 0.0003\n",
            "Epoch 84/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.5081]2025-10-28 18:43:48,360 - contrastive_pretraining - INFO - Epoch 6026 - contrastive_loss/batch: 0.5081\n",
            "2025-10-28 18:43:48,360 - contrastive_pretraining - INFO - Epoch 6026 - learning_rate: 0.0003\n",
            "Epoch 84/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.1102]\n",
            "2025-10-28 18:44:14,350 - contrastive_pretraining - INFO - Epoch 84 - Average Loss: 0.3943\n",
            "2025-10-28 18:44:14,350 - contrastive_pretraining - INFO - Epoch 83 - contrastive_loss/epoch: 0.3943\n",
            "Epoch 85/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4701]2025-10-28 18:44:16,933 - contrastive_pretraining - INFO - Epoch 6048 - contrastive_loss/batch: 0.4701\n",
            "2025-10-28 18:44:16,934 - contrastive_pretraining - INFO - Epoch 6048 - learning_rate: 0.0003\n",
            "Epoch 85/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.2834]2025-10-28 18:45:21,104 - contrastive_pretraining - INFO - Epoch 6098 - contrastive_loss/batch: 0.2834\n",
            "2025-10-28 18:45:21,105 - contrastive_pretraining - INFO - Epoch 6098 - learning_rate: 0.0003\n",
            "Epoch 85/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0181]\n",
            "2025-10-28 18:45:47,208 - contrastive_pretraining - INFO - Epoch 85 - Average Loss: 0.3685\n",
            "2025-10-28 18:45:47,209 - contrastive_pretraining - INFO - Epoch 84 - contrastive_loss/epoch: 0.3685\n",
            "Epoch 86/100:   0% 0/72 [00:03<?, ?it/s, Loss=0.3792]2025-10-28 18:45:50,499 - contrastive_pretraining - INFO - Epoch 6120 - contrastive_loss/batch: 0.3792\n",
            "2025-10-28 18:45:50,499 - contrastive_pretraining - INFO - Epoch 6120 - learning_rate: 0.0003\n",
            "Epoch 86/100:  69% 50/72 [01:07<00:28,  1.28s/it, Loss=0.3505]2025-10-28 18:46:54,640 - contrastive_pretraining - INFO - Epoch 6170 - contrastive_loss/batch: 0.3505\n",
            "2025-10-28 18:46:54,641 - contrastive_pretraining - INFO - Epoch 6170 - learning_rate: 0.0003\n",
            "Epoch 86/100: 100% 72/72 [01:33<00:00,  1.30s/it, Loss=0.1743]\n",
            "2025-10-28 18:47:20,579 - contrastive_pretraining - INFO - Epoch 86 - Average Loss: 0.3977\n",
            "2025-10-28 18:47:20,580 - contrastive_pretraining - INFO - Epoch 85 - contrastive_loss/epoch: 0.3977\n",
            "Epoch 87/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3071]2025-10-28 18:47:23,067 - contrastive_pretraining - INFO - Epoch 6192 - contrastive_loss/batch: 0.3071\n",
            "2025-10-28 18:47:23,068 - contrastive_pretraining - INFO - Epoch 6192 - learning_rate: 0.0003\n",
            "Epoch 87/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3110]2025-10-28 18:48:27,121 - contrastive_pretraining - INFO - Epoch 6242 - contrastive_loss/batch: 0.3110\n",
            "2025-10-28 18:48:27,121 - contrastive_pretraining - INFO - Epoch 6242 - learning_rate: 0.0003\n",
            "Epoch 87/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0605]\n",
            "2025-10-28 18:48:53,128 - contrastive_pretraining - INFO - Epoch 87 - Average Loss: 0.3726\n",
            "2025-10-28 18:48:53,128 - contrastive_pretraining - INFO - Epoch 86 - contrastive_loss/epoch: 0.3726\n",
            "Epoch 88/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.2509]2025-10-28 18:48:55,583 - contrastive_pretraining - INFO - Epoch 6264 - contrastive_loss/batch: 0.2509\n",
            "2025-10-28 18:48:55,586 - contrastive_pretraining - INFO - Epoch 6264 - learning_rate: 0.0003\n",
            "Epoch 88/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.4091]2025-10-28 18:49:59,690 - contrastive_pretraining - INFO - Epoch 6314 - contrastive_loss/batch: 0.4091\n",
            "2025-10-28 18:49:59,691 - contrastive_pretraining - INFO - Epoch 6314 - learning_rate: 0.0003\n",
            "Epoch 88/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0512]\n",
            "2025-10-28 18:50:25,526 - contrastive_pretraining - INFO - Epoch 88 - Average Loss: 0.3680\n",
            "2025-10-28 18:50:25,526 - contrastive_pretraining - INFO - Epoch 87 - contrastive_loss/epoch: 0.3680\n",
            "Epoch 89/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3612]2025-10-28 18:50:28,222 - contrastive_pretraining - INFO - Epoch 6336 - contrastive_loss/batch: 0.3612\n",
            "2025-10-28 18:50:28,223 - contrastive_pretraining - INFO - Epoch 6336 - learning_rate: 0.0003\n",
            "Epoch 89/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.3341]2025-10-28 18:51:32,263 - contrastive_pretraining - INFO - Epoch 6386 - contrastive_loss/batch: 0.3341\n",
            "2025-10-28 18:51:32,263 - contrastive_pretraining - INFO - Epoch 6386 - learning_rate: 0.0003\n",
            "Epoch 89/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0236]\n",
            "2025-10-28 18:51:58,264 - contrastive_pretraining - INFO - Epoch 89 - Average Loss: 0.3669\n",
            "2025-10-28 18:51:58,264 - contrastive_pretraining - INFO - Epoch 88 - contrastive_loss/epoch: 0.3669\n",
            "Epoch 90/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4363]2025-10-28 18:52:00,795 - contrastive_pretraining - INFO - Epoch 6408 - contrastive_loss/batch: 0.4363\n",
            "2025-10-28 18:52:00,796 - contrastive_pretraining - INFO - Epoch 6408 - learning_rate: 0.0003\n",
            "Epoch 90/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.3448]2025-10-28 18:53:04,715 - contrastive_pretraining - INFO - Epoch 6458 - contrastive_loss/batch: 0.3448\n",
            "2025-10-28 18:53:04,715 - contrastive_pretraining - INFO - Epoch 6458 - learning_rate: 0.0003\n",
            "Epoch 90/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0456]\n",
            "2025-10-28 18:53:30,687 - contrastive_pretraining - INFO - Epoch 90 - Average Loss: 0.4078\n",
            "2025-10-28 18:53:30,687 - contrastive_pretraining - INFO - Epoch 89 - contrastive_loss/epoch: 0.4078\n",
            "Epoch 91/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3990]2025-10-28 18:53:33,485 - contrastive_pretraining - INFO - Epoch 6480 - contrastive_loss/batch: 0.3990\n",
            "2025-10-28 18:53:33,485 - contrastive_pretraining - INFO - Epoch 6480 - learning_rate: 0.0003\n",
            "Epoch 91/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3960]2025-10-28 18:54:37,607 - contrastive_pretraining - INFO - Epoch 6530 - contrastive_loss/batch: 0.3960\n",
            "2025-10-28 18:54:37,608 - contrastive_pretraining - INFO - Epoch 6530 - learning_rate: 0.0003\n",
            "Epoch 91/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0224]\n",
            "2025-10-28 18:55:03,688 - contrastive_pretraining - INFO - Epoch 91 - Average Loss: 0.3696\n",
            "2025-10-28 18:55:03,688 - contrastive_pretraining - INFO - Epoch 90 - contrastive_loss/epoch: 0.3696\n",
            "Epoch 92/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4652]2025-10-28 18:55:06,231 - contrastive_pretraining - INFO - Epoch 6552 - contrastive_loss/batch: 0.4652\n",
            "2025-10-28 18:55:06,231 - contrastive_pretraining - INFO - Epoch 6552 - learning_rate: 0.0003\n",
            "Epoch 92/100:  69% 50/72 [01:06<00:28,  1.27s/it, Loss=0.3597]2025-10-28 18:56:10,128 - contrastive_pretraining - INFO - Epoch 6602 - contrastive_loss/batch: 0.3597\n",
            "2025-10-28 18:56:10,128 - contrastive_pretraining - INFO - Epoch 6602 - learning_rate: 0.0003\n",
            "Epoch 92/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0124]\n",
            "2025-10-28 18:56:35,996 - contrastive_pretraining - INFO - Epoch 92 - Average Loss: 0.3845\n",
            "2025-10-28 18:56:35,997 - contrastive_pretraining - INFO - Epoch 91 - contrastive_loss/epoch: 0.3845\n",
            "Epoch 93/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3513]2025-10-28 18:56:38,983 - contrastive_pretraining - INFO - Epoch 6624 - contrastive_loss/batch: 0.3513\n",
            "2025-10-28 18:56:38,984 - contrastive_pretraining - INFO - Epoch 6624 - learning_rate: 0.0003\n",
            "Epoch 93/100:  69% 50/72 [01:07<00:28,  1.28s/it, Loss=0.4338]2025-10-28 18:57:43,082 - contrastive_pretraining - INFO - Epoch 6674 - contrastive_loss/batch: 0.4338\n",
            "2025-10-28 18:57:43,082 - contrastive_pretraining - INFO - Epoch 6674 - learning_rate: 0.0003\n",
            "Epoch 93/100: 100% 72/72 [01:33<00:00,  1.29s/it, Loss=0.0404]\n",
            "2025-10-28 18:58:09,034 - contrastive_pretraining - INFO - Epoch 93 - Average Loss: 0.3701\n",
            "2025-10-28 18:58:09,034 - contrastive_pretraining - INFO - Epoch 92 - contrastive_loss/epoch: 0.3701\n",
            "Epoch 94/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3561]2025-10-28 18:58:11,405 - contrastive_pretraining - INFO - Epoch 6696 - contrastive_loss/batch: 0.3561\n",
            "2025-10-28 18:58:11,406 - contrastive_pretraining - INFO - Epoch 6696 - learning_rate: 0.0003\n",
            "Epoch 94/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.2643]2025-10-28 18:59:15,419 - contrastive_pretraining - INFO - Epoch 6746 - contrastive_loss/batch: 0.2643\n",
            "2025-10-28 18:59:15,419 - contrastive_pretraining - INFO - Epoch 6746 - learning_rate: 0.0003\n",
            "Epoch 94/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0159]\n",
            "2025-10-28 18:59:41,426 - contrastive_pretraining - INFO - Epoch 94 - Average Loss: 0.3782\n",
            "2025-10-28 18:59:41,427 - contrastive_pretraining - INFO - Epoch 93 - contrastive_loss/epoch: 0.3782\n",
            "Epoch 95/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4512]2025-10-28 18:59:44,180 - contrastive_pretraining - INFO - Epoch 6768 - contrastive_loss/batch: 0.4512\n",
            "2025-10-28 18:59:44,181 - contrastive_pretraining - INFO - Epoch 6768 - learning_rate: 0.0003\n",
            "Epoch 95/100:  69% 50/72 [01:07<00:28,  1.29s/it, Loss=0.4882]2025-10-28 19:00:48,435 - contrastive_pretraining - INFO - Epoch 6818 - contrastive_loss/batch: 0.4882\n",
            "2025-10-28 19:00:48,436 - contrastive_pretraining - INFO - Epoch 6818 - learning_rate: 0.0003\n",
            "Epoch 95/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0216]\n",
            "2025-10-28 19:01:14,411 - contrastive_pretraining - INFO - Epoch 95 - Average Loss: 0.3832\n",
            "2025-10-28 19:01:14,411 - contrastive_pretraining - INFO - Epoch 94 - contrastive_loss/epoch: 0.3832\n",
            "Epoch 96/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.4164]2025-10-28 19:01:16,949 - contrastive_pretraining - INFO - Epoch 6840 - contrastive_loss/batch: 0.4164\n",
            "2025-10-28 19:01:16,950 - contrastive_pretraining - INFO - Epoch 6840 - learning_rate: 0.0003\n",
            "Epoch 96/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.4360]2025-10-28 19:02:20,887 - contrastive_pretraining - INFO - Epoch 6890 - contrastive_loss/batch: 0.4360\n",
            "2025-10-28 19:02:20,888 - contrastive_pretraining - INFO - Epoch 6890 - learning_rate: 0.0003\n",
            "Epoch 96/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0265]\n",
            "2025-10-28 19:02:46,885 - contrastive_pretraining - INFO - Epoch 96 - Average Loss: 0.3751\n",
            "2025-10-28 19:02:46,886 - contrastive_pretraining - INFO - Epoch 95 - contrastive_loss/epoch: 0.3751\n",
            "Epoch 97/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.2711]2025-10-28 19:02:49,376 - contrastive_pretraining - INFO - Epoch 6912 - contrastive_loss/batch: 0.2711\n",
            "2025-10-28 19:02:49,377 - contrastive_pretraining - INFO - Epoch 6912 - learning_rate: 0.0003\n",
            "Epoch 97/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3288]2025-10-28 19:03:53,603 - contrastive_pretraining - INFO - Epoch 6962 - contrastive_loss/batch: 0.3288\n",
            "2025-10-28 19:03:53,603 - contrastive_pretraining - INFO - Epoch 6962 - learning_rate: 0.0003\n",
            "Epoch 97/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0512]\n",
            "2025-10-28 19:04:19,678 - contrastive_pretraining - INFO - Epoch 97 - Average Loss: 0.3684\n",
            "2025-10-28 19:04:19,679 - contrastive_pretraining - INFO - Epoch 96 - contrastive_loss/epoch: 0.3684\n",
            "Epoch 98/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3548]2025-10-28 19:04:22,127 - contrastive_pretraining - INFO - Epoch 6984 - contrastive_loss/batch: 0.3548\n",
            "2025-10-28 19:04:22,131 - contrastive_pretraining - INFO - Epoch 6984 - learning_rate: 0.0003\n",
            "Epoch 98/100:  69% 50/72 [01:06<00:28,  1.28s/it, Loss=0.3765]2025-10-28 19:05:26,146 - contrastive_pretraining - INFO - Epoch 7034 - contrastive_loss/batch: 0.3765\n",
            "2025-10-28 19:05:26,147 - contrastive_pretraining - INFO - Epoch 7034 - learning_rate: 0.0003\n",
            "Epoch 98/100: 100% 72/72 [01:32<00:00,  1.28s/it, Loss=0.0321]\n",
            "2025-10-28 19:05:52,085 - contrastive_pretraining - INFO - Epoch 98 - Average Loss: 0.3694\n",
            "2025-10-28 19:05:52,085 - contrastive_pretraining - INFO - Epoch 97 - contrastive_loss/epoch: 0.3694\n",
            "Epoch 99/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3601]2025-10-28 19:05:54,780 - contrastive_pretraining - INFO - Epoch 7056 - contrastive_loss/batch: 0.3601\n",
            "2025-10-28 19:05:54,780 - contrastive_pretraining - INFO - Epoch 7056 - learning_rate: 0.0003\n",
            "Epoch 99/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3948]2025-10-28 19:06:58,962 - contrastive_pretraining - INFO - Epoch 7106 - contrastive_loss/batch: 0.3948\n",
            "2025-10-28 19:06:58,963 - contrastive_pretraining - INFO - Epoch 7106 - learning_rate: 0.0003\n",
            "Epoch 99/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.3562]\n",
            "2025-10-28 19:07:24,968 - contrastive_pretraining - INFO - Epoch 99 - Average Loss: 0.3578\n",
            "2025-10-28 19:07:24,969 - contrastive_pretraining - INFO - Epoch 98 - contrastive_loss/epoch: 0.3578\n",
            "Epoch 100/100:   0% 0/72 [00:02<?, ?it/s, Loss=0.3099]2025-10-28 19:07:27,725 - contrastive_pretraining - INFO - Epoch 7128 - contrastive_loss/batch: 0.3099\n",
            "2025-10-28 19:07:27,728 - contrastive_pretraining - INFO - Epoch 7128 - learning_rate: 0.0003\n",
            "Epoch 100/100:  69% 50/72 [01:06<00:28,  1.29s/it, Loss=0.3262]2025-10-28 19:08:31,944 - contrastive_pretraining - INFO - Epoch 7178 - contrastive_loss/batch: 0.3262\n",
            "2025-10-28 19:08:31,944 - contrastive_pretraining - INFO - Epoch 7178 - learning_rate: 0.0003\n",
            "Epoch 100/100: 100% 72/72 [01:32<00:00,  1.29s/it, Loss=0.0211]\n",
            "2025-10-28 19:08:57,963 - contrastive_pretraining - INFO - Epoch 100 - Average Loss: 0.3738\n",
            "2025-10-28 19:08:57,963 - contrastive_pretraining - INFO - Epoch 99 - contrastive_loss/epoch: 0.3738\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "2025-10-28 19:09:04,224 - contrastive_pretraining - INFO - Checkpoint saved: checkpoints/contrastive_epoch_100.pth\n",
            "2025-10-28 19:09:04,612 - contrastive_pretraining - INFO - Contrastive pre-training completed! Model saved to /content/models/contrastive_pretrained.pth\n",
            "Contrastive loss plot saved: plots/contrastive_loss.png\n",
            "2025-10-28 19:09:05,154 - contrastive_pretraining - INFO - === Contrastive Pre-training Finished ===\n",
            "2025-10-28 19:09:12.518986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761678552.538194   42267 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761678552.544018   42267 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761678552.559309   42267 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761678552.559339   42267 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761678552.559343   42267 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761678552.559348   42267 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-28 19:09:12.563946: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✓ All directories created\n",
            "2025-10-28 19:09:22.079951: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761678562.099146    1236 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761678562.104929    1236 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761678562.119319    1236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761678562.119348    1236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761678562.119351    1236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761678562.119355    1236 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-28 19:09:22.123629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✓ All directories created\n",
            "✓ All directories created\n",
            "2025-10-28 19:09:24,105 - model_evaluation - INFO - === Model Evaluation on Test Set ===\n",
            "2025-10-28 19:09:24,448 - model_evaluation - ERROR - Trained model not found at /content/models/supervised_final.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the output\n",
        "!zip -r contrastive_training_output.zip /content/contrastive_training_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M-zJRaS3Muk",
        "outputId": "43a29580-2de6-4eff-b722-1376c7941072"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/contrastive_training_output/ (stored 0%)\n",
            "  adding: content/contrastive_training_output/checkpoints/ (stored 0%)\n",
            "  adding: content/contrastive_training_output/checkpoints/contrastive_epoch_80.pth (deflated 9%)\n",
            "  adding: content/contrastive_training_output/checkpoints/contrastive_epoch_60.pth (deflated 9%)\n",
            "  adding: content/contrastive_training_output/checkpoints/contrastive_epoch_40.pth (deflated 9%)\n",
            "  adding: content/contrastive_training_output/checkpoints/contrastive_epoch_100.pth (deflated 9%)\n",
            "  adding: content/contrastive_training_output/checkpoints/contrastive_epoch_20.pth (deflated 9%)\n",
            "  adding: content/contrastive_training_output/models/ (stored 0%)\n",
            "  adding: content/contrastive_training_output/models/contrastive_pretrained.pth (deflated 7%)\n",
            "  adding: content/contrastive_training_output/plots/ (stored 0%)\n",
            "  adding: content/contrastive_training_output/plots/contrastive_loss.png (deflated 23%)\n",
            "  adding: content/contrastive_training_output/logs/ (stored 0%)\n",
            "  adding: content/contrastive_training_output/logs/contrastive_pretraining.log (deflated 89%)\n",
            "  adding: content/contrastive_training_output/logs/model_evaluation.log (deflated 31%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training Phase 2 - Supervised Learning"
      ],
      "metadata": {
        "id": "wzbTABHGQ263"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"/content/SSCLNet-Implementation/train_supervised.py\""
      ],
      "metadata": {
        "id": "vYoNIzhXQT-E",
        "outputId": "000e091d-0fe8-4311-8882-abf07d5b95c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-28 19:23:25.030355: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761679405.052503   45876 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761679405.058822   45876 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761679405.074552   45876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761679405.074577   45876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761679405.074581   45876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761679405.074586   45876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-28 19:23:25.079437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "✓ All directories created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Evaluation"
      ],
      "metadata": {
        "id": "eLS_xJ6dRy8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing phase maybe:\n",
        "#python \"/content/SSCLNet-Implementation/eval.py\""
      ],
      "metadata": {
        "id": "c9GFNgJXRHML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
